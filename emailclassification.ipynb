{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf524a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                             BaggingClassifier, ExtraTreesClassifier,\n",
    "                             GradientBoostingClassifier, VotingClassifier,\n",
    "                             StackingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Determine Base Directory for Notebook/Script ---\n",
    "# This block intelligently determines the base director\n",
    "\n",
    "try:\n",
    "    # If running as a script, __file__ is defined\n",
    "    # Get the directory of the current script\n",
    "    current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    # Assuming spam.csv is directly in the same directory as the script\n",
    "    base_directory = current_script_dir\n",
    "    print(f\"Running as a script. Base directory set to: '{base_directory}'\")\n",
    "except NameError:\n",
    "    base_directory = os.getcwd()\n",
    "    print(f\"Running in a notebook environment. Base directory set to CWD: '{base_directory}'\")\n",
    "    print(\"Confirm 'spam.csv' is directly in this directory for the current DATA_PATH setting.\")\n",
    "\n",
    "\n",
    "# --- Configuration (Externalize for production) ---\n",
    "class Config:\n",
    "    #DATA_PATH = os.path.join(os.path.dirname(base_directory), 'spam.csv')\n",
    "    DATA_PATH = os.path.join(base_directory, 'spam.csv')\n",
    "    SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2' # Pre-trained Sentence Transformer model\n",
    "    LOG_FILE = os.path.join(base_directory, 'spam_classifier.log') # Log file name\n",
    "    RANDOM_STATE = 42 # Seed for reproducibility\n",
    "    TEST_SIZE = 0.2 # Proportion of the dataset to include in the test split\n",
    "    N_TRIALS_OPTUNA = 10 # Number of Optuna trials for hyperparameter tuning per model\n",
    "    PLOTS_DIR = os.path.join(base_directory, 'plots') # Directory to save plots (EDA, Confusion Matrices)\n",
    "    MODELS_DIR = os.path.join(base_directory, 'models') # Directory to save trained models\n",
    "\n",
    "# Ensure plot and model directories exist at startup\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the SpamClassifier with production-ready settings.\"\"\"\n",
    "        self._configure_logging()\n",
    "        self._verify_nltk_resources()\n",
    "        self._configure_matplotlib()\n",
    "\n",
    "        self.df = None\n",
    "        self.encoder = LabelEncoder() # Used to encode 'ham'/'spam' to numerical labels\n",
    "        self.ps = PorterStemmer() # Used for stemming text\n",
    "        self.sentence_transformer_model = None # Will hold the loaded SentenceTransformer\n",
    "        self.X, self.y = None, None # Features and target after vectorization\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None]*4 # Train/Test splits\n",
    "        self.clfs = {} # Dictionary to hold initialized classifier objects (before/after tuning)\n",
    "        self.best_tuned_models_params = {} # Stores best hyperparameters found by Optuna for each model\n",
    "        self.best_model = None # Stores the best trained model object (highest F1-score on spam)\n",
    "        self.best_model_name = None # Stores the name of the best model\n",
    "        self.performance_df = pd.DataFrame() # Stores performance metrics of all models evaluated\n",
    "        self._initialize_classifiers() # Set up initial classifier instances\n",
    "        logging.info(\"SpamClassifier initialized successfully.\")\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        \"\"\"Sets up production-grade logging to a file and the console.\"\"\"\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO, # Log INFO level and above messages\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s', # Standard log format\n",
    "            handlers=[\n",
    "                logging.FileHandler(Config.LOG_FILE), # Log to a file\n",
    "                logging.StreamHandler(sys.stdout) # Log to console (stdout)\n",
    "            ]\n",
    "        )\n",
    "        warnings.filterwarnings('ignore') # Suppress warnings to keep logs clean, manage critical warnings separately\n",
    "\n",
    "    def _verify_nltk_resources(self) -> None:\n",
    "        \"\"\"Ensures required NLTK data (punkt tokenizer, stopwords corpus) is available, downloading if necessary.\"\"\"\n",
    "        resources = [\n",
    "            ('tokenizers/punkt', 'punkt'),\n",
    "            ('corpora/stopwords', 'stopwords'),\n",
    "            ('tokenizers/punkt_tab', 'punkt_tab') # Add this line\n",
    "        ]\n",
    "        for path, package in resources:\n",
    "            try:\n",
    "                # Use nltk.data.find() to check if the resource is available\n",
    "                # This will raise LookupError if not found\n",
    "                nltk.data.find(path)\n",
    "                logging.info(f\"NLTK {package} resource found.\")\n",
    "            except LookupError:\n",
    "                logging.warning(f\"NLTK {package} not found. Attempting to download...\")\n",
    "                try:\n",
    "                    # Download the missing package\n",
    "                    nltk.download(package, quiet=True)\n",
    "                    logging.info(f\"NLTK {package} downloaded successfully.\")\n",
    "                except Exception as e:\n",
    "                    # Log critical error and exit if download fails\n",
    "                    logging.critical(f\"Failed to download NLTK {package}. Please check network or proxy settings. Error: {e}\")\n",
    "                    sys.exit(1)                \n",
    "\n",
    "    def _configure_matplotlib(self) -> None:\n",
    "        \"\"\"Configures Matplotlib and Seaborn for non-interactive plotting in a production environment.\"\"\"\n",
    "        plt.ioff() # Disable interactive mode (important for scripts run without a display)\n",
    "        sns.set(style='whitegrid', palette='viridis') # Set a professional and visually appealing style\n",
    "\n",
    "    def _initialize_classifiers(self) -> None:\n",
    "        self.clfs = {\n",
    "            'LR': LogisticRegression(\n",
    "                solver='liblinear', # Good for smaller datasets and L1/L2 regularization\n",
    "                penalty='l1',       # L1 regularization for feature selection\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced', # Automatically adjusts weights inversely proportional to class frequencies\n",
    "                max_iter=1000       # Increased max_iter for convergence\n",
    "            ),\n",
    "            'RF': RandomForestClassifier(\n",
    "                n_estimators=100,   # Number of trees in the forest\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1           # Use all available CPU cores for parallel processing\n",
    "            ),\n",
    "            'XGB': XGBClassifier(\n",
    "                n_estimators=100,   # Number of boosting rounds\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                #use_label_encoder=False, # Suppresses a future warning\n",
    "                eval_metric='logloss', # Evaluation metric, required for newer XGBoost versions\n",
    "                scale_pos_weight=1  # Placeholder: will be dynamically set in `eda` based on class imbalance\n",
    "            ),\n",
    "            'SVC': SVC(kernel='sigmoid', gamma=1.0, probability=True, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "            'KN': KNeighborsClassifier(),\n",
    "            # 'NB': MultinomialNB(), # Commented out as SentenceTransformers produce dense embeddings, not counts\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'BgC': BaggingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1),\n",
    "            'ETC': ExtraTreesClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "            'GBDT': GradientBoostingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'DT': DecisionTreeClassifier(max_depth=5, random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        }\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        \"\"\"Loads data from the specified CSV path with robust validation.\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(Config.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {os.path.abspath(Config.DATA_PATH)}\")\n",
    "\n",
    "            self.df = pd.read_csv(Config.DATA_PATH, encoding='latin-1') # 'latin-1' is common for spam.csv\n",
    "            if len(self.df) < 100: # Ensure sufficient data for meaningful analysis\n",
    "                raise ValueError(f\"Dataset too small ({len(self.df)} samples). Minimum 100 samples required for robust analysis.\")\n",
    "\n",
    "            logging.info(f\"Loaded {len(self.df)} records from {Config.DATA_PATH}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data loading failed: {e}\")\n",
    "            sys.exit(1) # Critical error, cannot proceed\n",
    "\n",
    "    def clean_data(self) -> None:\n",
    "        try:\n",
    "            \n",
    "            if 'v1' in self.df.columns and 'v2' in self.df.columns:\n",
    "                self.df = self.df[['v1', 'v2']].copy()\n",
    "                logging.info(\"Selected 'v1' and 'v2' columns from the dataset.\")\n",
    "            else:\n",
    "                found_v1 = next((col for col in self.df.columns if 'target' in col.lower() or 'label' in col.lower() or 'type' in col.lower()), None)\n",
    "                found_v2 = next((col for col in self.df.columns if 'text' in col.lower() or 'message' in col.lower() or 'sms' in col.lower()), None)\n",
    "\n",
    "                if found_v1 and found_v2:\n",
    "                    self.df = self.df[[found_v1, found_v2]].copy()\n",
    "                    logging.info(f\"Mapped columns '{found_v1}' to 'target' and '{found_v2}' to 'text' using heuristics.\")\n",
    "                else:\n",
    "                    # If neither standard names nor heuristics work, it's a critical error\n",
    "                    raise ValueError(f\"Could not find required 'target' and 'text' columns (v1/v2 or equivalents) in dataset. Found columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "            # Now that self.df only has 2 columns, renaming will succeed.\n",
    "            self.df.columns = ['target', 'text']\n",
    "\n",
    "            # Validate and potentially filter target values to ensure only 'ham' and 'spam' exist\n",
    "            valid_targets = {'ham', 'spam'}\n",
    "            invalid_targets = set(self.df['target'].unique()) - valid_targets\n",
    "            if invalid_targets:\n",
    "                logging.warning(f\"Invalid target values found: {invalid_targets}. Filtering out rows with these values.\")\n",
    "                self.df = self.df[self.df['target'].isin(valid_targets)]\n",
    "                if self.df.empty:\n",
    "                    raise ValueError(\"No valid 'ham' or 'spam' records remaining after filtering invalid targets. Dataset is empty.\")\n",
    "\n",
    "            # Encode target labels: 'ham' and 'spam' to numerical (e.g., 0 and 1)\n",
    "            # The encoder stores the mapping, which is important for inverse_transform later.\n",
    "            self.df['target'] = self.encoder.fit_transform(self.df['target'])\n",
    "            initial_rows = len(self.df)\n",
    "            self.df.drop_duplicates(inplace=True) # Remove duplicate rows\n",
    "            self.df.dropna(inplace=True) # Remove rows with any NaN values (should be none after column selection)\n",
    "\n",
    "            logging.info(f\"Cleaned dataset. Removed {initial_rows - len(self.df)} duplicates/nulls. Remaining: {len(self.df)} records.\")\n",
    "            if self.df.empty:\n",
    "                raise ValueError(\"Dataset became empty after cleaning steps. Check data quality or initial loading.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data cleaning failed: {e}\")\n",
    "            sys.exit(1) # Critical error, stop pipeline\n",
    "\n",
    "    def _safe_tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Tokenizes input text safely, ensuring it's a string and filtering non-alphanumeric tokens.\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) # Coerce non-string inputs to string\n",
    "            logging.debug(f\"Coerced non-string text to string for tokenization: {text[:50]}...\")\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower()) # Convert to lowercase and tokenize\n",
    "            # Filter for alphanumeric tokens and remove common single punctuation marks\n",
    "            return [t for t in tokens if t.isalnum() and t not in string.punctuation]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Tokenization failed for text (first 50 chars: '{text[:50]}...'). Returning empty list. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def eda(self) -> None:\n",
    "        try:\n",
    "            self.df['num_words'] = self.df['text'].apply(\n",
    "                lambda x: len(self._safe_tokenize(x)))\n",
    "            self.df['num_chars'] = self.df['text'].apply(len)\n",
    "            self.df['num_sentences'] = self.df['text'].apply(lambda x:len(nltk.sent_tokenize(x))) # Added sentences\n",
    "\n",
    "            # Calculate and set scale_pos_weight for XGBoost based on class imbalance\n",
    "            # This helps XGBoost handle imbalanced datasets by giving more weight to the minority class.\n",
    "            ham_count = self.df[self.df['target'] == self.encoder.transform(['ham'])[0]].shape[0]\n",
    "            spam_count = self.df[self.df['target'] == self.encoder.transform(['spam'])[0]].shape[0]\n",
    "            if spam_count > 0:\n",
    "                scale_pos_weight_val = ham_count / spam_count # ratio of negative to positive samples\n",
    "                # Update the XGBoost classifier instance directly\n",
    "                self.clfs['XGB'].set_params(scale_pos_weight=scale_pos_weight_val)\n",
    "                logging.info(f\"Set XGBoost scale_pos_weight to: {scale_pos_weight_val:.2f} (Ham:{ham_count}, Spam:{spam_count})\")\n",
    "            else:\n",
    "                logging.warning(\"No spam samples found to calculate scale_pos_weight for XGBoost. Defaulting to 1.\")\n",
    "\n",
    "            # --- Plotting Section ---\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # Unique timestamp for plot filenames\n",
    "\n",
    "            # 1. Target Distribution Pie Chart\n",
    "            fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "            self.df['target'].value_counts().plot(\n",
    "                kind='pie', ax=ax1, autopct='%1.1f%%',\n",
    "                labels=self.encoder.inverse_transform(self.df['target'].value_counts().index),\n",
    "                colors=sns.color_palette('pastel')[0:2],\n",
    "                explode=[0, 0.1] # Explode spam slice for emphasis\n",
    "            )\n",
    "            ax1.set_title('Target Class Distribution')\n",
    "            ax1.set_ylabel('') # Hide default y-label for pie chart for better aesthetics\n",
    "            fig1_filename = os.path.join(Config.PLOTS_DIR, f'target_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig1_filename, bbox_inches='tight') # bbox_inches='tight' prevents labels/titles from being cut off\n",
    "            plt.close(fig1) # Close the figure to free up memory\n",
    "            logging.info(f\"Target distribution plot saved to {fig1_filename}.\")\n",
    "\n",
    "            # 2. Word Count Distribution Histogram\n",
    "            fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax2.set_title('Word Count Distribution by Target Class')\n",
    "            ax2.set_xlabel('Number of Words')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.legend()\n",
    "            fig2_filename = os.path.join(Config.PLOTS_DIR, f'word_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig2_filename, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logging.info(f\"Word count distribution plot saved to {fig2_filename}.\")\n",
    "\n",
    "            # 3. Character Count Distribution Histogram\n",
    "            fig3, ax3 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax3.set_title('Character Count Distribution by Target Class')\n",
    "            ax3.set_xlabel('Number of Characters')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.legend()\n",
    "            fig3_filename = os.path.join(Config.PLOTS_DIR, f'char_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig3_filename, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logging.info(f\"Character count distribution plot saved to {fig3_filename}.\")\n",
    "\n",
    "            # 4. Correlation Heatmap\n",
    "            fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(self.df[['num_chars', 'num_words', 'num_sentences', 'target']].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
    "            ax4.set_title('Correlation Matrix of Text Features and Target')\n",
    "            fig4_filename = os.path.join(Config.PLOTS_DIR, f'correlation_heatmap_{timestamp}.png')\n",
    "            plt.savefig(fig4_filename, bbox_inches='tight')\n",
    "            plt.close(fig4)\n",
    "            logging.info(f\"Correlation heatmap plot saved to {fig4_filename}.\")\n",
    "\n",
    "            logging.info(f\"Descriptive statistics for Ham emails:\\n{self.df[self.df['target'] == self.encoder.transform(['ham'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "            logging.info(f\"Descriptive statistics for Spam emails:\\n{self.df[self.df['target'] == self.encoder.transform(['spam'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EDA process failed: {e}\")\n",
    "            raise # Re-raise the exception to stop the pipeline if EDA is critical\n",
    "\n",
    "    def transform_text(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text) # Coerce non-string inputs to string\n",
    "            logging.debug(f\"Coerced non-string text to string for transform_text: {text[:50]}...\")\n",
    "\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "        processed_tokens = [token for token in tokens if token.isalnum()]\n",
    "\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in processed_tokens if token not in stop_words and token not in string.punctuation]\n",
    "\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "\n",
    "        final_tokens = [token for token in stemmed_tokens if len(token) > 1 or token.isdigit()]\n",
    "\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def preprocess_text(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"\\n--- Text Preprocessing for EDA and Visualizations ---\")\n",
    "            # Apply transformation for EDA specific columns\n",
    "            self.df['transformed_text'] = self.df['text'].apply(self.transform_text)\n",
    "            logging.info(\"Text transformation for EDA complete. Example:\")\n",
    "            logging.info(f\"\\n{self.df[['text', 'transformed_text']].head().to_string()}\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            logging.info(\"\\nGenerating Word Clouds (saved to plots directory):\")\n",
    "            spam_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(spam_wc)\n",
    "            plt.title('Spam Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_spam_filename = os.path.join(Config.PLOTS_DIR, f'spam_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_spam_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Spam word cloud saved to {wc_spam_filename}.\")\n",
    "\n",
    "            ham_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(ham_wc)\n",
    "            plt.title('Ham Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_ham_filename = os.path.join(Config.PLOTS_DIR, f'ham_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_ham_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Ham word cloud saved to {wc_ham_filename}.\")\n",
    "\n",
    "            logging.info(\"\\nMost common words in Spam (saved as plot):\")\n",
    "            spam_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(spam_corpus, title='Top 30 Spam Words', filename=f'top_spam_words_{timestamp}.png')\n",
    "\n",
    "            logging.info(\"\\nMost common words in Ham (saved as plot):\")\n",
    "            ham_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(ham_corpus, title='Top 30 Ham Words', filename=f'top_ham_words_{timestamp}.png')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text preprocessing for EDA failed: {e}\")\n",
    "            sys.exit(1) # Critical error, stop pipeline\n",
    "\n",
    "    def _plot_most_common_words(self, corpus: list[str], title: str, n: int = 30, filename: str = \"common_words.png\") -> None:\n",
    "        \"\"\"Helper to plot and save most common words.\"\"\"\n",
    "        common_words = Counter(corpus).most_common(n)\n",
    "        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(x='Word', y='Count', data=df_common_words, ax=ax, palette='viridis')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        plot_filepath = os.path.join(Config.PLOTS_DIR, filename)\n",
    "        plt.savefig(plot_filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Plot '{title}' saved to {plot_filepath}.\")\n",
    "\n",
    "\n",
    "    def vectorize_text_with_embeddings(self) -> None:\n",
    "        \"\"\"\n",
    "        Converts text data into numerical features using SentenceTransformer\n",
    "        for contextual embeddings.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"\\n--- Text Vectorization (SentenceTransformer: {Config.SENTENCE_TRANSFORMER_MODEL}) ---\")\n",
    "            if self.sentence_transformer_model is None:\n",
    "                self.sentence_transformer_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)\n",
    "            \n",
    "            # Use raw text for SentenceTransformer as it handles internal tokenization/preprocessing best\n",
    "            # It's generally not recommended to stem/remove stopwords for S-BERT\n",
    "            self.X = self.sentence_transformer_model.encode(\n",
    "                self.df['text'].tolist(),\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                batch_size=64 # Adjust based on memory\n",
    "            )\n",
    "            self.y = self.df['target'].values\n",
    "            logging.info(f\"SentenceTransformer embedding complete. X shape: {self.X.shape}, Y shape: {self.y.shape}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text vectorization failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Splits the data into training and testing sets using stratified sampling\n",
    "        to maintain class proportions.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=Config.TEST_SIZE,\n",
    "                random_state=Config.RANDOM_STATE, stratify=self.y)\n",
    "            logging.info(f\"Data split: Train {len(self.X_train)} samples, Test {len(self.X_test)} samples.\")\n",
    "            logging.info(f\"Train target distribution: {np.bincount(self.y_train)}\")\n",
    "            logging.info(f\"Test target distribution: {np.bincount(self.y_test)}\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data splitting failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _objective(self, trial: optuna.trial.Trial, model_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Optuna objective function for hyperparameter tuning.\n",
    "        It defines the search space for a given model, trains it within an ImbPipeline\n",
    "        (to handle SMOTE correctly), and returns the cross-validated F1-score for 'spam' to be maximized.\n",
    "        \"\"\"\n",
    "        # Define hyperparameter search spaces for each model\n",
    "        if model_name == 'LR':\n",
    "            c_param = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "            solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            model = LogisticRegression(C=c_param, solver=solver, random_state=Config.RANDOM_STATE,\n",
    "                                       class_weight='balanced', max_iter=2000,\n",
    "                                       n_jobs=-1 if solver == 'saga' else None)\n",
    "        elif model_name == 'RF':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'XGB':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.005, 0.5)\n",
    "            subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "            gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "            current_scale_pos_weight = self.clfs['XGB'].get_params().get('scale_pos_weight', 1)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate, subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree, gamma=gamma,\n",
    "                                  random_state=Config.RANDOM_STATE,\n",
    "                                  eval_metric='logloss',\n",
    "                                  scale_pos_weight=current_scale_pos_weight)\n",
    "        elif model_name == 'SVC':\n",
    "            C_param = trial.suggest_loguniform('C', 1e-2, 1e2)\n",
    "            gamma_param = trial.suggest_loguniform('gamma', 1e-3, 1e1)\n",
    "            kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid']) # Linear also an option\n",
    "            model = SVC(C=C_param, gamma=gamma_param, kernel=kernel, probability=True,\n",
    "                        random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        elif model_name == 'KN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, n_jobs=-1)\n",
    "        elif model_name == 'AdaBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'BgC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            model = BaggingClassifier(n_estimators=n_estimators, random_state=Config.RANDOM_STATE, n_jobs=-1)\n",
    "        elif model_name == 'ETC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'GBDT':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'DT':\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf, criterion=criterion,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' is not configured for Optuna tuning.\")\n",
    "\n",
    "        # Create an ImbPipeline: SMOTE is applied *only* to the training fold in cross-validation.\n",
    "        # This prevents data leakage and provides a more realistic performance estimate.\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "\n",
    "        # Perform stratified k-fold cross-validation, optimizing for F1-score of the 'spam' class\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "        # Use make_scorer for specific positive label for F1 score, or simply 'f1' if positive class is 1\n",
    "        # The encoder transforms 'spam' to 1 in this dataset, so 'f1' should work.\n",
    "        scores = cross_val_score(pipeline, self.X_train, self.y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "        return scores.mean() # Return the mean F1-score across folds\n",
    "\n",
    "    def tune_models(self) -> None:\n",
    "        \"\"\"Performs hyperparameter tuning for each selected classifier using Optuna.\"\"\"\n",
    "        try:\n",
    "            if self.X_train is None or self.y_train is None:\n",
    "                logging.error(\"Data not split for tuning. Calling split_data().\")\n",
    "                self.split_data() # Ensure data is split before tuning\n",
    "\n",
    "            logging.info(\"Starting hyperparameter tuning with Optuna for selected models...\")\n",
    "            # We tune models with sufficient complexity or those where tuning typically yields large gains\n",
    "            # For simpler models like KN, tuning might be less critical or a wider range might be needed.\n",
    "            models_to_tune = ['LR', 'RF', 'XGB', 'SVC', 'ETC'] # Focus tuning on key models\n",
    "\n",
    "            for name in models_to_tune:\n",
    "                if name not in self.clfs:\n",
    "                    logging.warning(f\"Model '{name}' not found in initialized classifiers, skipping tuning.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Tuning {name} model with {Config.N_TRIALS_OPTUNA} trials...\")\n",
    "                study = optuna.create_study(direction='maximize', # Maximize the objective (F1-score)\n",
    "                                            sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_STATE),\n",
    "                                            study_name=f\"{name}_tuning_study\")\n",
    "                \n",
    "                # Suppress Optuna logs from callbacks for cleaner output if desired, or let them flow\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning) # Suppress \"Figure has been deleted\" warnings from Optuna plots\n",
    "                    study.optimize(lambda trial: self._objective(trial, name),\n",
    "                                   n_trials=Config.N_TRIALS_OPTUNA,\n",
    "                                   show_progress_bar=True,\n",
    "                                   gc_after_trial=True) # <--- ADD THIS LINE HERE\n",
    "                                   # Removed plot_optimization_history from direct callbacks in _objective\n",
    "                                   # Can be called separately if plotting all studies is needed.\n",
    "\n",
    "                self.best_tuned_models_params[name] = study.best_trial.params\n",
    "                logging.info(f\"Best parameters for {name}: {study.best_trial.params}\")\n",
    "                logging.info(f\"Best cross-validated F1-score for {name}: {study.best_trial.value:.4f}\")\n",
    "\n",
    "                self.clfs[name].set_params(**study.best_trial.params)\n",
    "                # Re-apply `scale_pos_weight` for XGBoost if it was set dynamically and not part of tuning.\n",
    "                if name == 'XGB':\n",
    "                    current_scale_pos_weight = self.clfs[name].get_params().get('scale_pos_weight', 1)\n",
    "                    self.clfs[name].set_params(scale_pos_weight=current_scale_pos_weight)\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning completed for all selected models.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Model tuning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def train_final_models(self) -> None:\n",
    "        try:\n",
    "            # Ensure X_train, y_train, X_test, y_test are set.\n",
    "            if self.X_train is None or self.X_test is None:\n",
    "                 logging.error(\"Data not split for final training. Calling split_data().\")\n",
    "                 self.split_data()\n",
    "\n",
    "            logging.info(\"Applying SMOTE to the entire training data for final model training...\")\n",
    "            smote = SMOTE(random_state=Config.RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
    "            logging.info(f\"SMOTE applied. Original train: {len(self.X_train)} samples. Resampled train: {len(X_train_resampled)} samples.\")\n",
    "\n",
    "            results = [] # To store metrics for all models\n",
    "            best_f1_overall = -1 # To track the highest F1-score for 'spam'\n",
    "            self.best_model = None # Reset best model before evaluation\n",
    "            self.best_model_name = None # Reset best model name\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") # Unique timestamp for plot/model filenames\n",
    "\n",
    "            for name, model in self.clfs.items(): # Iterate over all initialized classifiers (now potentially tuned)\n",
    "                logging.info(f\"Training final {name} model on resampled data and evaluating...\")\n",
    "                \n",
    "                # Handle potential errors during training/prediction for a single model to not stop entire pipeline\n",
    "                try:\n",
    "                    model.fit(X_train_resampled, y_train_resampled) # Train the model\n",
    "                    y_pred = model.predict(self.X_test) # Make predictions on the unseen test set\n",
    "\n",
    "                    accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                    # Precision, Recall, F1-score for the 'spam' class (positive class)\n",
    "                    precision = precision_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    recall = recall_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    f1 = f1_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "\n",
    "                    report_dict = classification_report(self.y_test, y_pred, target_names=self.encoder.classes_, output_dict=True)\n",
    "\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'Precision (Spam)': precision,\n",
    "                        'Recall (Spam)': recall,\n",
    "                        'F1-Score (Spam)': f1,\n",
    "                        'Full Classification Report': report_dict # Store the comprehensive report\n",
    "                    })\n",
    "\n",
    "                    logging.info(f\"\\n--- Performance for {name} ---\")\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    logging.info(f\"Precision (Spam): {precision:.4f}\")\n",
    "                    logging.info(f\"Recall (Spam): {recall:.4f}\")\n",
    "                    logging.info(f\"F1-Score (Spam): {f1:.4f}\")\n",
    "                    logging.info(f\"\\nFull Classification Report for {name}:\\n{classification_report(self.y_test, y_pred, target_names=self.encoder.classes_)}\")\n",
    "\n",
    "                    # --- Plot and Log Confusion Matrix ---\n",
    "                    cm = confusion_matrix(self.y_test, y_pred)\n",
    "                    logging.info(f\"\\nRaw Confusion Matrix for {name}:\\n{cm}\") # Log the raw matrix\n",
    "\n",
    "                    fig_cm, ax_cm = plt.subplots(figsize=(7, 6))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=self.encoder.classes_, # Predicted labels\n",
    "                                yticklabels=self.encoder.classes_, # True labels\n",
    "                                linecolor='gray', linewidths=0.5, # Add grid lines for better readability\n",
    "                                annot_kws={\"size\": 14}) # Adjust annotation font size\n",
    "                    ax_cm.set_xlabel('Predicted Label', fontsize=12)\n",
    "                    ax_cm.set_ylabel('True Label', fontsize=12)\n",
    "                    ax_cm.set_title(f'Confusion Matrix for {name}', fontsize=14)\n",
    "                    cm_filename = os.path.join(Config.PLOTS_DIR, f'confusion_matrix_{name}_{timestamp}.png')\n",
    "                    plt.savefig(cm_filename, bbox_inches='tight')\n",
    "                    plt.close(fig_cm) # Close the figure to free memory\n",
    "                    logging.info(f\"Confusion matrix plot for {name} saved to {cm_filename}.\")\n",
    "\n",
    "                    # Identify the best performing model based on F1-score for the 'spam' class\n",
    "                    if f1 > best_f1_overall:\n",
    "                        best_f1_overall = f1\n",
    "                        self.best_model_name = name\n",
    "                        self.best_model = model # Store the actual best model object\n",
    "                except Exception as model_e:\n",
    "                    logging.error(f\"Error training or evaluating model {name}: {model_e}\")\n",
    "                    # Optionally, append a failed entry to results\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision (Spam)': np.nan,\n",
    "                        'Recall (Spam)': np.nan,\n",
    "                        'F1-Score (Spam)': np.nan,\n",
    "                        'Full Classification Report': {'error': str(model_e)}\n",
    "                    })\n",
    "\n",
    "\n",
    "            self.performance_df = pd.DataFrame(results) # Convert results list to DataFrame\n",
    "            # Sort by F1-Score (Spam) to easily identify the best model\n",
    "            self.performance_df = self.performance_df.sort_values(by='F1-Score (Spam)', ascending=False).reset_index(drop=True)\n",
    "            logging.info(f\"\\n--- Overall Best Model Identified: {self.best_model_name} (F1-Score on Spam: {best_f1_overall:.4f}) ---\")\n",
    "            logging.info(\"All model evaluations completed.\")\n",
    "\n",
    "            self._save_best_model() # Save only the best performing model for deployment\n",
    "\n",
    "            # Plot overall performance comparison\n",
    "            self._plot_performance_comparison(timestamp)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Final model training and evaluation failed: {e}\")\n",
    "            sys.exit(1) # Critical error, stop pipeline\n",
    "\n",
    "    def _save_best_model(self) -> None:\n",
    "        \"\"\"Saves the best performing model and related components to a pickle file.\"\"\"\n",
    "        try:\n",
    "            if self.best_model is None or self.best_model_name is None:\n",
    "                logging.warning(\"No best model identified or stored. Skipping model save operation.\")\n",
    "                return\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            # Create a descriptive filename including the model name and timestamp\n",
    "            model_filename = os.path.join(Config.MODELS_DIR, f'best_model_{self.best_model_name}_{timestamp}.pkl')\n",
    "\n",
    "            # Serialize and save the best model along with the SentenceTransformer and LabelEncoder.\n",
    "            # These are essential for making predictions on new, raw text.\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.best_model,\n",
    "                    'transformer': self.sentence_transformer_model,\n",
    "                    'encoder': self.encoder,\n",
    "                    'model_name': self.best_model_name,\n",
    "                    'performance_summary': self.performance_df.to_dict('records') # Save full performance summary\n",
    "                }, f)\n",
    "            logging.info(f\"Best performing model ({self.best_model_name}) saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save the best model: {e}\")\n",
    "\n",
    "    def _plot_performance_comparison(self, timestamp: str) -> None:\n",
    "        \"\"\"Plots and saves the comparison of model performance metrics.\"\"\"\n",
    "        if self.performance_df.empty:\n",
    "            logging.warning(\"Performance DataFrame is empty, cannot plot comparison.\")\n",
    "            return\n",
    "\n",
    "        # Prepare data for plotting\n",
    "        plot_df = self.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        sns.barplot(x='Model', y='Score', hue='Metric', data=plot_df_melted, palette='tab10', ax=ax)\n",
    "        ax.set_ylim(0.5, 1.0) # Set a sensible y-limit for scores\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title('Model Performance Comparison (Test Set)')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plot_filename = os.path.join(Config.PLOTS_DIR, f'model_performance_comparison_{timestamp}.png')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Model performance comparison plot saved to {plot_filename}.\")\n",
    "\n",
    "    def run_pipeline(self) -> bool:\n",
    "        \"\"\"Executes the full spam classification pipeline sequentially.\"\"\"\n",
    "        steps = [\n",
    "            ('Data Loading', self.load_data),\n",
    "            ('Data Cleaning', self.clean_data),\n",
    "            ('EDA and Feature Engineering', self.eda), # Descriptive step name\n",
    "            ('Text Preprocessing for EDA', self.preprocess_text), # New step for EDA specific text processing\n",
    "            ('Text Vectorization (Embeddings)', self.vectorize_text_with_embeddings),\n",
    "            ('Data Splitting', self.split_data), # Explicit splitting step\n",
    "            ('Hyperparameter Tuning', self.tune_models), # Optuna tuning step\n",
    "            ('Final Model Training & Evaluation', self.train_final_models)\n",
    "        ]\n",
    "\n",
    "        for name, step in steps:\n",
    "            try:\n",
    "                logging.info(f\"\\n--- Starting Pipeline Step: {name} ---\")\n",
    "                step() # Execute the current pipeline step\n",
    "                logging.info(f\"--- Completed Pipeline Step: {name} ---\\n\")\n",
    "            except SystemExit: # Catch sys.exit() calls from critical errors within steps\n",
    "                logging.critical(f\"Pipeline stopped due to critical error in step: '{name}'.\")\n",
    "                return False # Indicate pipeline failure\n",
    "            except Exception as e: # Catch any other unexpected exceptions from a step\n",
    "                logging.critical(f\"Pipeline failed unexpectedly in step '{name}': {e}\")\n",
    "                return False # Indicate pipeline failure\n",
    "\n",
    "        logging.info(\"Spam classification pipeline completed successfully.\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def load_for_inference(model_path: str) -> 'SpamClassifier':\n",
    "        \"\"\"\n",
    "        Loads a saved model and its components (SentenceTransformer, LabelEncoder) for making predictions.\n",
    "        This is a static method, allowing loading without first initializing the full SpamClassifier class.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found at {os.path.abspath(model_path)}\")\n",
    "\n",
    "            with open(model_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "            # Create a new SpamClassifier instance to encapsulate the loaded components.\n",
    "            # This ensures all necessary helper methods (like _safe_tokenize) are available.\n",
    "            classifier = SpamClassifier()\n",
    "            classifier.best_model = data['model']\n",
    "            classifier.sentence_transformer_model = data['transformer']\n",
    "            classifier.encoder = data['encoder']\n",
    "            classifier.best_model_name = data.get('model_name', 'Unknown_Model') # Get saved model name\n",
    "\n",
    "            # Re-initialize PorterStemmer as it's not directly picklable within the model object.\n",
    "            # It's a stateless component.\n",
    "            classifier.ps = PorterStemmer()\n",
    "\n",
    "            logging.info(f\"Model '{classifier.best_model_name}' loaded successfully from {model_path} for inference.\")\n",
    "            return classifier\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to load model for inference from {model_path}: {e}\")\n",
    "            raise # Re-raise the exception for external handling (e.g., in __main__)\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        if self.best_model is None or self.sentence_transformer_model is None or self.encoder is None:\n",
    "            logging.error(\"Model components not loaded. Please run run_pipeline() or load model using load_for_inference() before calling predict().\")\n",
    "            raise RuntimeError(\"Model components not available for prediction.\")\n",
    "\n",
    "        try:\n",
    "            # Vectorize the input text. `encode` expects a list of strings.\n",
    "            vector = self.sentence_transformer_model.encode([text], convert_to_tensor=False)\n",
    "\n",
    "            # Make prediction using the best model\n",
    "            prediction_encoded = self.best_model.predict(vector)[0]\n",
    "\n",
    "            # Decode the numerical prediction back to 'ham' or 'spam'\n",
    "            prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n",
    "            return prediction_label\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction failed for text '{text[:50]}...': {e}\")\n",
    "            return \"error\" # Return an error state for production systems\n",
    "\n",
    "\n",
    "# --- Main execution block for training, evaluation, and inference demonstration ---\n",
    "if __name__ == '__main__':\n",
    "    # Initialize and run the full spam classification pipeline\n",
    "    classifier = SpamClassifier()\n",
    "    pipeline_success = classifier.run_pipeline()\n",
    "\n",
    "    if pipeline_success:\n",
    "        logging.info(\"\\n=== Spam Classification Pipeline Completed Successfully ===\")\n",
    "        logging.info(\"Overall Model Performance Summary (Sorted by F1-Score on Spam):\")\n",
    "        # Print a clean summary of the performance DataFrame\n",
    "        # Ensure we're printing the F1-Score (Spam) for relevance\n",
    "        print(classifier.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].to_string())\n",
    "\n",
    "        logging.info(f\"\\nBest Performing Model Identified: {classifier.best_model_name}\")\n",
    "        logging.info(f\"Check '{Config.PLOTS_DIR}' for EDA and Confusion Matrix plots.\")\n",
    "        logging.info(f\"Check '{Config.MODELS_DIR}' for the saved best model.\")\n",
    "\n",
    "        # --- Example of Loading Saved Model and Making Predictions (Inference) ---\n",
    "        try:\n",
    "            logging.info(\"\\n--- Demonstrating Model Inference from Saved Model ---\")\n",
    "            # Find the most recently saved model file for demonstration purposes.\n",
    "            # In a real deployment, you would specify the exact path to your desired model.\n",
    "            model_files = [f for f in os.listdir(Config.MODELS_DIR) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                # Sort files by modification time to get the latest one\n",
    "                latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join(Config.MODELS_DIR, f)))\n",
    "                latest_model_path = os.path.join(Config.MODELS_DIR, latest_model_file)\n",
    "\n",
    "                logging.info(f\"Attempting to load the latest best model from: {latest_model_path}\")\n",
    "                loaded_classifier = SpamClassifier.load_for_inference(latest_model_path)\n",
    "\n",
    "                # Define a few test cases\n",
    "                test_spam_text_1 = \"WINNER! You have been selected for a 1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.\"\n",
    "                test_spam_text_2 = \"URGENT! Your bank account has been locked due to suspicious activity. Verify immediately at http://bit.ly/malicious-site to avoid closure.\"\n",
    "                test_ham_text_1 = \"Hey, just checking in. How are you doing today? Let's catch up soon for coffee!\"\n",
    "                test_ham_text_2 = \"Hi mom, can you pick up milk and bread on your way home? Thanks, love you!\"\n",
    "                test_empty_text = \"???!!!#@%\" # Example of text that might become empty after preprocessing\n",
    "\n",
    "                print(f\"\\nPrediction for SPAM text 1: '{test_spam_text_1}' -> {loaded_classifier.predict(test_spam_text_1)}\")\n",
    "                print(f\"Prediction for SPAM text 2: '{test_spam_text_2}' -> {loaded_classifier.predict(test_spam_text_2)}\")\n",
    "                print(f\"Prediction for HAM text 1: '{test_ham_text_1}' -> {loaded_classifier.predict(test_ham_text_1)}\")\n",
    "                print(f\"Prediction for HAM text 2: '{test_ham_text_2}' -> {loaded_classifier.predict(test_ham_text_2)}\")\n",
    "                print(f\"Prediction for EMPTY/NOISY text: '{test_empty_text}' -> {loaded_classifier.predict(test_empty_text)}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(\"No model files found in the 'models' directory to demonstrate inference. Run the pipeline first.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the inference demonstration: {e}\")\n",
    "            sys.exit(1) # Indicate an error in the inference demo phase\n",
    "    else:\n",
    "        logging.critical(\"Spam classification pipeline failed during execution. Please review the log file for details.\")\n",
    "        sys.exit(1) # Exit with an error code if the pipeline failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64908aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/spam_classifier_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a notebook environment. Base directory set to CWD: '/home/dev/spam_classifier_project'\n",
      "2025-07-31 19:11:33,663 - INFO - NLTK punkt resource found.\n",
      "2025-07-31 19:11:33,664 - INFO - NLTK stopwords resource found.\n",
      "2025-07-31 19:11:33,665 - INFO - NLTK punkt_tab resource found.\n",
      "2025-07-31 19:11:33,668 - INFO - SpamClassifier initialized successfully.\n",
      "2025-07-31 19:11:33,669 - INFO - \n",
      "--- Starting Pipeline Step: Data Loading ---\n",
      "2025-07-31 19:11:33,686 - INFO - Loaded 5572 records from /home/dev/spam_classifier_project/spam.csv.\n",
      "2025-07-31 19:11:33,688 - INFO - --- Completed Pipeline Step: Data Loading ---\n",
      "\n",
      "2025-07-31 19:11:33,689 - INFO - \n",
      "--- Starting Pipeline Step: Data Cleaning ---\n",
      "2025-07-31 19:11:33,692 - INFO - Selected 'v1' and 'v2' columns from the dataset.\n",
      "2025-07-31 19:11:33,704 - INFO - Cleaned dataset. Removed 403 duplicates/nulls. Remaining: 5169 records.\n",
      "2025-07-31 19:11:33,705 - INFO - --- Completed Pipeline Step: Data Cleaning ---\n",
      "\n",
      "2025-07-31 19:11:33,706 - INFO - \n",
      "--- Starting Pipeline Step: EDA and Feature Engineering ---\n",
      "2025-07-31 19:11:35,108 - INFO - Set XGBoost scale_pos_weight to: 6.92 (Ham:4516, Spam:653)\n",
      "2025-07-31 19:11:35,278 - INFO - Target distribution plot saved to /home/dev/spam_classifier_project/plots/target_distribution_20250731_191135.png.\n",
      "2025-07-31 19:11:36,052 - INFO - Word count distribution plot saved to /home/dev/spam_classifier_project/plots/word_count_distribution_20250731_191135.png.\n",
      "2025-07-31 19:11:36,821 - INFO - Character count distribution plot saved to /home/dev/spam_classifier_project/plots/char_count_distribution_20250731_191135.png.\n",
      "2025-07-31 19:11:37,190 - INFO - Correlation heatmap plot saved to /home/dev/spam_classifier_project/plots/correlation_heatmap_20250731_191135.png.\n",
      "2025-07-31 19:11:37,207 - INFO - Descriptive statistics for Ham emails:\n",
      "         num_chars    num_words  num_sentences\n",
      "count  4516.000000  4516.000000    4516.000000\n",
      "mean     70.459256    13.908769       1.820195\n",
      "std      56.358207    10.835081       1.383657\n",
      "min       2.000000     0.000000       1.000000\n",
      "25%      34.000000     7.000000       1.000000\n",
      "50%      52.000000    10.500000       1.000000\n",
      "75%      90.000000    18.000000       2.000000\n",
      "max     910.000000   162.000000      38.000000\n",
      "2025-07-31 19:11:37,220 - INFO - Descriptive statistics for Spam emails:\n",
      "        num_chars   num_words  num_sentences\n",
      "count  653.000000  653.000000     653.000000\n",
      "mean   137.891271   22.166922       2.970904\n",
      "std     30.137753    5.926027       1.488425\n",
      "min     13.000000    1.000000       1.000000\n",
      "25%    132.000000   19.000000       2.000000\n",
      "50%    149.000000   24.000000       3.000000\n",
      "75%    157.000000   26.000000       4.000000\n",
      "max    224.000000   34.000000       9.000000\n",
      "2025-07-31 19:11:37,221 - INFO - --- Completed Pipeline Step: EDA and Feature Engineering ---\n",
      "\n",
      "2025-07-31 19:11:37,223 - INFO - \n",
      "--- Starting Pipeline Step: Text Preprocessing for EDA ---\n",
      "2025-07-31 19:11:37,224 - INFO - \n",
      "--- Text Preprocessing for EDA and Visualizations ---\n",
      "2025-07-31 19:11:40,483 - INFO - Text transformation for EDA complete. Example:\n",
      "2025-07-31 19:11:40,487 - INFO - \n",
      "                                                                                                                                                          text                                                                                                               transformed_text\n",
      "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                       go jurong point crazi avail bugi great world la buffet cine got amor wat\n",
      "1                                                                                                                                Ok lar... Joking wif u oni...                                                                                                            ok lar joke wif oni\n",
      "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  free entri 2 wkli comp win fa cup final tkt 21st may text fa 87121 receiv entri question std txt rate appli 08452810075over18\n",
      "3                                                                                                            U dun say so early hor... U c already then say...                                                                                                  dun say earli hor alreadi say\n",
      "4                                                                                                Nah I don't think he goes to usf, he lives around here though                                                                                           nah think goe usf live around though\n",
      "2025-07-31 19:11:40,488 - INFO - \n",
      "Generating Word Clouds (saved to plots directory):\n",
      "2025-07-31 19:11:41,599 - INFO - Spam word cloud saved to /home/dev/spam_classifier_project/plots/spam_wordcloud_20250731_191140.png.\n",
      "2025-07-31 19:11:43,473 - INFO - Ham word cloud saved to /home/dev/spam_classifier_project/plots/ham_wordcloud_20250731_191140.png.\n",
      "2025-07-31 19:11:43,474 - INFO - \n",
      "Most common words in Spam (saved as plot):\n",
      "2025-07-31 19:11:44,292 - INFO - Plot 'Top 30 Spam Words' saved to /home/dev/spam_classifier_project/plots/top_spam_words_20250731_191140.png.\n",
      "2025-07-31 19:11:44,293 - INFO - \n",
      "Most common words in Ham (saved as plot):\n",
      "2025-07-31 19:11:45,107 - INFO - Plot 'Top 30 Ham Words' saved to /home/dev/spam_classifier_project/plots/top_ham_words_20250731_191140.png.\n",
      "2025-07-31 19:11:45,110 - INFO - --- Completed Pipeline Step: Text Preprocessing for EDA ---\n",
      "\n",
      "2025-07-31 19:11:45,111 - INFO - \n",
      "--- Starting Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "2025-07-31 19:11:45,112 - INFO - \n",
      "--- Text Vectorization (SentenceTransformer: all-MiniLM-L6-v2) ---\n",
      "2025-07-31 19:11:46,668 - INFO - Use pytorch device_name: cpu\n",
      "2025-07-31 19:11:46,670 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 81/81 [00:38<00:00,  2.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:12:35,460 - INFO - SentenceTransformer embedding complete. X shape: (5169, 384), Y shape: (5169,).\n",
      "2025-07-31 19:12:35,461 - INFO - --- Completed Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "\n",
      "2025-07-31 19:12:35,462 - INFO - \n",
      "--- Starting Pipeline Step: Data Splitting ---\n",
      "2025-07-31 19:12:35,475 - INFO - Data split: Train 4135 samples, Test 1034 samples.\n",
      "2025-07-31 19:12:35,476 - INFO - Train target distribution: [3613  522]\n",
      "2025-07-31 19:12:35,478 - INFO - Test target distribution: [903 131]\n",
      "2025-07-31 19:12:35,479 - INFO - --- Completed Pipeline Step: Data Splitting ---\n",
      "\n",
      "2025-07-31 19:12:35,481 - INFO - \n",
      "--- Starting Pipeline Step: Hyperparameter Tuning ---\n",
      "2025-07-31 19:12:35,482 - INFO - Starting hyperparameter tuning with Optuna for selected models...\n",
      "2025-07-31 19:12:35,483 - INFO - Tuning LR model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-31 19:12:35,487] A new study created in memory with name: LR_tuning_study\n",
      "  0%|          | 0/15 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:39,583] Trial 0 finished with value: 0.8195201694380092 and parameters: {'C': 0.017670169402947963, 'solver': 'liblinear'}. Best is trial 0 with value: 0.8195201694380092.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.81952:   7%|         | 1/15 [00:07<01:01,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:43,218] Trial 1 finished with value: 0.9067637144074732 and parameters: {'C': 0.39079671568228835, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  13%|        | 2/15 [00:08<00:51,  3.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:44,036] Trial 2 finished with value: 0.5636527533822596 and parameters: {'C': 0.00022310108018679258, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  20%|        | 3/15 [00:11<00:30,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:46,719] Trial 3 finished with value: 0.9252627284490291 and parameters: {'C': 1.7718847354806828, 'solver': 'saga'}. Best is trial 3 with value: 0.9252627284490291.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.925263:  27%|       | 4/15 [00:13<00:28,  2.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:48,786] Trial 4 finished with value: 0.9358347576312023 and parameters: {'C': 9.877700294007917, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  33%|      | 5/15 [00:15<00:23,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:51,170] Trial 5 finished with value: 0.7982814551985044 and parameters: {'C': 0.0012601639723276807, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  40%|      | 6/15 [00:18<00:21,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:53,799] Trial 6 finished with value: 0.8533267901688955 and parameters: {'C': 0.039054412752107935, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  47%|     | 7/15 [00:20<00:19,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:56,423] Trial 7 finished with value: 0.7962869532134003 and parameters: {'C': 0.0006870101665590031, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  53%|    | 8/15 [00:22<00:17,  2.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:57,560] Trial 8 finished with value: 0.8553585616674987 and parameters: {'C': 0.054502936945582565, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  60%|    | 9/15 [00:23<00:12,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:12:58,679] Trial 9 finished with value: 0.8757717445900015 and parameters: {'C': 0.12173252504194051, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  67%|   | 10/15 [00:25<00:08,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:01,208] Trial 10 finished with value: 0.9347469338394623 and parameters: {'C': 73.7864208342295, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  73%|  | 11/15 [00:28<00:08,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:03,799] Trial 11 finished with value: 0.9338781698536784 and parameters: {'C': 65.64817611753449, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  80%|  | 12/15 [00:30<00:06,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:06,362] Trial 12 finished with value: 0.9357473287321831 and parameters: {'C': 78.72383571224226, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  87%| | 13/15 [00:32<00:04,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:08,255] Trial 13 finished with value: 0.9331448068701527 and parameters: {'C': 8.224108054741553, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  93%|| 14/15 [00:34<00:02,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:10,175] Trial 14 finished with value: 0.9331984783268465 and parameters: {'C': 6.611757606926467, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835: 100%|| 15/15 [00:35<00:00,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:13:10,533 - INFO - Best parameters for LR: {'C': 9.877700294007917, 'solver': 'liblinear'}\n",
      "2025-07-31 19:13:10,534 - INFO - Best cross-validated F1-score for LR: 0.9358\n",
      "2025-07-31 19:13:10,536 - INFO - Tuning RF model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-31 19:13:10,538] A new study created in memory with name: RF_tuning_study\n",
      "  0%|          | 0/15 [00:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:38,057] Trial 0 finished with value: 0.9105635300372142 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.9105635300372142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.910564:   7%|         | 1/15 [00:38<06:30, 27.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:13:48,618] Trial 1 finished with value: 0.9127467132964885 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  13%|        | 2/15 [01:13<03:49, 17.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:14:23,876] Trial 2 finished with value: 0.9091251939576921 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  20%|        | 3/15 [01:46<05:08, 25.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:14:56,692] Trial 3 finished with value: 0.9170959825132309 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  27%|       | 4/15 [02:10<05:13, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:15:20,769] Trial 4 finished with value: 0.9058311877181092 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  33%|      | 5/15 [02:32<04:28, 26.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:15:43,129] Trial 5 finished with value: 0.9090006738787435 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  40%|      | 6/15 [03:04<03:48, 25.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:16:14,604] Trial 6 finished with value: 0.9097470973538184 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  47%|     | 7/15 [03:23<03:39, 27.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:16:33,826] Trial 7 finished with value: 0.9093436919014714 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  53%|    | 8/15 [03:35<02:53, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:16:45,923] Trial 8 finished with value: 0.9052117927047891 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  60%|    | 9/15 [03:49<02:04, 20.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:17:00,375] Trial 9 finished with value: 0.9089022907441778 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  67%|   | 10/15 [04:35<01:34, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:17:46,214] Trial 10 finished with value: 0.9116850200695618 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  73%|  | 11/15 [05:14<01:48, 27.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:18:25,532] Trial 11 finished with value: 0.9108888736746182 and parameters: {'n_estimators': 277, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  80%|  | 12/15 [05:23<01:32, 30.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:18:33,825] Trial 12 finished with value: 0.9084614524269989 and parameters: {'n_estimators': 59, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  87%| | 13/15 [06:07<00:48, 24.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:19:17,725] Trial 13 finished with value: 0.911159221076747 and parameters: {'n_estimators': 250, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  93%|| 14/15 [06:19<00:29, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:19:29,836] Trial 14 finished with value: 0.9118333987728944 and parameters: {'n_estimators': 90, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096: 100%|| 15/15 [06:19<00:00, 25.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:19:30,194 - INFO - Best parameters for RF: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-07-31 19:19:30,196 - INFO - Best cross-validated F1-score for RF: 0.9171\n",
      "2025-07-31 19:19:30,197 - INFO - Tuning XGB model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-31 19:19:30,199] A new study created in memory with name: XGB_tuning_study\n",
      "  0%|          | 0/15 [00:22<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:19:52,357] Trial 0 finished with value: 0.922533456170615 and parameters: {'n_estimators': 144, 'max_depth': 12, 'learning_rate': 0.14553179565665345, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'gamma': 1.7699302940633311e-07}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:   7%|         | 1/15 [00:41<05:14, 22.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:20:11,456] Trial 1 finished with value: 0.8968282232826492 and parameters: {'n_estimators': 64, 'max_depth': 11, 'learning_rate': 0.07965261308120507, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'gamma': 0.574485163632042}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  13%|        | 2/15 [01:16<04:26, 20.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:20:46,564] Trial 2 finished with value: 0.8411553008352888 and parameters: {'n_estimators': 258, 'max_depth': 5, 'learning_rate': 0.011551009439226469, 'subsample': 0.6733618039413735, 'colsample_bytree': 0.7216968971838151, 'gamma': 0.00015777981883364995}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  20%|        | 3/15 [01:34<05:25, 27.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:21:04,211] Trial 3 finished with value: 0.924591148223439 and parameters: {'n_estimators': 158, 'max_depth': 5, 'learning_rate': 0.08369042894376064, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'gamma': 8.528933855762793e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  27%|       | 4/15 [02:36<04:17, 23.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:22:06,410] Trial 4 finished with value: 0.8409469883992775 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.01254057843022616, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'gamma': 2.3528990899815284e-08}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  33%|      | 5/15 [02:57<06:13, 37.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:22:27,943] Trial 5 finished with value: 0.6888456060996102 and parameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.006746417134006626, 'subsample': 0.9795542149013333, 'colsample_bytree': 0.9862528132298237, 'gamma': 0.02932100047183291}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  40%|      | 6/15 [03:05<04:48, 32.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:22:36,198] Trial 6 finished with value: 0.909142632349273 and parameters: {'n_estimators': 126, 'max_depth': 3, 'learning_rate': 0.11679817513130797, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'gamma': 9.149877525022172e-05}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  47%|     | 7/15 [03:28<03:13, 24.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:22:58,569] Trial 7 finished with value: 0.7659710875806484 and parameters: {'n_estimators': 58, 'max_depth': 12, 'learning_rate': 0.01646379567211809, 'subsample': 0.8650089137415928, 'colsample_bytree': 0.7246844304357644, 'gamma': 0.00014472520367197597}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  53%|    | 8/15 [03:37<02:45, 23.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:23:07,769] Trial 8 finished with value: 0.9134719551686679 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.43464957555697725, 'subsample': 0.9100531293444458, 'colsample_bytree': 0.9757995766256756, 'gamma': 0.14408501080722544}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  60%|    | 9/15 [04:42<01:54, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:24:12,551] Trial 9 finished with value: 0.8332102676585844 and parameters: {'n_estimators': 200, 'max_depth': 12, 'learning_rate': 0.007515450322528414, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'gamma': 4.005370050283172e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  67%|   | 10/15 [05:29<02:46, 33.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:24:59,969] Trial 10 finished with value: 0.9143035711477321 and parameters: {'n_estimators': 287, 'max_depth': 7, 'learning_rate': 0.03621799474202481, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.8391524267229545, 'gamma': 0.0033264162114920023}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  73%|  | 11/15 [05:43<02:30, 37.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:25:13,337] Trial 11 finished with value: 0.9243616144942959 and parameters: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.22955406185548316, 'subsample': 0.7518416973680894, 'colsample_bytree': 0.7248996679248748, 'gamma': 1.3465901496770342e-07}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  80%|  | 12/15 [05:53<01:30, 30.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:25:23,746] Trial 12 finished with value: 0.9179095040995241 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.32976584052032165, 'subsample': 0.7273145000499233, 'colsample_bytree': 0.7624979833916741, 'gamma': 1.307420395434413e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  87%| | 13/15 [06:05<00:48, 24.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:25:36,054] Trial 13 finished with value: 0.9265312829611185 and parameters: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531:  93%|| 14/15 [06:22<00:20, 20.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:25:52,978] Trial 14 finished with value: 0.8645482549698402 and parameters: {'n_estimators': 78, 'max_depth': 6, 'learning_rate': 0.04374146076402921, 'subsample': 0.6041792656687146, 'colsample_bytree': 0.8955427636018558, 'gamma': 9.146410590181663e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531: 100%|| 15/15 [06:23<00:00, 25.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 19:25:53,285 - INFO - Best parameters for XGB: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}\n",
      "2025-07-31 19:25:53,287 - INFO - Best cross-validated F1-score for XGB: 0.9265\n",
      "2025-07-31 19:25:53,288 - INFO - Tuning SVC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-31 19:25:53,290] A new study created in memory with name: SVC_tuning_study\n",
      "  0%|          | 0/15 [04:47<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:30:40,911] Trial 0 finished with value: 0.48185640015644654 and parameters: {'C': 0.31489116479568624, 'gamma': 6.351221010640703, 'kernel': 'rbf'}. Best is trial 0 with value: 0.48185640015644654.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.481856:   7%|         | 1/15 [11:20<1:07:10, 287.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:37:13,868] Trial 1 finished with value: 0.8040040979459049 and parameters: {'C': 0.04207988669606638, 'gamma': 0.004207053950287938, 'kernel': 'sigmoid'}. Best is trial 1 with value: 0.8040040979459049.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.804004:  13%|        | 2/15 [11:47<1:15:46, 349.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:37:41,022] Trial 2 finished with value: 0.9221662225082857 and parameters: {'C': 2.5378155082656657, 'gamma': 0.679657809075816, 'kernel': 'sigmoid'}. Best is trial 2 with value: 0.9221662225082857.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.922166:  20%|        | 3/15 [12:54<40:29, 202.43s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:38:48,164] Trial 3 finished with value: 0.9274468578122226 and parameters: {'C': 21.368329072358772, 'gamma': 0.0070689749506246055, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  27%|       | 4/15 [14:41<27:19, 149.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:40:35,088] Trial 4 finished with value: 0.9000954051681628 and parameters: {'C': 0.1648044642797898, 'gamma': 0.12561043700013558, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  33%|      | 5/15 [18:22<22:18, 133.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:44:15,714] Trial 5 finished with value: 0.8466772244638058 and parameters: {'C': 2.801635158716261, 'gamma': 0.003613894271216527, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  40%|      | 6/15 [18:58<24:30, 163.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:44:52,123] Trial 6 finished with value: 0.8754548416250257 and parameters: {'C': 0.6672367170464207, 'gamma': 1.382623217936987, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  47%|     | 7/15 [22:38<16:14, 121.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:48:31,802] Trial 7 finished with value: 0.839863591318843 and parameters: {'C': 2.342384984711291, 'gamma': 0.0015339162591163618, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  53%|    | 8/15 [28:01<17:50, 153.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:53:54,938] Trial 8 finished with value: 0.08042653953868908 and parameters: {'C': 0.018205657658407266, 'gamma': 6.245139574743075, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  60%|    | 9/15 [33:43<20:37, 206.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 19:59:36,923] Trial 9 finished with value: 0.8047218721060352 and parameters: {'C': 0.1653693718282443, 'gamma': 0.002458603276328005, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  67%|   | 10/15 [34:10<20:40, 248.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:00:03,323] Trial 10 finished with value: 0.9415302848927896 and parameters: {'C': 60.33178530661243, 'gamma': 0.028504320627871515, 'kernel': 'sigmoid'}. Best is trial 10 with value: 0.9415302848927896.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 10. Best value: 0.94153:  73%|  | 11/15 [34:35<12:01, 180.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:00:28,976] Trial 11 finished with value: 0.9424880088028337 and parameters: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  80%|  | 12/15 [34:55<06:39, 133.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:00:49,082] Trial 12 finished with value: 0.9392053629128879 and parameters: {'C': 88.19429776626716, 'gamma': 0.03713740624438133, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  87%| | 13/15 [35:20<03:17, 98.98s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:13,319] Trial 13 finished with value: 0.9385834226508812 and parameters: {'C': 97.65296156943181, 'gamma': 0.025774482038992817, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  93%|| 14/15 [35:45<01:16, 76.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:38,086] Trial 14 finished with value: 0.9365904399909507 and parameters: {'C': 15.59864319752155, 'gamma': 0.13883438990307442, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488: 100%|| 15/15 [35:45<00:00, 143.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 20:01:38,378 - INFO - Best parameters for SVC: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}\n",
      "2025-07-31 20:01:38,380 - INFO - Best cross-validated F1-score for SVC: 0.9425\n",
      "2025-07-31 20:01:38,382 - INFO - Tuning ETC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-07-31 20:01:38,384] A new study created in memory with name: ETC_tuning_study\n",
      "  0%|          | 0/15 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:42,769] Trial 0 finished with value: 0.905345439765127 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:   7%|         | 1/15 [00:06<01:05,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:45,348] Trial 1 finished with value: 0.9033105221386787 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  13%|        | 2/15 [00:12<00:44,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:51,360] Trial 2 finished with value: 0.9048320961326024 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  20%|        | 3/15 [00:19<00:55,  4.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:01:57,838] Trial 3 finished with value: 0.9191989621106709 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  27%|       | 4/15 [00:23<00:59,  5.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:02,118] Trial 4 finished with value: 0.9037124762642886 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  33%|      | 5/15 [00:28<00:49,  4.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:06,948] Trial 5 finished with value: 0.9055210896400828 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  40%|      | 6/15 [00:33<00:44,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:12,189] Trial 6 finished with value: 0.8996331187012563 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  47%|     | 7/15 [00:38<00:40,  5.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:16,560] Trial 7 finished with value: 0.8992309400726045 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  53%|    | 8/15 [00:40<00:33,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:18,987] Trial 8 finished with value: 0.8987381998441906 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  60%|    | 9/15 [00:44<00:24,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:22,464] Trial 9 finished with value: 0.9065322904908435 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  67%|   | 10/15 [00:52<00:19,  3.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:30,826] Trial 10 finished with value: 0.9112263682741635 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  73%|  | 11/15 [01:00<00:20,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:39,024] Trial 11 finished with value: 0.907783505879283 and parameters: {'n_estimators': 290, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  80%|  | 12/15 [01:08<00:18,  6.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:47,376] Trial 12 finished with value: 0.9131610037820834 and parameters: {'n_estimators': 293, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  87%| | 13/15 [01:15<00:13,  6.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:02:54,332] Trial 13 finished with value: 0.9150964771322074 and parameters: {'n_estimators': 250, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  93%|| 14/15 [01:22<00:06,  6.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-07-31 20:03:00,892] Trial 14 finished with value: 0.9171971586951543 and parameters: {'n_estimators': 242, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199: 100%|| 15/15 [01:22<00:00,  5.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 20:03:01,224 - INFO - Best parameters for ETC: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-07-31 20:03:01,225 - INFO - Best cross-validated F1-score for ETC: 0.9192\n",
      "2025-07-31 20:03:01,226 - INFO - Hyperparameter tuning completed for all selected models.\n",
      "2025-07-31 20:03:01,228 - INFO - --- Completed Pipeline Step: Hyperparameter Tuning ---\n",
      "\n",
      "2025-07-31 20:03:01,229 - INFO - \n",
      "--- Starting Pipeline Step: Final Model Training & Evaluation ---\n",
      "2025-07-31 20:03:01,231 - INFO - Applying SMOTE to the entire training data for final model training...\n",
      "2025-07-31 20:03:01,329 - INFO - SMOTE applied. Original train: 4135 samples. Resampled train: 7226 samples.\n",
      "2025-07-31 20:03:01,331 - INFO - Training final LR model on resampled data and evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-31 20:03:02,759 - INFO - \n",
      "--- Performance for LR ---\n",
      "2025-07-31 20:03:02,760 - INFO - Accuracy: 0.9787\n",
      "2025-07-31 20:03:02,762 - INFO - Precision (Spam): 0.8978\n",
      "2025-07-31 20:03:02,763 - INFO - Recall (Spam): 0.9389\n",
      "2025-07-31 20:03:02,763 - INFO - F1-Score (Spam): 0.9179\n",
      "2025-07-31 20:03:02,778 - INFO - \n",
      "Full Classification Report for LR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.90      0.94      0.92       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.94      0.96      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:03:02,783 - INFO - \n",
      "Raw Confusion Matrix for LR:\n",
      "[[889  14]\n",
      " [  8 123]]\n",
      "2025-07-31 20:03:02,964 - INFO - Confusion matrix plot for LR saved to /home/dev/spam_classifier_project/plots/confusion_matrix_LR_20250731_200301.png.\n",
      "2025-07-31 20:03:02,965 - INFO - Training final RF model on resampled data and evaluating...\n",
      "2025-07-31 20:03:11,112 - INFO - \n",
      "--- Performance for RF ---\n",
      "2025-07-31 20:03:11,114 - INFO - Accuracy: 0.9826\n",
      "2025-07-31 20:03:11,116 - INFO - Precision (Spam): 0.9593\n",
      "2025-07-31 20:03:11,117 - INFO - Recall (Spam): 0.9008\n",
      "2025-07-31 20:03:11,121 - INFO - F1-Score (Spam): 0.9291\n",
      "2025-07-31 20:03:11,158 - INFO - \n",
      "Full Classification Report for RF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.96      0.90      0.93       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.95      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:03:11,167 - INFO - \n",
      "Raw Confusion Matrix for RF:\n",
      "[[898   5]\n",
      " [ 13 118]]\n",
      "2025-07-31 20:03:11,475 - INFO - Confusion matrix plot for RF saved to /home/dev/spam_classifier_project/plots/confusion_matrix_RF_20250731_200301.png.\n",
      "2025-07-31 20:03:11,476 - INFO - Training final XGB model on resampled data and evaluating...\n",
      "2025-07-31 20:03:21,578 - INFO - \n",
      "--- Performance for XGB ---\n",
      "2025-07-31 20:03:21,580 - INFO - Accuracy: 0.9845\n",
      "2025-07-31 20:03:21,581 - INFO - Precision (Spam): 0.9389\n",
      "2025-07-31 20:03:21,582 - INFO - Recall (Spam): 0.9389\n",
      "2025-07-31 20:03:21,582 - INFO - F1-Score (Spam): 0.9389\n",
      "2025-07-31 20:03:21,597 - INFO - \n",
      "Full Classification Report for XGB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:03:21,601 - INFO - \n",
      "Raw Confusion Matrix for XGB:\n",
      "[[895   8]\n",
      " [  8 123]]\n",
      "2025-07-31 20:03:21,751 - INFO - Confusion matrix plot for XGB saved to /home/dev/spam_classifier_project/plots/confusion_matrix_XGB_20250731_200301.png.\n",
      "2025-07-31 20:03:21,752 - INFO - Training final SVC model on resampled data and evaluating...\n",
      "2025-07-31 20:03:31,367 - INFO - \n",
      "--- Performance for SVC ---\n",
      "2025-07-31 20:03:31,368 - INFO - Accuracy: 0.9855\n",
      "2025-07-31 20:03:31,369 - INFO - Precision (Spam): 0.9394\n",
      "2025-07-31 20:03:31,370 - INFO - Recall (Spam): 0.9466\n",
      "2025-07-31 20:03:31,372 - INFO - F1-Score (Spam): 0.9430\n",
      "2025-07-31 20:03:31,384 - INFO - \n",
      "Full Classification Report for SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.95      0.94       131\n",
      "\n",
      "    accuracy                           0.99      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.99      0.99      0.99      1034\n",
      "\n",
      "2025-07-31 20:03:31,388 - INFO - \n",
      "Raw Confusion Matrix for SVC:\n",
      "[[895   8]\n",
      " [  7 124]]\n",
      "2025-07-31 20:03:31,562 - INFO - Confusion matrix plot for SVC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_SVC_20250731_200301.png.\n",
      "2025-07-31 20:03:31,563 - INFO - Training final KN model on resampled data and evaluating...\n",
      "2025-07-31 20:03:31,790 - INFO - \n",
      "--- Performance for KN ---\n",
      "2025-07-31 20:03:31,791 - INFO - Accuracy: 0.7611\n",
      "2025-07-31 20:03:31,792 - INFO - Precision (Spam): 0.3466\n",
      "2025-07-31 20:03:31,793 - INFO - Recall (Spam): 1.0000\n",
      "2025-07-31 20:03:31,794 - INFO - F1-Score (Spam): 0.5147\n",
      "2025-07-31 20:03:31,807 - INFO - \n",
      "Full Classification Report for KN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.73      0.84       903\n",
      "        spam       0.35      1.00      0.51       131\n",
      "\n",
      "    accuracy                           0.76      1034\n",
      "   macro avg       0.67      0.86      0.68      1034\n",
      "weighted avg       0.92      0.76      0.80      1034\n",
      "\n",
      "2025-07-31 20:03:31,810 - INFO - \n",
      "Raw Confusion Matrix for KN:\n",
      "[[656 247]\n",
      " [  0 131]]\n",
      "2025-07-31 20:03:32,010 - INFO - Confusion matrix plot for KN saved to /home/dev/spam_classifier_project/plots/confusion_matrix_KN_20250731_200301.png.\n",
      "2025-07-31 20:03:32,011 - INFO - Training final AdaBoost model on resampled data and evaluating...\n",
      "2025-07-31 20:04:55,966 - INFO - \n",
      "--- Performance for AdaBoost ---\n",
      "2025-07-31 20:04:55,967 - INFO - Accuracy: 0.9758\n",
      "2025-07-31 20:04:55,968 - INFO - Precision (Spam): 0.8681\n",
      "2025-07-31 20:04:55,969 - INFO - Recall (Spam): 0.9542\n",
      "2025-07-31 20:04:55,970 - INFO - F1-Score (Spam): 0.9091\n",
      "2025-07-31 20:04:55,984 - INFO - \n",
      "Full Classification Report for AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.87      0.95      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.93      0.97      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:04:55,987 - INFO - \n",
      "Raw Confusion Matrix for AdaBoost:\n",
      "[[884  19]\n",
      " [  6 125]]\n",
      "2025-07-31 20:04:56,155 - INFO - Confusion matrix plot for AdaBoost saved to /home/dev/spam_classifier_project/plots/confusion_matrix_AdaBoost_20250731_200301.png.\n",
      "2025-07-31 20:04:56,156 - INFO - Training final BgC model on resampled data and evaluating...\n",
      "2025-07-31 20:07:39,244 - INFO - \n",
      "--- Performance for BgC ---\n",
      "2025-07-31 20:07:39,247 - INFO - Accuracy: 0.9787\n",
      "2025-07-31 20:07:39,249 - INFO - Precision (Spam): 0.9291\n",
      "2025-07-31 20:07:39,251 - INFO - Recall (Spam): 0.9008\n",
      "2025-07-31 20:07:39,254 - INFO - F1-Score (Spam): 0.9147\n",
      "2025-07-31 20:07:39,291 - INFO - \n",
      "Full Classification Report for BgC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.90      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.95      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:07:39,299 - INFO - \n",
      "Raw Confusion Matrix for BgC:\n",
      "[[894   9]\n",
      " [ 13 118]]\n",
      "2025-07-31 20:07:39,581 - INFO - Confusion matrix plot for BgC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_BgC_20250731_200301.png.\n",
      "2025-07-31 20:07:39,582 - INFO - Training final ETC model on resampled data and evaluating...\n",
      "2025-07-31 20:07:41,017 - INFO - \n",
      "--- Performance for ETC ---\n",
      "2025-07-31 20:07:41,018 - INFO - Accuracy: 0.9749\n",
      "2025-07-31 20:07:41,020 - INFO - Precision (Spam): 0.9200\n",
      "2025-07-31 20:07:41,022 - INFO - Recall (Spam): 0.8779\n",
      "2025-07-31 20:07:41,023 - INFO - F1-Score (Spam): 0.8984\n",
      "2025-07-31 20:07:41,049 - INFO - \n",
      "Full Classification Report for ETC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.99       903\n",
      "        spam       0.92      0.88      0.90       131\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.95      0.93      0.94      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n",
      "2025-07-31 20:07:41,056 - INFO - \n",
      "Raw Confusion Matrix for ETC:\n",
      "[[893  10]\n",
      " [ 16 115]]\n",
      "2025-07-31 20:07:41,313 - INFO - Confusion matrix plot for ETC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_ETC_20250731_200301.png.\n",
      "2025-07-31 20:07:41,314 - INFO - Training final GBDT model on resampled data and evaluating...\n",
      "2025-07-31 20:11:32,833 - INFO - \n",
      "--- Performance for GBDT ---\n",
      "2025-07-31 20:11:32,835 - INFO - Accuracy: 0.9836\n",
      "2025-07-31 20:11:32,836 - INFO - Precision (Spam): 0.9318\n",
      "2025-07-31 20:11:32,837 - INFO - Recall (Spam): 0.9389\n",
      "2025-07-31 20:11:32,838 - INFO - F1-Score (Spam): 0.9354\n",
      "2025-07-31 20:11:32,851 - INFO - \n",
      "Full Classification Report for GBDT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.96      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-07-31 20:11:32,854 - INFO - \n",
      "Raw Confusion Matrix for GBDT:\n",
      "[[894   9]\n",
      " [  8 123]]\n",
      "2025-07-31 20:11:33,026 - INFO - Confusion matrix plot for GBDT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_GBDT_20250731_200301.png.\n",
      "2025-07-31 20:11:33,026 - INFO - Training final DT model on resampled data and evaluating...\n",
      "2025-07-31 20:11:36,940 - INFO - \n",
      "--- Performance for DT ---\n",
      "2025-07-31 20:11:36,941 - INFO - Accuracy: 0.9381\n",
      "2025-07-31 20:11:36,942 - INFO - Precision (Spam): 0.6982\n",
      "2025-07-31 20:11:36,942 - INFO - Recall (Spam): 0.9008\n",
      "2025-07-31 20:11:36,943 - INFO - F1-Score (Spam): 0.7867\n",
      "2025-07-31 20:11:36,956 - INFO - \n",
      "Full Classification Report for DT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.94      0.96       903\n",
      "        spam       0.70      0.90      0.79       131\n",
      "\n",
      "    accuracy                           0.94      1034\n",
      "   macro avg       0.84      0.92      0.88      1034\n",
      "weighted avg       0.95      0.94      0.94      1034\n",
      "\n",
      "2025-07-31 20:11:36,959 - INFO - \n",
      "Raw Confusion Matrix for DT:\n",
      "[[852  51]\n",
      " [ 13 118]]\n",
      "2025-07-31 20:11:37,116 - INFO - Confusion matrix plot for DT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_DT_20250731_200301.png.\n",
      "2025-07-31 20:11:37,119 - INFO - \n",
      "--- Overall Best Model Identified: SVC (F1-Score on Spam: 0.9430) ---\n",
      "2025-07-31 20:11:37,120 - INFO - All model evaluations completed.\n",
      "2025-07-31 20:11:37,125 - INFO - Best performing model (SVC) saved to /home/dev/spam_classifier_project/models/best_model_SVC_20250731_201137.pkl\n",
      "2025-07-31 20:11:37,759 - INFO - Model performance comparison plot saved to /home/dev/spam_classifier_project/plots/model_performance_comparison_20250731_200301.png.\n",
      "2025-07-31 20:11:37,760 - INFO - --- Completed Pipeline Step: Final Model Training & Evaluation ---\n",
      "\n",
      "2025-07-31 20:11:37,761 - INFO - Spam classification pipeline completed successfully.\n",
      "2025-07-31 20:11:37,762 - INFO - \n",
      "=== Spam Classification Pipeline Completed Successfully ===\n",
      "2025-07-31 20:11:37,763 - INFO - Overall Model Performance Summary (Sorted by F1-Score on Spam):\n",
      "      Model  Accuracy  Precision (Spam)  Recall (Spam)  F1-Score (Spam)\n",
      "0       SVC  0.985493          0.939394       0.946565         0.942966\n",
      "1       XGB  0.984526          0.938931       0.938931         0.938931\n",
      "2      GBDT  0.983559          0.931818       0.938931         0.935361\n",
      "3        RF  0.982592          0.959350       0.900763         0.929134\n",
      "4        LR  0.978723          0.897810       0.938931         0.917910\n",
      "5       BgC  0.978723          0.929134       0.900763         0.914729\n",
      "6  AdaBoost  0.975822          0.868056       0.954198         0.909091\n",
      "7       ETC  0.974855          0.920000       0.877863         0.898438\n",
      "8        DT  0.938104          0.698225       0.900763         0.786667\n",
      "9        KN  0.761122          0.346561       1.000000         0.514735\n",
      "2025-07-31 20:11:37,769 - INFO - \n",
      "Best Performing Model Identified: SVC\n",
      "2025-07-31 20:11:37,770 - INFO - Check '/home/dev/spam_classifier_project/plots' for EDA and Confusion Matrix plots.\n",
      "2025-07-31 20:11:37,771 - INFO - Check '/home/dev/spam_classifier_project/models' for the saved best model.\n",
      "2025-07-31 20:11:37,772 - INFO - \n",
      "--- Demonstrating Model Inference from Saved Model ---\n",
      "2025-07-31 20:11:37,773 - INFO - Attempting to load the latest best model from: /home/dev/spam_classifier_project/models/best_model_SVC_20250731_201137.pkl\n",
      "2025-07-31 20:11:37,777 - INFO - NLTK punkt resource found.\n",
      "2025-07-31 20:11:37,779 - INFO - NLTK stopwords resource found.\n",
      "2025-07-31 20:11:37,780 - INFO - NLTK punkt_tab resource found.\n",
      "2025-07-31 20:11:37,783 - INFO - SpamClassifier initialized successfully.\n",
      "2025-07-31 20:11:37,785 - INFO - Loading SentenceTransformer by name: 'all-MiniLM-L6-v2'\n",
      "2025-07-31 20:11:37,788 - INFO - Use pytorch device_name: cpu\n",
      "2025-07-31 20:11:37,789 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-07-31 20:11:45,210 - INFO - Model 'SVC' loaded successfully from /home/dev/spam_classifier_project/models/best_model_SVC_20250731_201137.pkl for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00,  9.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prediction for SPAM text 1: 'WINNER! You have been selected for a 1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.' -> spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 16.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for SPAM text 2: 'URGENT! Your bank account has been locked due to suspicious activity. Verify immediately at http://bit.ly/malicious-site to avoid closure.' -> ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 23.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for HAM text 1: 'Hey, just checking in. How are you doing today? Let's catch up soon for coffee!' -> ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 52.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for HAM text 2: 'Hi mom, can you pick up milk and bread on your way home? Thanks, love you!' -> ham\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 48.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for EMPTY/NOISY text: '???!!!#@%' -> spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Spam Classification Pipeline with EDA, Text Preprocessing, and Model Training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                             BaggingClassifier, ExtraTreesClassifier,\n",
    "                             GradientBoostingClassifier, VotingClassifier,\n",
    "                             StackingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Determine Base Directory for Notebook/Script ---\n",
    "try:\n",
    "    current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    base_directory = current_script_dir\n",
    "    print(f\"Running as a script. Base directory set to: '{base_directory}'\")\n",
    "except NameError:\n",
    "    base_directory = os.getcwd()\n",
    "    print(f\"Running in a notebook environment. Base directory set to CWD: '{base_directory}'\")\n",
    "\n",
    "\n",
    "# --- Configuration (Externalize for production) ---\n",
    "class Config:\n",
    "    DATA_PATH = os.path.join(base_directory, 'spam.csv')\n",
    "    SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n",
    "    LOG_FILE = os.path.join(base_directory, 'spam_classifier.log')\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    N_TRIALS_OPTUNA = 15\n",
    "    PLOTS_DIR = os.path.join(base_directory, 'plots')\n",
    "    MODELS_DIR = os.path.join(base_directory, 'models')\n",
    "\n",
    "# Ensure plot and model directories exist at startup\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self._configure_logging()\n",
    "        self._verify_nltk_resources()\n",
    "        self._configure_matplotlib()\n",
    "        self.df = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.sentence_transformer_model = None\n",
    "        self.X, self.y = None, None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None]*4\n",
    "        self.clfs = {}\n",
    "        self.best_tuned_models_params = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.performance_df = pd.DataFrame()\n",
    "        self._initialize_classifiers()\n",
    "        logging.info(\"SpamClassifier initialized successfully.\")\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(Config.LOG_FILE),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "    def _verify_nltk_resources(self) -> None:\n",
    "        resources = [\n",
    "            ('tokenizers/punkt', 'punkt'),\n",
    "            ('corpora/stopwords', 'stopwords'),\n",
    "            ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "        ]\n",
    "        for path, package in resources:\n",
    "            try:\n",
    "                nltk.data.find(path)\n",
    "                logging.info(f\"NLTK {package} resource found.\")\n",
    "            except LookupError:\n",
    "                logging.warning(f\"NLTK {package} not found. Attempting to download...\")\n",
    "                try:\n",
    "                    nltk.download(package, quiet=True)\n",
    "                    logging.info(f\"NLTK {package} downloaded successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.critical(f\"Failed to download NLTK {package}. Error: {e}\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _configure_matplotlib(self) -> None:\n",
    "        plt.ioff()\n",
    "        sns.set(style='whitegrid', palette='viridis')\n",
    "\n",
    "    def _initialize_classifiers(self) -> None:\n",
    "        self.clfs = {\n",
    "            'LR': LogisticRegression(\n",
    "                solver='liblinear',\n",
    "                penalty='l1',\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'RF': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'XGB': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=1\n",
    "            ),\n",
    "            'SVC': SVC(kernel='sigmoid', gamma=1.0, probability=True, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "            'KN': KNeighborsClassifier(),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'BgC': BaggingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1),\n",
    "            'ETC': ExtraTreesClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "            'GBDT': GradientBoostingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'DT': DecisionTreeClassifier(max_depth=5, random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        }\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        try:\n",
    "            if not os.path.exists(Config.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {os.path.abspath(Config.DATA_PATH)}\")\n",
    "            self.df = pd.read_csv(Config.DATA_PATH, encoding='latin-1')\n",
    "            if len(self.df) < 100:\n",
    "                raise ValueError(f\"Dataset too small ({len(self.df)} samples). Minimum 100 samples required for robust analysis.\")\n",
    "            logging.info(f\"Loaded {len(self.df)} records from {Config.DATA_PATH}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data loading failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def clean_data(self) -> None:\n",
    "        try:\n",
    "            if 'v1' in self.df.columns and 'v2' in self.df.columns:\n",
    "                self.df = self.df[['v1', 'v2']].copy()\n",
    "                logging.info(\"Selected 'v1' and 'v2' columns from the dataset.\")\n",
    "            else:\n",
    "                found_v1 = next((col for col in self.df.columns if 'target' in col.lower() or 'label' in col.lower() or 'type' in col.lower()), None)\n",
    "                found_v2 = next((col for col in self.df.columns if 'text' in col.lower() or 'message' in col.lower() or 'sms' in col.lower()), None)\n",
    "                if found_v1 and found_v2:\n",
    "                    self.df = self.df[[found_v1, found_v2]].copy()\n",
    "                    logging.info(f\"Mapped columns '{found_v1}' to 'target' and '{found_v2}' to 'text' using heuristics.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find required 'target' and 'text' columns (v1/v2 or equivalents) in dataset. Found columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "            self.df.columns = ['target', 'text']\n",
    "            valid_targets = {'ham', 'spam'}\n",
    "            invalid_targets = set(self.df['target'].unique()) - valid_targets\n",
    "            if invalid_targets:\n",
    "                logging.warning(f\"Invalid target values found: {invalid_targets}. Filtering out rows with these values.\")\n",
    "                self.df = self.df[self.df['target'].isin(valid_targets)]\n",
    "                if self.df.empty:\n",
    "                    raise ValueError(\"No valid 'ham' or 'spam' records remaining after filtering invalid targets. Dataset is empty.\")\n",
    "\n",
    "            self.df['target'] = self.encoder.fit_transform(self.df['target'])\n",
    "            initial_rows = len(self.df)\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            self.df.dropna(inplace=True)\n",
    "\n",
    "            logging.info(f\"Cleaned dataset. Removed {initial_rows - len(self.df)} duplicates/nulls. Remaining: {len(self.df)} records.\")\n",
    "            if self.df.empty:\n",
    "                raise ValueError(\"Dataset became empty after cleaning steps. Check data quality or initial loading.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data cleaning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _safe_tokenize(self, text: str) -> list[str]:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for tokenization: {text[:50]}...\")\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "            return [t for t in tokens if t.isalnum() and t not in string.punctuation]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Tokenization failed for text (first 50 chars: '{text[:50]}...'). Returning empty list. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def eda(self) -> None:\n",
    "        try:\n",
    "            self.df['num_words'] = self.df['text'].apply(lambda x: len(self._safe_tokenize(x)))\n",
    "            self.df['num_chars'] = self.df['text'].apply(len)\n",
    "            self.df['num_sentences'] = self.df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "\n",
    "            ham_count = self.df[self.df['target'] == self.encoder.transform(['ham'])[0]].shape[0]\n",
    "            spam_count = self.df[self.df['target'] == self.encoder.transform(['spam'])[0]].shape[0]\n",
    "            if spam_count > 0:\n",
    "                scale_pos_weight_val = ham_count / spam_count\n",
    "                self.clfs['XGB'].set_params(scale_pos_weight=scale_pos_weight_val)\n",
    "                logging.info(f\"Set XGBoost scale_pos_weight to: {scale_pos_weight_val:.2f} (Ham:{ham_count}, Spam:{spam_count})\")\n",
    "            else:\n",
    "                logging.warning(\"No spam samples found to calculate scale_pos_weight for XGBoost. Defaulting to 1.\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "            self.df['target'].value_counts().plot(\n",
    "                kind='pie', ax=ax1, autopct='%1.1f%%',\n",
    "                labels=self.encoder.inverse_transform(self.df['target'].value_counts().index),\n",
    "                colors=sns.color_palette('pastel')[0:2],\n",
    "                explode=[0, 0.1]\n",
    "            )\n",
    "            ax1.set_title('Target Class Distribution')\n",
    "            ax1.set_ylabel('')\n",
    "            fig1_filename = os.path.join(Config.PLOTS_DIR, f'target_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig1_filename, bbox_inches='tight')\n",
    "            plt.close(fig1)\n",
    "            logging.info(f\"Target distribution plot saved to {fig1_filename}.\")\n",
    "\n",
    "            fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax2.set_title('Word Count Distribution by Target Class')\n",
    "            ax2.set_xlabel('Number of Words')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.legend()\n",
    "            fig2_filename = os.path.join(Config.PLOTS_DIR, f'word_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig2_filename, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logging.info(f\"Word count distribution plot saved to {fig2_filename}.\")\n",
    "\n",
    "            fig3, ax3 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax3.set_title('Character Count Distribution by Target Class')\n",
    "            ax3.set_xlabel('Number of Characters')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.legend()\n",
    "            fig3_filename = os.path.join(Config.PLOTS_DIR, f'char_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig3_filename, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logging.info(f\"Character count distribution plot saved to {fig3_filename}.\")\n",
    "\n",
    "            fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(self.df[['num_chars', 'num_words', 'num_sentences', 'target']].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
    "            ax4.set_title('Correlation Matrix of Text Features and Target')\n",
    "            fig4_filename = os.path.join(Config.PLOTS_DIR, f'correlation_heatmap_{timestamp}.png')\n",
    "            plt.savefig(fig4_filename, bbox_inches='tight')\n",
    "            plt.close(fig4)\n",
    "            logging.info(f\"Correlation heatmap plot saved to {fig4_filename}.\")\n",
    "\n",
    "            logging.info(f\"Descriptive statistics for Ham emails:\\n{self.df[self.df['target'] == self.encoder.transform(['ham'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "            logging.info(f\"Descriptive statistics for Spam emails:\\n{self.df[self.df['target'] == self.encoder.transform(['spam'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EDA process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform_text(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for transform_text: {text[:50]}...\")\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        processed_tokens = [token for token in tokens if token.isalnum()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in processed_tokens if token not in stop_words and token not in string.punctuation]\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "        final_tokens = [token for token in stemmed_tokens if len(token) > 1 or token.isdigit()]\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def preprocess_text(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"\\n--- Text Preprocessing for EDA and Visualizations ---\")\n",
    "            self.df['transformed_text'] = self.df['text'].apply(self.transform_text)\n",
    "            logging.info(\"Text transformation for EDA complete. Example:\")\n",
    "            logging.info(f\"\\n{self.df[['text', 'transformed_text']].head().to_string()}\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            logging.info(\"\\nGenerating Word Clouds (saved to plots directory):\")\n",
    "            spam_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(spam_wc)\n",
    "            plt.title('Spam Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_spam_filename = os.path.join(Config.PLOTS_DIR, f'spam_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_spam_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Spam word cloud saved to {wc_spam_filename}.\")\n",
    "\n",
    "            ham_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(ham_wc)\n",
    "            plt.title('Ham Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_ham_filename = os.path.join(Config.PLOTS_DIR, f'ham_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_ham_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Ham word cloud saved to {wc_ham_filename}.\")\n",
    "\n",
    "            logging.info(\"\\nMost common words in Spam (saved as plot):\")\n",
    "            spam_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(spam_corpus, title='Top 30 Spam Words', filename=f'top_spam_words_{timestamp}.png')\n",
    "\n",
    "            logging.info(\"\\nMost common words in Ham (saved as plot):\")\n",
    "            ham_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(ham_corpus, title='Top 30 Ham Words', filename=f'top_ham_words_{timestamp}.png')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text preprocessing for EDA failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _plot_most_common_words(self, corpus: list[str], title: str, n: int = 30, filename: str = \"common_words.png\") -> None:\n",
    "        common_words = Counter(corpus).most_common(n)\n",
    "        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(x='Word', y='Count', data=df_common_words, ax=ax, palette='viridis')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "        ax.set_title(title)\n",
    "        plot_filepath = os.path.join(Config.PLOTS_DIR, filename)\n",
    "        plt.savefig(plot_filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Plot '{title}' saved to {plot_filepath}.\")\n",
    "\n",
    "    def vectorize_text_with_embeddings(self) -> None:\n",
    "        try:\n",
    "            logging.info(f\"\\n--- Text Vectorization (SentenceTransformer: {Config.SENTENCE_TRANSFORMER_MODEL}) ---\")\n",
    "            if self.sentence_transformer_model is None:\n",
    "                self.sentence_transformer_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "            self.X = self.sentence_transformer_model.encode(\n",
    "                self.df['text'].tolist(),\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                batch_size=64\n",
    "            )\n",
    "            self.y = self.df['target'].values\n",
    "            logging.info(f\"SentenceTransformer embedding complete. X shape: {self.X.shape}, Y shape: {self.y.shape}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text vectorization failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=Config.TEST_SIZE,\n",
    "                random_state=Config.RANDOM_STATE, stratify=self.y)\n",
    "            logging.info(f\"Data split: Train {len(self.X_train)} samples, Test {len(self.X_test)} samples.\")\n",
    "            logging.info(f\"Train target distribution: {np.bincount(self.y_train)}\")\n",
    "            logging.info(f\"Test target distribution: {np.bincount(self.y_test)}\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data splitting failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _objective(self, trial: optuna.trial.Trial, model_name: str) -> float:\n",
    "        if model_name == 'LR':\n",
    "            c_param = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "            solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            model = LogisticRegression(C=c_param, solver=solver, random_state=Config.RANDOM_STATE,\n",
    "                                       class_weight='balanced', max_iter=2000,\n",
    "                                       n_jobs=-1 if solver == 'saga' else None)\n",
    "        elif model_name == 'RF':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'XGB':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.005, 0.5)\n",
    "            subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "            gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "            current_scale_pos_weight = self.clfs['XGB'].get_params().get('scale_pos_weight', 1)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate, subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree, gamma=gamma,\n",
    "                                  random_state=Config.RANDOM_STATE,\n",
    "                                  eval_metric='logloss',\n",
    "                                  scale_pos_weight=current_scale_pos_weight)\n",
    "        elif model_name == 'SVC':\n",
    "            C_param = trial.suggest_loguniform('C', 1e-2, 1e2)\n",
    "            gamma_param = trial.suggest_loguniform('gamma', 1e-3, 1e1)\n",
    "            kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n",
    "            model = SVC(C=C_param, gamma=gamma_param, kernel=kernel, probability=True,\n",
    "                        random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        elif model_name == 'KN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, n_jobs=-1)\n",
    "        elif model_name == 'AdaBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'BgC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            model = BaggingClassifier(n_estimators=n_estimators, random_state=Config.RANDOM_STATE, n_jobs=-1)\n",
    "        elif model_name == 'ETC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'GBDT':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'DT':\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf, criterion=criterion,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' is not configured for Optuna tuning.\")\n",
    "\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "        scores = cross_val_score(pipeline, self.X_train, self.y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    def tune_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.y_train is None:\n",
    "                logging.error(\"Data not split for tuning. Calling split_data().\")\n",
    "                self.split_data()\n",
    "\n",
    "            logging.info(\"Starting hyperparameter tuning with Optuna for selected models...\")\n",
    "            models_to_tune = ['LR', 'RF', 'XGB', 'SVC', 'ETC']\n",
    "\n",
    "            for name in models_to_tune:\n",
    "                if name not in self.clfs:\n",
    "                    logging.warning(f\"Model '{name}' not found in initialized classifiers, skipping tuning.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Tuning {name} model with {Config.N_TRIALS_OPTUNA} trials...\")\n",
    "                study = optuna.create_study(direction='maximize',\n",
    "                                            sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_STATE),\n",
    "                                            study_name=f\"{name}_tuning_study\")\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    study.optimize(lambda trial: self._objective(trial, name),\n",
    "                                   n_trials=Config.N_TRIALS_OPTUNA,\n",
    "                                   show_progress_bar=True,\n",
    "                                   gc_after_trial=True)\n",
    "\n",
    "                self.best_tuned_models_params[name] = study.best_trial.params\n",
    "                logging.info(f\"Best parameters for {name}: {study.best_trial.params}\")\n",
    "                logging.info(f\"Best cross-validated F1-score for {name}: {study.best_trial.value:.4f}\")\n",
    "\n",
    "                self.clfs[name].set_params(**study.best_trial.params)\n",
    "                if name == 'XGB':\n",
    "                    current_scale_pos_weight = self.clfs[name].get_params().get('scale_pos_weight', 1)\n",
    "                    self.clfs[name].set_params(scale_pos_weight=current_scale_pos_weight)\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning completed for all selected models.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Model tuning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def train_final_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.X_test is None:\n",
    "                 logging.error(\"Data not split for final training. Calling split_data().\")\n",
    "                 self.split_data()\n",
    "\n",
    "            logging.info(\"Applying SMOTE to the entire training data for final model training...\")\n",
    "            smote = SMOTE(random_state=Config.RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
    "            logging.info(f\"SMOTE applied. Original train: {len(self.X_train)} samples. Resampled train: {len(X_train_resampled)} samples.\")\n",
    "\n",
    "            results = []\n",
    "            best_f1_overall = -1\n",
    "            self.best_model = None\n",
    "            self.best_model_name = None\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            for name, model in self.clfs.items():\n",
    "                logging.info(f\"Training final {name} model on resampled data and evaluating...\")\n",
    "                try:\n",
    "                    # To use the ImbPipeline, we need to pass the model, not just the classifier\n",
    "                    pipeline = ImbPipeline([('smote', SMOTE(random_state=Config.RANDOM_STATE)), ('classifier', model)])\n",
    "                    pipeline.fit(self.X_train, self.y_train)\n",
    "                    y_pred = pipeline.predict(self.X_test)\n",
    "\n",
    "                    accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                    precision = precision_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    recall = recall_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    f1 = f1_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "\n",
    "                    report_dict = classification_report(self.y_test, y_pred, target_names=self.encoder.classes_, output_dict=True)\n",
    "\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'Precision (Spam)': precision,\n",
    "                        'Recall (Spam)': recall,\n",
    "                        'F1-Score (Spam)': f1,\n",
    "                        'Full Classification Report': report_dict\n",
    "                    })\n",
    "\n",
    "                    logging.info(f\"\\n--- Performance for {name} ---\")\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    logging.info(f\"Precision (Spam): {precision:.4f}\")\n",
    "                    logging.info(f\"Recall (Spam): {recall:.4f}\")\n",
    "                    logging.info(f\"F1-Score (Spam): {f1:.4f}\")\n",
    "                    logging.info(f\"\\nFull Classification Report for {name}:\\n{classification_report(self.y_test, y_pred, target_names=self.encoder.classes_)}\")\n",
    "\n",
    "                    cm = confusion_matrix(self.y_test, y_pred)\n",
    "                    logging.info(f\"\\nRaw Confusion Matrix for {name}:\\n{cm}\")\n",
    "\n",
    "                    fig_cm, ax_cm = plt.subplots(figsize=(7, 6))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=self.encoder.classes_,\n",
    "                                yticklabels=self.encoder.classes_,\n",
    "                                linecolor='gray', linewidths=0.5,\n",
    "                                annot_kws={\"size\": 14})\n",
    "                    ax_cm.set_xlabel('Predicted Label', fontsize=12)\n",
    "                    ax_cm.set_ylabel('True Label', fontsize=12)\n",
    "                    ax_cm.set_title(f'Confusion Matrix for {name}', fontsize=14)\n",
    "                    cm_filename = os.path.join(Config.PLOTS_DIR, f'confusion_matrix_{name}_{timestamp}.png')\n",
    "                    plt.savefig(cm_filename, bbox_inches='tight')\n",
    "                    plt.close(fig_cm)\n",
    "                    logging.info(f\"Confusion matrix plot for {name} saved to {cm_filename}.\")\n",
    "\n",
    "                    if f1 > best_f1_overall:\n",
    "                        best_f1_overall = f1\n",
    "                        self.best_model_name = name\n",
    "                        self.best_model = pipeline # Store the entire pipeline\n",
    "                except Exception as model_e:\n",
    "                    logging.error(f\"Error training or evaluating model {name}: {model_e}\")\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision (Spam)': np.nan,\n",
    "                        'Recall (Spam)': np.nan,\n",
    "                        'F1-Score (Spam)': np.nan,\n",
    "                        'Full Classification Report': {'error': str(model_e)}\n",
    "                    })\n",
    "\n",
    "            self.performance_df = pd.DataFrame(results)\n",
    "            self.performance_df = self.performance_df.sort_values(by='F1-Score (Spam)', ascending=False).reset_index(drop=True)\n",
    "            logging.info(f\"\\n--- Overall Best Model Identified: {self.best_model_name} (F1-Score on Spam: {best_f1_overall:.4f}) ---\")\n",
    "            logging.info(\"All model evaluations completed.\")\n",
    "            self._save_best_model()\n",
    "            self._plot_performance_comparison(timestamp)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Final model training and evaluation failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _save_best_model(self) -> None:\n",
    "        \"\"\"Saves the best performing model and related components to a pickle file.\"\"\"\n",
    "        try:\n",
    "            if self.best_model is None or self.best_model_name is None:\n",
    "                logging.warning(\"No best model identified or stored. Skipping model save operation.\")\n",
    "                return\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = os.path.join(Config.MODELS_DIR, f'best_model_{self.best_model_name}_{timestamp}.pkl')\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.best_model,\n",
    "                    'transformer': Config.SENTENCE_TRANSFORMER_MODEL, # THIS IS THE FIX\n",
    "                    'encoder': self.encoder,\n",
    "                    'model_name': self.best_model_name,\n",
    "                    'performance_summary': self.performance_df.to_dict('records')\n",
    "                }, f)\n",
    "            logging.info(f\"Best performing model ({self.best_model_name}) saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save the best model: {e}\")\n",
    "\n",
    "    def _plot_performance_comparison(self, timestamp: str) -> None:\n",
    "        if self.performance_df.empty:\n",
    "            logging.warning(\"Performance DataFrame is empty, cannot plot comparison.\")\n",
    "            return\n",
    "        plot_df = self.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        sns.barplot(x='Model', y='Score', hue='Metric', data=plot_df_melted, palette='tab10', ax=ax)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title('Model Performance Comparison (Test Set)')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(Config.PLOTS_DIR, f'model_performance_comparison_{timestamp}.png')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Model performance comparison plot saved to {plot_filename}.\")\n",
    "\n",
    "    def run_pipeline(self) -> bool:\n",
    "        steps = [\n",
    "            ('Data Loading', self.load_data),\n",
    "            ('Data Cleaning', self.clean_data),\n",
    "            ('EDA and Feature Engineering', self.eda),\n",
    "            ('Text Preprocessing for EDA', self.preprocess_text),\n",
    "            ('Text Vectorization (Embeddings)', self.vectorize_text_with_embeddings),\n",
    "            ('Data Splitting', self.split_data),\n",
    "            ('Hyperparameter Tuning', self.tune_models),\n",
    "            ('Final Model Training & Evaluation', self.train_final_models)\n",
    "        ]\n",
    "        for name, step in steps:\n",
    "            try:\n",
    "                logging.info(f\"\\n--- Starting Pipeline Step: {name} ---\")\n",
    "                step()\n",
    "                logging.info(f\"--- Completed Pipeline Step: {name} ---\\n\")\n",
    "            except SystemExit:\n",
    "                logging.critical(f\"Pipeline stopped due to critical error in step: '{name}'.\")\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                logging.critical(f\"Pipeline failed unexpectedly in step '{name}': {e}\")\n",
    "                return False\n",
    "        logging.info(\"Spam classification pipeline completed successfully.\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def load_for_inference(model_path: str) -> 'SpamClassifier':\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found at {os.path.abspath(model_path)}\")\n",
    "            with open(model_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            classifier = SpamClassifier()\n",
    "            classifier.best_model = data['model']\n",
    "            classifier.encoder = data['encoder']\n",
    "            classifier.best_model_name = data.get('model_name', 'Unknown_Model')\n",
    "            transformer_data = data['transformer']\n",
    "            if isinstance(transformer_data, str):\n",
    "                logging.info(f\"Loading SentenceTransformer by name: '{transformer_data}'\")\n",
    "                classifier.sentence_transformer_model = SentenceTransformer(transformer_data)\n",
    "            else:\n",
    "                logging.warning(\"Loaded SentenceTransformer object directly from pickle.\")\n",
    "                classifier.sentence_transformer_model = transformer_data\n",
    "            classifier.ps = PorterStemmer()\n",
    "            logging.info(f\"Model '{classifier.best_model_name}' loaded successfully from {model_path} for inference.\")\n",
    "            return classifier\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to load model for inference from {model_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        if self.best_model is None or self.sentence_transformer_model is None or self.encoder is None:\n",
    "            logging.error(\"Model components not loaded. Please run run_pipeline() or load model using load_for_inference() before calling predict().\")\n",
    "            raise RuntimeError(\"Model components not available for prediction.\")\n",
    "        try:\n",
    "            vector = self.sentence_transformer_model.encode([text], convert_to_tensor=False)\n",
    "            prediction_encoded = self.best_model.predict(vector)[0]\n",
    "            prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n",
    "            return prediction_label\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction failed for text '{text[:50]}...': {e}\")\n",
    "            return \"error\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = SpamClassifier()\n",
    "    pipeline_success = classifier.run_pipeline()\n",
    "\n",
    "    if pipeline_success:\n",
    "        logging.info(\"\\n=== Spam Classification Pipeline Completed Successfully ===\")\n",
    "        logging.info(\"Overall Model Performance Summary (Sorted by F1-Score on Spam):\")\n",
    "        print(classifier.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].to_string())\n",
    "        logging.info(f\"\\nBest Performing Model Identified: {classifier.best_model_name}\")\n",
    "        logging.info(f\"Check '{Config.PLOTS_DIR}' for EDA and Confusion Matrix plots.\")\n",
    "        logging.info(f\"Check '{Config.MODELS_DIR}' for the saved best model.\")\n",
    "\n",
    "        try:\n",
    "            logging.info(\"\\n--- Demonstrating Model Inference from Saved Model ---\")\n",
    "            model_files = [f for f in os.listdir(Config.MODELS_DIR) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join(Config.MODELS_DIR, f)))\n",
    "                latest_model_path = os.path.join(Config.MODELS_DIR, latest_model_file)\n",
    "                logging.info(f\"Attempting to load the latest best model from: {latest_model_path}\")\n",
    "                loaded_classifier = SpamClassifier.load_for_inference(latest_model_path)\n",
    "                test_spam_text_1 = \"WINNER! You have been selected for a 1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.\"\n",
    "                test_spam_text_2 = \"URGENT! Your bank account has been locked due to suspicious activity. Verify immediately at http://bit.ly/malicious-site to avoid closure.\"\n",
    "                test_ham_text_1 = \"Hey, just checking in. How are you doing today? Let's catch up soon for coffee!\"\n",
    "                test_ham_text_2 = \"Hi mom, can you pick up milk and bread on your way home? Thanks, love you!\"\n",
    "                test_empty_text = \"???!!!#@%\"\n",
    "                print(f\"\\nPrediction for SPAM text 1: '{test_spam_text_1}' -> {loaded_classifier.predict(test_spam_text_1)}\")\n",
    "                print(f\"Prediction for SPAM text 2: '{test_spam_text_2}' -> {loaded_classifier.predict(test_spam_text_2)}\")\n",
    "                print(f\"Prediction for HAM text 1: '{test_ham_text_1}' -> {loaded_classifier.predict(test_ham_text_1)}\")\n",
    "                print(f\"Prediction for HAM text 2: '{test_ham_text_2}' -> {loaded_classifier.predict(test_ham_text_2)}\")\n",
    "                print(f\"Prediction for EMPTY/NOISY text: '{test_empty_text}' -> {loaded_classifier.predict(test_empty_text)}\")\n",
    "            else:\n",
    "                logging.warning(\"No model files found in the 'models' directory to demonstrate inference. Run the pipeline first.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the inference demonstration: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.critical(\"Spam classification pipeline failed during execution. Please review the log file for details.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b45039a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a notebook environment. Base directory set to CWD: '/home/dev/spam_classifier_project'\n",
      "2025-08-01 14:36:27,208 - INFO - NLTK punkt resource found.\n",
      "2025-08-01 14:36:27,209 - INFO - NLTK stopwords resource found.\n",
      "2025-08-01 14:36:27,210 - INFO - NLTK punkt_tab resource found.\n",
      "2025-08-01 14:36:27,212 - INFO - Initialized all individual and ensemble classifiers.\n",
      "2025-08-01 14:36:27,213 - INFO - SpamClassifier initialized successfully.\n",
      "2025-08-01 14:36:27,216 - INFO - \n",
      "--- Starting Pipeline Step: Data Loading ---\n",
      "2025-08-01 14:36:27,236 - INFO - Loaded 5572 records from /home/dev/spam_classifier_project/spam.csv.\n",
      "2025-08-01 14:36:27,237 - INFO - --- Completed Pipeline Step: Data Loading ---\n",
      "\n",
      "2025-08-01 14:36:27,239 - INFO - \n",
      "--- Starting Pipeline Step: Data Cleaning ---\n",
      "2025-08-01 14:36:27,241 - INFO - Selected 'v1' and 'v2' columns from the dataset.\n",
      "2025-08-01 14:36:27,256 - INFO - Cleaned dataset. Removed 403 duplicates/nulls. Remaining: 5169 records.\n",
      "2025-08-01 14:36:27,258 - INFO - --- Completed Pipeline Step: Data Cleaning ---\n",
      "\n",
      "2025-08-01 14:36:27,260 - INFO - \n",
      "--- Starting Pipeline Step: EDA and Feature Engineering ---\n",
      "2025-08-01 14:36:28,496 - INFO - Set XGBoost scale_pos_weight to: 6.92 (Ham:4516, Spam:653)\n",
      "2025-08-01 14:36:28,644 - INFO - Target distribution plot saved to /home/dev/spam_classifier_project/plots/target_distribution_20250801_143628.png.\n",
      "2025-08-01 14:36:29,408 - INFO - Word count distribution plot saved to /home/dev/spam_classifier_project/plots/word_count_distribution_20250801_143628.png.\n",
      "2025-08-01 14:36:30,134 - INFO - Character count distribution plot saved to /home/dev/spam_classifier_project/plots/char_count_distribution_20250801_143628.png.\n",
      "2025-08-01 14:36:30,536 - INFO - Correlation heatmap plot saved to /home/dev/spam_classifier_project/plots/correlation_heatmap_20250801_143628.png.\n",
      "2025-08-01 14:36:30,551 - INFO - Descriptive statistics for Ham emails:\n",
      "         num_chars    num_words  num_sentences\n",
      "count  4516.000000  4516.000000    4516.000000\n",
      "mean     70.459256    13.908769       1.820195\n",
      "std      56.358207    10.835081       1.383657\n",
      "min       2.000000     0.000000       1.000000\n",
      "25%      34.000000     7.000000       1.000000\n",
      "50%      52.000000    10.500000       1.000000\n",
      "75%      90.000000    18.000000       2.000000\n",
      "max     910.000000   162.000000      38.000000\n",
      "2025-08-01 14:36:30,565 - INFO - Descriptive statistics for Spam emails:\n",
      "        num_chars   num_words  num_sentences\n",
      "count  653.000000  653.000000     653.000000\n",
      "mean   137.891271   22.166922       2.970904\n",
      "std     30.137753    5.926027       1.488425\n",
      "min     13.000000    1.000000       1.000000\n",
      "25%    132.000000   19.000000       2.000000\n",
      "50%    149.000000   24.000000       3.000000\n",
      "75%    157.000000   26.000000       4.000000\n",
      "max    224.000000   34.000000       9.000000\n",
      "2025-08-01 14:36:30,566 - INFO - --- Completed Pipeline Step: EDA and Feature Engineering ---\n",
      "\n",
      "2025-08-01 14:36:30,568 - INFO - \n",
      "--- Starting Pipeline Step: Text Preprocessing for EDA ---\n",
      "2025-08-01 14:36:30,569 - INFO - \n",
      "--- Text Preprocessing for EDA and Visualizations ---\n",
      "2025-08-01 14:36:33,886 - INFO - Text transformation for EDA complete. Example:\n",
      "2025-08-01 14:36:33,890 - INFO - \n",
      "                                                                                                                                                          text                                                                                                               transformed_text\n",
      "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                       go jurong point crazi avail bugi great world la buffet cine got amor wat\n",
      "1                                                                                                                                Ok lar... Joking wif u oni...                                                                                                            ok lar joke wif oni\n",
      "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  free entri 2 wkli comp win fa cup final tkt 21st may text fa 87121 receiv entri question std txt rate appli 08452810075over18\n",
      "3                                                                                                            U dun say so early hor... U c already then say...                                                                                                  dun say earli hor alreadi say\n",
      "4                                                                                                Nah I don't think he goes to usf, he lives around here though                                                                                           nah think goe usf live around though\n",
      "2025-08-01 14:36:33,892 - INFO - \n",
      "Generating Word Clouds (saved to plots directory):\n",
      "2025-08-01 14:36:35,269 - INFO - Spam word cloud saved to /home/dev/spam_classifier_project/plots/spam_wordcloud_20250801_143633.png.\n",
      "2025-08-01 14:36:37,047 - INFO - Ham word cloud saved to /home/dev/spam_classifier_project/plots/ham_wordcloud_20250801_143633.png.\n",
      "2025-08-01 14:36:37,048 - INFO - \n",
      "Most common words in Spam (saved as plot):\n",
      "2025-08-01 14:36:37,866 - INFO - Plot 'Top 30 Spam Words' saved to /home/dev/spam_classifier_project/plots/top_spam_words_20250801_143633.png.\n",
      "2025-08-01 14:36:37,868 - INFO - \n",
      "Most common words in Ham (saved as plot):\n",
      "2025-08-01 14:36:38,701 - INFO - Plot 'Top 30 Ham Words' saved to /home/dev/spam_classifier_project/plots/top_ham_words_20250801_143633.png.\n",
      "2025-08-01 14:36:38,703 - INFO - --- Completed Pipeline Step: Text Preprocessing for EDA ---\n",
      "\n",
      "2025-08-01 14:36:38,704 - INFO - \n",
      "--- Starting Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "2025-08-01 14:36:38,705 - INFO - \n",
      "--- Text Vectorization (SentenceTransformer: all-MiniLM-L6-v2) ---\n",
      "2025-08-01 14:36:38,708 - INFO - Use pytorch device_name: cpu\n",
      "2025-08-01 14:36:38,709 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 81/81 [00:39<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 14:37:25,677 - INFO - SentenceTransformer embedding complete. X shape: (5169, 384), Y shape: (5169,).\n",
      "2025-08-01 14:37:25,678 - INFO - --- Completed Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "\n",
      "2025-08-01 14:37:25,679 - INFO - \n",
      "--- Starting Pipeline Step: Data Splitting ---\n",
      "2025-08-01 14:37:25,687 - INFO - Data split: Train 4135 samples, Test 1034 samples.\n",
      "2025-08-01 14:37:25,688 - INFO - Train target distribution: [3613  522]\n",
      "2025-08-01 14:37:25,689 - INFO - Test target distribution: [903 131]\n",
      "2025-08-01 14:37:25,691 - INFO - --- Completed Pipeline Step: Data Splitting ---\n",
      "\n",
      "2025-08-01 14:37:25,692 - INFO - \n",
      "--- Starting Pipeline Step: Hyperparameter Tuning ---\n",
      "2025-08-01 14:37:25,693 - INFO - Starting hyperparameter tuning with Optuna for selected models...\n",
      "2025-08-01 14:37:25,695 - INFO - Tuning LR model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-01 14:37:25,698] A new study created in memory with name: LR_tuning_study\n",
      "  0%|          | 0/15 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:29,397] Trial 0 finished with value: 0.8195201694380092 and parameters: {'C': 0.017670169402947963, 'solver': 'liblinear'}. Best is trial 0 with value: 0.8195201694380092.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.81952:   7%|         | 1/15 [00:06<00:57,  4.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:32,645] Trial 1 finished with value: 0.9067637144074732 and parameters: {'C': 0.39079671568228835, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  13%|        | 2/15 [00:07<00:46,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:33,566] Trial 2 finished with value: 0.5636527533822596 and parameters: {'C': 0.00022310108018679258, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  20%|        | 3/15 [00:10<00:28,  2.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:36,188] Trial 3 finished with value: 0.9252627284490291 and parameters: {'C': 1.7718847354806828, 'solver': 'saga'}. Best is trial 3 with value: 0.9252627284490291.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.925263:  27%|       | 4/15 [00:12<00:26,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:38,020] Trial 4 finished with value: 0.9358347576312023 and parameters: {'C': 9.877700294007917, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  33%|      | 5/15 [00:14<00:22,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:40,565] Trial 5 finished with value: 0.7982814551985044 and parameters: {'C': 0.0012601639723276807, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  40%|      | 6/15 [00:17<00:20,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:42,914] Trial 6 finished with value: 0.8533267901688955 and parameters: {'C': 0.039054412752107935, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  47%|     | 7/15 [00:19<00:18,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:45,304] Trial 7 finished with value: 0.7962869532134003 and parameters: {'C': 0.0006870101665590031, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  53%|    | 8/15 [00:20<00:16,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:46,423] Trial 8 finished with value: 0.8553585616674987 and parameters: {'C': 0.054502936945582565, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  60%|    | 9/15 [00:21<00:11,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:47,553] Trial 9 finished with value: 0.8757717445900015 and parameters: {'C': 0.12173252504194051, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  67%|   | 10/15 [00:24<00:08,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:49,990] Trial 10 finished with value: 0.9347469338394623 and parameters: {'C': 73.7864208342295, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  73%|  | 11/15 [00:26<00:07,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:52,463] Trial 11 finished with value: 0.9338781698536784 and parameters: {'C': 65.64817611753449, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  80%|  | 12/15 [00:29<00:06,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:54,906] Trial 12 finished with value: 0.9357473287321831 and parameters: {'C': 78.72383571224226, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  87%| | 13/15 [00:30<00:04,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:56,664] Trial 13 finished with value: 0.9331448068701527 and parameters: {'C': 8.224108054741553, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  93%|| 14/15 [00:32<00:02,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:37:58,481] Trial 14 finished with value: 0.9331984783268465 and parameters: {'C': 6.611757606926467, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835: 100%|| 15/15 [00:33<00:00,  2.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 14:37:58,805 - INFO - Best parameters for LR: {'C': 9.877700294007917, 'solver': 'liblinear'}\n",
      "2025-08-01 14:37:58,807 - INFO - Best cross-validated F1-score for LR: 0.9358\n",
      "2025-08-01 14:37:58,808 - INFO - Tuning RF model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-01 14:37:58,810] A new study created in memory with name: RF_tuning_study\n",
      "  0%|          | 0/15 [00:27<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:38:25,816] Trial 0 finished with value: 0.9105635300372142 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.9105635300372142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.910564:   7%|         | 1/15 [00:37<06:22, 27.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:38:35,840] Trial 1 finished with value: 0.9127467132964885 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  13%|        | 2/15 [01:11<03:43, 17.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:39:10,618] Trial 2 finished with value: 0.9091251939576921 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  20%|        | 3/15 [01:44<05:02, 25.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:39:43,773] Trial 3 finished with value: 0.9170959825132309 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  27%|       | 4/15 [02:08<05:11, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:40:07,779] Trial 4 finished with value: 0.9058311877181092 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  33%|      | 5/15 [02:31<04:27, 26.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:40:30,234] Trial 5 finished with value: 0.9090006738787435 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  40%|      | 6/15 [03:03<03:47, 25.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:41:01,815] Trial 6 finished with value: 0.9097470973538184 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  47%|     | 7/15 [03:22<03:39, 27.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:41:21,295] Trial 7 finished with value: 0.9093436919014714 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  53%|    | 8/15 [03:34<02:53, 24.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:41:33,507] Trial 8 finished with value: 0.9052117927047891 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  60%|    | 9/15 [03:48<02:05, 20.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:41:47,690] Trial 9 finished with value: 0.9089022907441778 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  67%|   | 10/15 [04:34<01:34, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:42:33,644] Trial 10 finished with value: 0.9116850200695618 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  73%|  | 11/15 [05:14<01:48, 27.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:43:13,229] Trial 11 finished with value: 0.9108888736746182 and parameters: {'n_estimators': 277, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  80%|  | 12/15 [05:22<01:32, 30.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:43:21,616] Trial 12 finished with value: 0.9084614524269989 and parameters: {'n_estimators': 59, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  87%| | 13/15 [06:05<00:48, 24.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:44:04,212] Trial 13 finished with value: 0.911159221076747 and parameters: {'n_estimators': 250, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  93%|| 14/15 [06:17<00:29, 29.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:44:15,888] Trial 14 finished with value: 0.9118333987728944 and parameters: {'n_estimators': 90, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096: 100%|| 15/15 [06:17<00:00, 25.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 14:44:16,197 - INFO - Best parameters for RF: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-08-01 14:44:16,198 - INFO - Best cross-validated F1-score for RF: 0.9171\n",
      "2025-08-01 14:44:16,201 - INFO - Tuning XGB model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-01 14:44:16,203] A new study created in memory with name: XGB_tuning_study\n",
      "  0%|          | 0/15 [00:25<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:44:41,555] Trial 0 finished with value: 0.922533456170615 and parameters: {'n_estimators': 144, 'max_depth': 12, 'learning_rate': 0.14553179565665345, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'gamma': 1.7699302940633311e-07}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:   7%|         | 1/15 [00:47<05:58, 25.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:45:03,264] Trial 1 finished with value: 0.8968282232826492 and parameters: {'n_estimators': 64, 'max_depth': 11, 'learning_rate': 0.07965261308120507, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'gamma': 0.574485163632042}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  13%|        | 2/15 [01:23<05:02, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:45:40,023] Trial 2 finished with value: 0.8411553008352888 and parameters: {'n_estimators': 258, 'max_depth': 5, 'learning_rate': 0.011551009439226469, 'subsample': 0.6733618039413735, 'colsample_bytree': 0.7216968971838151, 'gamma': 0.00015777981883364995}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  20%|        | 3/15 [01:41<05:53, 29.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:45:57,453] Trial 3 finished with value: 0.924591148223439 and parameters: {'n_estimators': 158, 'max_depth': 5, 'learning_rate': 0.08369042894376064, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'gamma': 8.528933855762793e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  27%|       | 4/15 [02:45<04:31, 24.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:47:01,524] Trial 4 finished with value: 0.8409469883992775 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.01254057843022616, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'gamma': 2.3528990899815284e-08}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  33%|      | 5/15 [03:05<06:29, 38.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:47:21,577] Trial 5 finished with value: 0.6888456060996102 and parameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.006746417134006626, 'subsample': 0.9795542149013333, 'colsample_bytree': 0.9862528132298237, 'gamma': 0.02932100047183291}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  40%|      | 6/15 [03:13<04:52, 32.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:47:29,925] Trial 6 finished with value: 0.909142632349273 and parameters: {'n_estimators': 126, 'max_depth': 3, 'learning_rate': 0.11679817513130797, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'gamma': 9.149877525022172e-05}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  47%|     | 7/15 [03:36<03:16, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:47:52,719] Trial 7 finished with value: 0.7659710875806484 and parameters: {'n_estimators': 58, 'max_depth': 12, 'learning_rate': 0.01646379567211809, 'subsample': 0.8650089137415928, 'colsample_bytree': 0.7246844304357644, 'gamma': 0.00014472520367197597}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  53%|    | 8/15 [03:49<02:48, 24.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:48:05,600] Trial 8 finished with value: 0.9134719551686679 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.43464957555697725, 'subsample': 0.9100531293444458, 'colsample_bytree': 0.9757995766256756, 'gamma': 0.14408501080722544}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  60%|    | 9/15 [04:58<02:03, 20.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:49:14,397] Trial 9 finished with value: 0.8332102676585844 and parameters: {'n_estimators': 200, 'max_depth': 12, 'learning_rate': 0.007515450322528414, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'gamma': 4.005370050283172e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  67%|   | 10/15 [05:48<02:57, 35.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:04,321] Trial 10 finished with value: 0.9143035711477321 and parameters: {'n_estimators': 287, 'max_depth': 7, 'learning_rate': 0.03621799474202481, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.8391524267229545, 'gamma': 0.0033264162114920023}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  73%|  | 11/15 [06:01<02:39, 39.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:17,796] Trial 11 finished with value: 0.9243616144942959 and parameters: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.22955406185548316, 'subsample': 0.7518416973680894, 'colsample_bytree': 0.7248996679248748, 'gamma': 1.3465901496770342e-07}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  80%|  | 12/15 [06:12<01:35, 31.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:28,504] Trial 12 finished with value: 0.9179095040995241 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.32976584052032165, 'subsample': 0.7273145000499233, 'colsample_bytree': 0.7624979833916741, 'gamma': 1.307420395434413e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  87%| | 13/15 [06:24<00:50, 25.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:40,875] Trial 13 finished with value: 0.9265312829611185 and parameters: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531:  93%|| 14/15 [06:41<00:21, 21.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:50:58,077] Trial 14 finished with value: 0.8645482549698402 and parameters: {'n_estimators': 78, 'max_depth': 6, 'learning_rate': 0.04374146076402921, 'subsample': 0.6041792656687146, 'colsample_bytree': 0.8955427636018558, 'gamma': 9.146410590181663e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531: 100%|| 15/15 [06:42<00:00, 26.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 14:50:58,386 - INFO - Best parameters for XGB: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}\n",
      "2025-08-01 14:50:58,389 - INFO - Best cross-validated F1-score for XGB: 0.9265\n",
      "2025-08-01 14:50:58,392 - INFO - Tuning SVC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-01 14:50:58,394] A new study created in memory with name: SVC_tuning_study\n",
      "  0%|          | 0/15 [04:48<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 14:55:46,416] Trial 0 finished with value: 0.48185640015644654 and parameters: {'C': 0.31489116479568624, 'gamma': 6.351221010640703, 'kernel': 'rbf'}. Best is trial 0 with value: 0.48185640015644654.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.481856:   7%|         | 1/15 [10:37<1:07:16, 288.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:01:36,066] Trial 1 finished with value: 0.8040040979459049 and parameters: {'C': 0.04207988669606638, 'gamma': 0.004207053950287938, 'kernel': 'sigmoid'}. Best is trial 1 with value: 0.8040040979459049.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.804004:  13%|        | 2/15 [11:01<1:10:17, 324.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:02:00,274] Trial 2 finished with value: 0.9221662225082857 and parameters: {'C': 2.5378155082656657, 'gamma': 0.679657809075816, 'kernel': 'sigmoid'}. Best is trial 2 with value: 0.9221662225082857.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.922166:  20%|        | 3/15 [11:59<37:27, 187.32s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:02:57,912] Trial 3 finished with value: 0.9274468578122226 and parameters: {'C': 21.368329072358772, 'gamma': 0.0070689749506246055, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  27%|       | 4/15 [13:37<24:57, 136.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:04:36,354] Trial 4 finished with value: 0.9000954051681628 and parameters: {'C': 0.1648044642797898, 'gamma': 0.12561043700013558, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  33%|      | 5/15 [17:12<20:25, 122.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:08:11,166] Trial 5 finished with value: 0.8466772244638058 and parameters: {'C': 2.801635158716261, 'gamma': 0.003613894271216527, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  40%|      | 6/15 [17:48<23:05, 153.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:08:47,211] Trial 6 finished with value: 0.8754548416250257 and parameters: {'C': 0.6672367170464207, 'gamma': 1.382623217936987, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  47%|     | 7/15 [21:26<15:23, 115.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:12:24,950] Trial 7 finished with value: 0.839863591318843 and parameters: {'C': 2.342384984711291, 'gamma': 0.0015339162591163618, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  53%|    | 8/15 [26:37<17:15, 147.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:17:35,938] Trial 8 finished with value: 0.08042653953868908 and parameters: {'C': 0.018205657658407266, 'gamma': 6.245139574743075, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  60%|    | 9/15 [32:06<19:53, 198.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:23:04,863] Trial 9 finished with value: 0.8047218721060352 and parameters: {'C': 0.1653693718282443, 'gamma': 0.002458603276328005, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  67%|   | 10/15 [32:31<19:55, 239.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:23:29,869] Trial 10 finished with value: 0.9415302848927896 and parameters: {'C': 60.33178530661243, 'gamma': 0.028504320627871515, 'kernel': 'sigmoid'}. Best is trial 10 with value: 0.9415302848927896.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 10. Best value: 0.94153:  73%|  | 11/15 [32:58<11:34, 173.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:23:56,538] Trial 11 finished with value: 0.9424880088028337 and parameters: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  80%|  | 12/15 [33:18<06:26, 128.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:24:16,963] Trial 12 finished with value: 0.9392053629128879 and parameters: {'C': 88.19429776626716, 'gamma': 0.03713740624438133, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  87%| | 13/15 [33:41<03:12, 96.02s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:24:40,353] Trial 13 finished with value: 0.9385834226508812 and parameters: {'C': 97.65296156943181, 'gamma': 0.025774482038992817, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  93%|| 14/15 [34:05<01:14, 74.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:04,240] Trial 14 finished with value: 0.9365904399909507 and parameters: {'C': 15.59864319752155, 'gamma': 0.13883438990307442, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488: 100%|| 15/15 [34:06<00:00, 136.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 15:25:04,497 - INFO - Best parameters for SVC: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}\n",
      "2025-08-01 15:25:04,499 - INFO - Best cross-validated F1-score for SVC: 0.9425\n",
      "2025-08-01 15:25:04,501 - INFO - Tuning ETC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-01 15:25:04,505] A new study created in memory with name: ETC_tuning_study\n",
      "  0%|          | 0/15 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:09,290] Trial 0 finished with value: 0.905345439765127 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:   7%|         | 1/15 [00:07<01:11,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:11,984] Trial 1 finished with value: 0.9033105221386787 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  13%|        | 2/15 [00:14<00:48,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:18,918] Trial 2 finished with value: 0.9048320961326024 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  20%|        | 3/15 [00:21<01:02,  5.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:25,989] Trial 3 finished with value: 0.9191989621106709 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  27%|       | 4/15 [00:26<01:05,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:30,924] Trial 4 finished with value: 0.9037124762642886 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  33%|      | 5/15 [00:31<00:55,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:36,302] Trial 5 finished with value: 0.9055210896400828 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  40%|      | 6/15 [00:38<00:49,  5.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:42,666] Trial 6 finished with value: 0.8996331187012563 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  47%|     | 7/15 [00:42<00:46,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:47,358] Trial 7 finished with value: 0.8992309400726045 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  53%|    | 8/15 [00:45<00:38,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:50,126] Trial 8 finished with value: 0.8987381998441906 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  60%|    | 9/15 [00:49<00:27,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:25:53,725] Trial 9 finished with value: 0.9065322904908435 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  67%|   | 10/15 [00:58<00:21,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:26:02,552] Trial 10 finished with value: 0.9112263682741635 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  73%|  | 11/15 [01:07<00:22,  5.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:26:11,536] Trial 11 finished with value: 0.907783505879283 and parameters: {'n_estimators': 290, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  80%|  | 12/15 [01:16<00:20,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:26:20,908] Trial 12 finished with value: 0.9131610037820834 and parameters: {'n_estimators': 293, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  87%| | 13/15 [01:24<00:14,  7.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:26:28,432] Trial 13 finished with value: 0.9150964771322074 and parameters: {'n_estimators': 250, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  93%|| 14/15 [01:30<00:07,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-01 15:26:35,162] Trial 14 finished with value: 0.9171971586951543 and parameters: {'n_estimators': 242, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199: 100%|| 15/15 [01:31<00:00,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 15:26:35,515 - INFO - Best parameters for ETC: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-08-01 15:26:35,517 - INFO - Best cross-validated F1-score for ETC: 0.9192\n",
      "2025-08-01 15:26:35,518 - INFO - Hyperparameter tuning completed for all selected models.\n",
      "2025-08-01 15:26:35,519 - INFO - --- Completed Pipeline Step: Hyperparameter Tuning ---\n",
      "\n",
      "2025-08-01 15:26:35,520 - INFO - \n",
      "--- Starting Pipeline Step: Final Model Training & Evaluation ---\n",
      "2025-08-01 15:26:35,521 - INFO - Applying SMOTE to the entire training data for final model training...\n",
      "2025-08-01 15:26:35,563 - INFO - SMOTE applied. Original train: 4135 samples. Resampled train: 7226 samples.\n",
      "2025-08-01 15:26:35,565 - INFO - Training final LR model on resampled data and evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-01 15:26:37,092 - INFO - \n",
      "--- Performance for LR ---\n",
      "2025-08-01 15:26:37,093 - INFO - Accuracy: 0.9787\n",
      "2025-08-01 15:26:37,094 - INFO - Precision (Spam): 0.8978\n",
      "2025-08-01 15:26:37,095 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-01 15:26:37,096 - INFO - F1-Score (Spam): 0.9179\n",
      "2025-08-01 15:26:37,108 - INFO - \n",
      "Full Classification Report for LR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.90      0.94      0.92       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.94      0.96      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:26:37,112 - INFO - \n",
      "Raw Confusion Matrix for LR:\n",
      "[[889  14]\n",
      " [  8 123]]\n",
      "2025-08-01 15:26:37,396 - INFO - Confusion matrix plot for LR saved to /home/dev/spam_classifier_project/plots/confusion_matrix_LR_20250801_152635.png.\n",
      "2025-08-01 15:26:37,397 - INFO - Training final RF model on resampled data and evaluating...\n",
      "2025-08-01 15:26:46,012 - INFO - \n",
      "--- Performance for RF ---\n",
      "2025-08-01 15:26:46,014 - INFO - Accuracy: 0.9826\n",
      "2025-08-01 15:26:46,015 - INFO - Precision (Spam): 0.9593\n",
      "2025-08-01 15:26:46,017 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-01 15:26:46,019 - INFO - F1-Score (Spam): 0.9291\n",
      "2025-08-01 15:26:46,043 - INFO - \n",
      "Full Classification Report for RF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.96      0.90      0.93       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.95      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:26:46,050 - INFO - \n",
      "Raw Confusion Matrix for RF:\n",
      "[[898   5]\n",
      " [ 13 118]]\n",
      "2025-08-01 15:26:46,327 - INFO - Confusion matrix plot for RF saved to /home/dev/spam_classifier_project/plots/confusion_matrix_RF_20250801_152635.png.\n",
      "2025-08-01 15:26:46,328 - INFO - Training final XGB model on resampled data and evaluating...\n",
      "2025-08-01 15:26:59,162 - INFO - \n",
      "--- Performance for XGB ---\n",
      "2025-08-01 15:26:59,164 - INFO - Accuracy: 0.9845\n",
      "2025-08-01 15:26:59,165 - INFO - Precision (Spam): 0.9389\n",
      "2025-08-01 15:26:59,167 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-01 15:26:59,168 - INFO - F1-Score (Spam): 0.9389\n",
      "2025-08-01 15:26:59,183 - INFO - \n",
      "Full Classification Report for XGB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:26:59,186 - INFO - \n",
      "Raw Confusion Matrix for XGB:\n",
      "[[895   8]\n",
      " [  8 123]]\n",
      "2025-08-01 15:26:59,388 - INFO - Confusion matrix plot for XGB saved to /home/dev/spam_classifier_project/plots/confusion_matrix_XGB_20250801_152635.png.\n",
      "2025-08-01 15:26:59,390 - INFO - Training final SVC model on resampled data and evaluating...\n",
      "2025-08-01 15:27:08,808 - INFO - \n",
      "--- Performance for SVC ---\n",
      "2025-08-01 15:27:08,809 - INFO - Accuracy: 0.9855\n",
      "2025-08-01 15:27:08,811 - INFO - Precision (Spam): 0.9394\n",
      "2025-08-01 15:27:08,812 - INFO - Recall (Spam): 0.9466\n",
      "2025-08-01 15:27:08,813 - INFO - F1-Score (Spam): 0.9430\n",
      "2025-08-01 15:27:08,829 - INFO - \n",
      "Full Classification Report for SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.95      0.94       131\n",
      "\n",
      "    accuracy                           0.99      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.99      0.99      0.99      1034\n",
      "\n",
      "2025-08-01 15:27:08,832 - INFO - \n",
      "Raw Confusion Matrix for SVC:\n",
      "[[895   8]\n",
      " [  7 124]]\n",
      "2025-08-01 15:27:09,007 - INFO - Confusion matrix plot for SVC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_SVC_20250801_152635.png.\n",
      "2025-08-01 15:27:09,008 - INFO - Training final KN model on resampled data and evaluating...\n",
      "2025-08-01 15:27:09,262 - INFO - \n",
      "--- Performance for KN ---\n",
      "2025-08-01 15:27:09,264 - INFO - Accuracy: 0.7611\n",
      "2025-08-01 15:27:09,265 - INFO - Precision (Spam): 0.3466\n",
      "2025-08-01 15:27:09,269 - INFO - Recall (Spam): 1.0000\n",
      "2025-08-01 15:27:09,271 - INFO - F1-Score (Spam): 0.5147\n",
      "2025-08-01 15:27:09,290 - INFO - \n",
      "Full Classification Report for KN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.73      0.84       903\n",
      "        spam       0.35      1.00      0.51       131\n",
      "\n",
      "    accuracy                           0.76      1034\n",
      "   macro avg       0.67      0.86      0.68      1034\n",
      "weighted avg       0.92      0.76      0.80      1034\n",
      "\n",
      "2025-08-01 15:27:09,295 - INFO - \n",
      "Raw Confusion Matrix for KN:\n",
      "[[656 247]\n",
      " [  0 131]]\n",
      "2025-08-01 15:27:09,545 - INFO - Confusion matrix plot for KN saved to /home/dev/spam_classifier_project/plots/confusion_matrix_KN_20250801_152635.png.\n",
      "2025-08-01 15:27:09,546 - INFO - Training final AdaBoost model on resampled data and evaluating...\n",
      "2025-08-01 15:28:36,103 - INFO - \n",
      "--- Performance for AdaBoost ---\n",
      "2025-08-01 15:28:36,104 - INFO - Accuracy: 0.9758\n",
      "2025-08-01 15:28:36,105 - INFO - Precision (Spam): 0.8681\n",
      "2025-08-01 15:28:36,106 - INFO - Recall (Spam): 0.9542\n",
      "2025-08-01 15:28:36,107 - INFO - F1-Score (Spam): 0.9091\n",
      "2025-08-01 15:28:36,119 - INFO - \n",
      "Full Classification Report for AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.87      0.95      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.93      0.97      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:28:36,123 - INFO - \n",
      "Raw Confusion Matrix for AdaBoost:\n",
      "[[884  19]\n",
      " [  6 125]]\n",
      "2025-08-01 15:28:36,278 - INFO - Confusion matrix plot for AdaBoost saved to /home/dev/spam_classifier_project/plots/confusion_matrix_AdaBoost_20250801_152635.png.\n",
      "2025-08-01 15:28:36,279 - INFO - Training final BgC model on resampled data and evaluating...\n",
      "2025-08-01 15:31:37,178 - INFO - \n",
      "--- Performance for BgC ---\n",
      "2025-08-01 15:31:37,181 - INFO - Accuracy: 0.9787\n",
      "2025-08-01 15:31:37,182 - INFO - Precision (Spam): 0.9291\n",
      "2025-08-01 15:31:37,184 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-01 15:31:37,186 - INFO - F1-Score (Spam): 0.9147\n",
      "2025-08-01 15:31:37,214 - INFO - \n",
      "Full Classification Report for BgC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.90      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.95      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:31:37,220 - INFO - \n",
      "Raw Confusion Matrix for BgC:\n",
      "[[894   9]\n",
      " [ 13 118]]\n",
      "2025-08-01 15:31:37,392 - INFO - Confusion matrix plot for BgC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_BgC_20250801_152635.png.\n",
      "2025-08-01 15:31:37,393 - INFO - Training final ETC model on resampled data and evaluating...\n",
      "2025-08-01 15:31:38,928 - INFO - \n",
      "--- Performance for ETC ---\n",
      "2025-08-01 15:31:38,929 - INFO - Accuracy: 0.9749\n",
      "2025-08-01 15:31:38,931 - INFO - Precision (Spam): 0.9200\n",
      "2025-08-01 15:31:38,933 - INFO - Recall (Spam): 0.8779\n",
      "2025-08-01 15:31:38,934 - INFO - F1-Score (Spam): 0.8984\n",
      "2025-08-01 15:31:38,951 - INFO - \n",
      "Full Classification Report for ETC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.99       903\n",
      "        spam       0.92      0.88      0.90       131\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.95      0.93      0.94      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n",
      "2025-08-01 15:31:38,954 - INFO - \n",
      "Raw Confusion Matrix for ETC:\n",
      "[[893  10]\n",
      " [ 16 115]]\n",
      "2025-08-01 15:31:39,140 - INFO - Confusion matrix plot for ETC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_ETC_20250801_152635.png.\n",
      "2025-08-01 15:31:39,141 - INFO - Training final GBDT model on resampled data and evaluating...\n",
      "2025-08-01 15:35:31,990 - INFO - \n",
      "--- Performance for GBDT ---\n",
      "2025-08-01 15:35:31,990 - INFO - Accuracy: 0.9836\n",
      "2025-08-01 15:35:31,991 - INFO - Precision (Spam): 0.9318\n",
      "2025-08-01 15:35:31,993 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-01 15:35:31,994 - INFO - F1-Score (Spam): 0.9354\n",
      "2025-08-01 15:35:32,010 - INFO - \n",
      "Full Classification Report for GBDT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.96      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-01 15:35:32,014 - INFO - \n",
      "Raw Confusion Matrix for GBDT:\n",
      "[[894   9]\n",
      " [  8 123]]\n",
      "2025-08-01 15:35:32,181 - INFO - Confusion matrix plot for GBDT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_GBDT_20250801_152635.png.\n",
      "2025-08-01 15:35:32,182 - INFO - Training final DT model on resampled data and evaluating...\n",
      "2025-08-01 15:35:36,499 - INFO - \n",
      "--- Performance for DT ---\n",
      "2025-08-01 15:35:36,500 - INFO - Accuracy: 0.9381\n",
      "2025-08-01 15:35:36,501 - INFO - Precision (Spam): 0.6982\n",
      "2025-08-01 15:35:36,501 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-01 15:35:36,502 - INFO - F1-Score (Spam): 0.7867\n",
      "2025-08-01 15:35:36,516 - INFO - \n",
      "Full Classification Report for DT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.94      0.96       903\n",
      "        spam       0.70      0.90      0.79       131\n",
      "\n",
      "    accuracy                           0.94      1034\n",
      "   macro avg       0.84      0.92      0.88      1034\n",
      "weighted avg       0.95      0.94      0.94      1034\n",
      "\n",
      "2025-08-01 15:35:36,520 - INFO - \n",
      "Raw Confusion Matrix for DT:\n",
      "[[852  51]\n",
      " [ 13 118]]\n",
      "2025-08-01 15:35:36,692 - INFO - Confusion matrix plot for DT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_DT_20250801_152635.png.\n",
      "2025-08-01 15:35:36,693 - INFO - Training final Voting model on resampled data and evaluating...\n",
      "2025-08-01 15:35:51,811 - INFO - \n",
      "--- Performance for Voting ---\n",
      "2025-08-01 15:35:51,812 - INFO - Accuracy: 0.9913\n",
      "2025-08-01 15:35:51,814 - INFO - Precision (Spam): 0.9919\n",
      "2025-08-01 15:35:51,815 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-01 15:35:51,816 - INFO - F1-Score (Spam): 0.9647\n",
      "2025-08-01 15:35:51,829 - INFO - \n",
      "Full Classification Report for Voting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00       903\n",
      "        spam       0.99      0.94      0.96       131\n",
      "\n",
      "    accuracy                           0.99      1034\n",
      "   macro avg       0.99      0.97      0.98      1034\n",
      "weighted avg       0.99      0.99      0.99      1034\n",
      "\n",
      "2025-08-01 15:35:51,831 - INFO - \n",
      "Raw Confusion Matrix for Voting:\n",
      "[[902   1]\n",
      " [  8 123]]\n",
      "2025-08-01 15:35:51,994 - INFO - Confusion matrix plot for Voting saved to /home/dev/spam_classifier_project/plots/confusion_matrix_Voting_20250801_152635.png.\n",
      "2025-08-01 15:35:52,004 - INFO - \n",
      "--- Overall Best Model Identified: Voting (F1-Score on Spam: 0.9647) ---\n",
      "2025-08-01 15:35:52,005 - INFO - All model evaluations completed.\n",
      "2025-08-01 15:35:52,055 - INFO - Best performing model (Voting) saved to /home/dev/spam_classifier_project/models/best_model_Voting_20250801_153552.pkl\n",
      "2025-08-01 15:35:52,738 - INFO - Model performance comparison plot saved to /home/dev/spam_classifier_project/plots/model_performance_comparison_20250801_152635.png.\n",
      "2025-08-01 15:35:52,739 - INFO - --- Completed Pipeline Step: Final Model Training & Evaluation ---\n",
      "\n",
      "2025-08-01 15:35:52,740 - INFO - Spam classification pipeline completed successfully.\n",
      "2025-08-01 15:35:52,741 - INFO - \n",
      "=== Spam Classification Pipeline Completed Successfully ===\n",
      "2025-08-01 15:35:52,742 - INFO - Overall Model Performance Summary (Sorted by F1-Score on Spam):\n",
      "       Model  Accuracy  Precision (Spam)  Recall (Spam)  F1-Score (Spam)\n",
      "0     Voting  0.991296          0.991935       0.938931         0.964706\n",
      "1        SVC  0.985493          0.939394       0.946565         0.942966\n",
      "2        XGB  0.984526          0.938931       0.938931         0.938931\n",
      "3       GBDT  0.983559          0.931818       0.938931         0.935361\n",
      "4         RF  0.982592          0.959350       0.900763         0.929134\n",
      "5         LR  0.978723          0.897810       0.938931         0.917910\n",
      "6        BgC  0.978723          0.929134       0.900763         0.914729\n",
      "7   AdaBoost  0.975822          0.868056       0.954198         0.909091\n",
      "8        ETC  0.974855          0.920000       0.877863         0.898438\n",
      "9         DT  0.938104          0.698225       0.900763         0.786667\n",
      "10        KN  0.761122          0.346561       1.000000         0.514735\n",
      "2025-08-01 15:35:52,747 - INFO - \n",
      "Best Performing Model Identified: Voting\n",
      "2025-08-01 15:35:52,748 - INFO - Check '/home/dev/spam_classifier_project/plots' for EDA and Confusion Matrix plots.\n",
      "2025-08-01 15:35:52,749 - INFO - Check '/home/dev/spam_classifier_project/models' for the saved best model.\n",
      "2025-08-01 15:35:52,750 - INFO - \n",
      "--- Demonstrating Model Inference from Saved Model ---\n",
      "2025-08-01 15:35:52,753 - INFO - Attempting to load the latest best model from: /home/dev/spam_classifier_project/models/best_model_Voting_20250801_153552.pkl\n",
      "2025-08-01 15:35:52,807 - INFO - NLTK punkt resource found.\n",
      "2025-08-01 15:35:52,808 - INFO - NLTK stopwords resource found.\n",
      "2025-08-01 15:35:52,809 - INFO - NLTK punkt_tab resource found.\n",
      "2025-08-01 15:35:52,813 - INFO - Initialized all individual and ensemble classifiers.\n",
      "2025-08-01 15:35:52,814 - INFO - SpamClassifier initialized successfully.\n",
      "2025-08-01 15:35:52,815 - INFO - Loading SentenceTransformer by name: 'all-MiniLM-L6-v2'\n",
      "2025-08-01 15:35:52,820 - INFO - Use pytorch device_name: cpu\n",
      "2025-08-01 15:35:52,821 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-01 15:35:59,019 - INFO - Model 'Voting' loaded successfully from /home/dev/spam_classifier_project/models/best_model_Voting_20250801_153552.pkl for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00,  4.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for SPAM text: 'WINNER! You have been selected for a 1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.' -> Label: spam, Spam Confidence: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|| 1/1 [00:00<00:00, 42.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for HAM text: 'Hey, just checking in. How are you doing today? Let's catch up soon for coffee!' -> Label: ham, Ham Confidence: 0.9926\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                             BaggingClassifier, ExtraTreesClassifier,\n",
    "                             GradientBoostingClassifier, VotingClassifier,\n",
    "                             StackingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Determine Base Directory for Notebook/Script ---\n",
    "try:\n",
    "    current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    base_directory = current_script_dir\n",
    "    print(f\"Running as a script. Base directory set to: '{base_directory}'\")\n",
    "except NameError:\n",
    "    base_directory = os.getcwd()\n",
    "    print(f\"Running in a notebook environment. Base directory set to CWD: '{base_directory}'\")\n",
    "\n",
    "\n",
    "# --- Configuration (Externalize for production) ---\n",
    "class Config:\n",
    "    DATA_PATH = os.path.join(base_directory, 'spam.csv')\n",
    "    SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n",
    "    LOG_FILE = os.path.join(base_directory, 'spam_classifier.log')\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    N_TRIALS_OPTUNA = 15\n",
    "    PLOTS_DIR = os.path.join(base_directory, 'plots')\n",
    "    MODELS_DIR = os.path.join(base_directory, 'models')\n",
    "\n",
    "# Ensure plot and model directories exist at startup\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self._configure_logging()\n",
    "        self._verify_nltk_resources()\n",
    "        self._configure_matplotlib()\n",
    "        self.df = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.sentence_transformer_model = None\n",
    "        self.X, self.y = None, None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None]*4\n",
    "        self.clfs = {}\n",
    "        self.best_tuned_models_params = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.performance_df = pd.DataFrame()\n",
    "        self._initialize_classifiers()\n",
    "        logging.info(\"SpamClassifier initialized successfully.\")\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(Config.LOG_FILE),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "    def _verify_nltk_resources(self) -> None:\n",
    "        resources = [\n",
    "            ('tokenizers/punkt', 'punkt'),\n",
    "            ('corpora/stopwords', 'stopwords'),\n",
    "            ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "        ]\n",
    "        for path, package in resources:\n",
    "            try:\n",
    "                nltk.data.find(path)\n",
    "                logging.info(f\"NLTK {package} resource found.\")\n",
    "            except LookupError:\n",
    "                logging.warning(f\"NLTK {package} not found. Attempting to download...\")\n",
    "                try:\n",
    "                    nltk.download(package, quiet=True)\n",
    "                    logging.info(f\"NLTK {package} downloaded successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.critical(f\"Failed to download NLTK {package}. Error: {e}\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _configure_matplotlib(self) -> None:\n",
    "        plt.ioff()\n",
    "        sns.set(style='whitegrid', palette='viridis')\n",
    "\n",
    "    def _initialize_classifiers(self) -> None:\n",
    "        \"\"\"Initializes all individual and ensemble classifiers.\"\"\"\n",
    "        self.clfs = {\n",
    "            'LR': LogisticRegression(\n",
    "                solver='liblinear',\n",
    "                penalty='l1',\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'RF': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'XGB': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=1\n",
    "            ),\n",
    "            'SVC': SVC(kernel='sigmoid', gamma=1.0, probability=True, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "            'KN': KNeighborsClassifier(),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'BgC': BaggingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1),\n",
    "            'ETC': ExtraTreesClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "            'GBDT': GradientBoostingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'DT': DecisionTreeClassifier(max_depth=5, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "        }\n",
    "        # Add a VotingClassifier using some of the best models\n",
    "        self.clfs['Voting'] = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb', self.clfs['XGB']),\n",
    "                ('svc', self.clfs['SVC']),\n",
    "                ('rf', self.clfs['RF']),\n",
    "            ],\n",
    "            voting='soft',  # Use 'soft' voting for probability-based prediction\n",
    "            weights=[0.3, 0.4, 0.3],  # Example weights (can be tuned)\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        logging.info(\"Initialized all individual and ensemble classifiers.\")\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        try:\n",
    "            if not os.path.exists(Config.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {os.path.abspath(Config.DATA_PATH)}\")\n",
    "            self.df = pd.read_csv(Config.DATA_PATH, encoding='latin-1')\n",
    "            if len(self.df) < 100:\n",
    "                raise ValueError(f\"Dataset too small ({len(self.df)} samples). Minimum 100 samples required for robust analysis.\")\n",
    "            logging.info(f\"Loaded {len(self.df)} records from {Config.DATA_PATH}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data loading failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def clean_data(self) -> None:\n",
    "        try:\n",
    "            if 'v1' in self.df.columns and 'v2' in self.df.columns:\n",
    "                self.df = self.df[['v1', 'v2']].copy()\n",
    "                logging.info(\"Selected 'v1' and 'v2' columns from the dataset.\")\n",
    "            else:\n",
    "                found_v1 = next((col for col in self.df.columns if 'target' in col.lower() or 'label' in col.lower() or 'type' in col.lower()), None)\n",
    "                found_v2 = next((col for col in self.df.columns if 'text' in col.lower() or 'message' in col.lower() or 'sms' in col.lower()), None)\n",
    "                if found_v1 and found_v2:\n",
    "                    self.df = self.df[[found_v1, found_v2]].copy()\n",
    "                    logging.info(f\"Mapped columns '{found_v1}' to 'target' and '{found_v2}' to 'text' using heuristics.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find required 'target' and 'text' columns (v1/v2 or equivalents) in dataset. Found columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "            self.df.columns = ['target', 'text']\n",
    "            valid_targets = {'ham', 'spam'}\n",
    "            invalid_targets = set(self.df['target'].unique()) - valid_targets\n",
    "            if invalid_targets:\n",
    "                logging.warning(f\"Invalid target values found: {invalid_targets}. Filtering out rows with these values.\")\n",
    "                self.df = self.df[self.df['target'].isin(valid_targets)]\n",
    "                if self.df.empty:\n",
    "                    raise ValueError(\"No valid 'ham' or 'spam' records remaining after filtering invalid targets. Dataset is empty.\")\n",
    "\n",
    "            self.df['target'] = self.encoder.fit_transform(self.df['target'])\n",
    "            initial_rows = len(self.df)\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            self.df.dropna(inplace=True)\n",
    "\n",
    "            logging.info(f\"Cleaned dataset. Removed {initial_rows - len(self.df)} duplicates/nulls. Remaining: {len(self.df)} records.\")\n",
    "            if self.df.empty:\n",
    "                raise ValueError(\"Dataset became empty after cleaning steps. Check data quality or initial loading.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data cleaning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _safe_tokenize(self, text: str) -> list[str]:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for tokenization: {text[:50]}...\")\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "            return [t for t in tokens if t.isalnum() and t not in string.punctuation]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Tokenization failed for text (first 50 chars: '{text[:50]}...'). Returning empty list. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def eda(self) -> None:\n",
    "        try:\n",
    "            self.df['num_words'] = self.df['text'].apply(lambda x: len(self._safe_tokenize(x)))\n",
    "            self.df['num_chars'] = self.df['text'].apply(len)\n",
    "            self.df['num_sentences'] = self.df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "\n",
    "            ham_count = self.df[self.df['target'] == self.encoder.transform(['ham'])[0]].shape[0]\n",
    "            spam_count = self.df[self.df['target'] == self.encoder.transform(['spam'])[0]].shape[0]\n",
    "            if spam_count > 0:\n",
    "                scale_pos_weight_val = ham_count / spam_count\n",
    "                if 'XGB' in self.clfs:\n",
    "                    self.clfs['XGB'].set_params(scale_pos_weight=scale_pos_weight_val)\n",
    "                logging.info(f\"Set XGBoost scale_pos_weight to: {scale_pos_weight_val:.2f} (Ham:{ham_count}, Spam:{spam_count})\")\n",
    "            else:\n",
    "                logging.warning(\"No spam samples found to calculate scale_pos_weight for XGBoost. Defaulting to 1.\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "            self.df['target'].value_counts().plot(\n",
    "                kind='pie', ax=ax1, autopct='%1.1f%%',\n",
    "                labels=self.encoder.inverse_transform(self.df['target'].value_counts().index),\n",
    "                colors=sns.color_palette('pastel')[0:2],\n",
    "                explode=[0, 0.1]\n",
    "            )\n",
    "            ax1.set_title('Target Class Distribution')\n",
    "            ax1.set_ylabel('')\n",
    "            fig1_filename = os.path.join(Config.PLOTS_DIR, f'target_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig1_filename, bbox_inches='tight')\n",
    "            plt.close(fig1)\n",
    "            logging.info(f\"Target distribution plot saved to {fig1_filename}.\")\n",
    "\n",
    "            fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax2.set_title('Word Count Distribution by Target Class')\n",
    "            ax2.set_xlabel('Number of Words')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.legend()\n",
    "            fig2_filename = os.path.join(Config.PLOTS_DIR, f'word_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig2_filename, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logging.info(f\"Word count distribution plot saved to {fig2_filename}.\")\n",
    "\n",
    "            fig3, ax3 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax3.set_title('Character Count Distribution by Target Class')\n",
    "            ax3.set_xlabel('Number of Characters')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.legend()\n",
    "            fig3_filename = os.path.join(Config.PLOTS_DIR, f'char_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig3_filename, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logging.info(f\"Character count distribution plot saved to {fig3_filename}.\")\n",
    "\n",
    "            fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(self.df[['num_chars', 'num_words', 'num_sentences', 'target']].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
    "            ax4.set_title('Correlation Matrix of Text Features and Target')\n",
    "            fig4_filename = os.path.join(Config.PLOTS_DIR, f'correlation_heatmap_{timestamp}.png')\n",
    "            plt.savefig(fig4_filename, bbox_inches='tight')\n",
    "            plt.close(fig4)\n",
    "            logging.info(f\"Correlation heatmap plot saved to {fig4_filename}.\")\n",
    "\n",
    "            logging.info(f\"Descriptive statistics for Ham emails:\\n{self.df[self.df['target'] == self.encoder.transform(['ham'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "            logging.info(f\"Descriptive statistics for Spam emails:\\n{self.df[self.df['target'] == self.encoder.transform(['spam'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EDA process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform_text(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for transform_text: {text[:50]}...\")\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        processed_tokens = [token for token in tokens if token.isalnum()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in processed_tokens if token not in stop_words and token not in string.punctuation]\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "        final_tokens = [token for token in stemmed_tokens if len(token) > 1 or token.isdigit()]\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def preprocess_text(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"\\n--- Text Preprocessing for EDA and Visualizations ---\")\n",
    "            self.df['transformed_text'] = self.df['text'].apply(self.transform_text)\n",
    "            logging.info(\"Text transformation for EDA complete. Example:\")\n",
    "            logging.info(f\"\\n{self.df[['text', 'transformed_text']].head().to_string()}\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            logging.info(\"\\nGenerating Word Clouds (saved to plots directory):\")\n",
    "            spam_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(spam_wc)\n",
    "            plt.title('Spam Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_spam_filename = os.path.join(Config.PLOTS_DIR, f'spam_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_spam_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Spam word cloud saved to {wc_spam_filename}.\")\n",
    "\n",
    "            ham_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(ham_wc)\n",
    "            plt.title('Ham Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_ham_filename = os.path.join(Config.PLOTS_DIR, f'ham_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_ham_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Ham word cloud saved to {wc_ham_filename}.\")\n",
    "\n",
    "            logging.info(\"\\nMost common words in Spam (saved as plot):\")\n",
    "            spam_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(spam_corpus, title='Top 30 Spam Words', filename=f'top_spam_words_{timestamp}.png')\n",
    "\n",
    "            logging.info(\"\\nMost common words in Ham (saved as plot):\")\n",
    "            ham_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(ham_corpus, title='Top 30 Ham Words', filename=f'top_ham_words_{timestamp}.png')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text preprocessing for EDA failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _plot_most_common_words(self, corpus: list[str], title: str, n: int = 30, filename: str = \"common_words.png\") -> None:\n",
    "        common_words = Counter(corpus).most_common(n)\n",
    "        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(x='Word', y='Count', data=df_common_words, ax=ax, palette='viridis')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "        ax.set_title(title)\n",
    "        plot_filepath = os.path.join(Config.PLOTS_DIR, filename)\n",
    "        plt.savefig(plot_filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Plot '{title}' saved to {plot_filepath}.\")\n",
    "\n",
    "    def vectorize_text_with_embeddings(self) -> None:\n",
    "        try:\n",
    "            logging.info(f\"\\n--- Text Vectorization (SentenceTransformer: {Config.SENTENCE_TRANSFORMER_MODEL}) ---\")\n",
    "            if self.sentence_transformer_model is None:\n",
    "                self.sentence_transformer_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "            self.X = self.sentence_transformer_model.encode(\n",
    "                self.df['text'].tolist(),\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                batch_size=64\n",
    "            )\n",
    "            self.y = self.df['target'].values\n",
    "            logging.info(f\"SentenceTransformer embedding complete. X shape: {self.X.shape}, Y shape: {self.y.shape}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text vectorization failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=Config.TEST_SIZE,\n",
    "                random_state=Config.RANDOM_STATE, stratify=self.y)\n",
    "            logging.info(f\"Data split: Train {len(self.X_train)} samples, Test {len(self.X_test)} samples.\")\n",
    "            logging.info(f\"Train target distribution: {np.bincount(self.y_train)}\")\n",
    "            logging.info(f\"Test target distribution: {np.bincount(self.y_test)}\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data splitting failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _objective(self, trial: optuna.trial.Trial, model_name: str) -> float:\n",
    "        if model_name == 'LR':\n",
    "            c_param = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "            solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            model = LogisticRegression(C=c_param, solver=solver, random_state=Config.RANDOM_STATE,\n",
    "                                       class_weight='balanced', max_iter=2000,\n",
    "                                       n_jobs=-1 if solver == 'saga' else None)\n",
    "        elif model_name == 'RF':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'XGB':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.005, 0.5)\n",
    "            subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "            gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "            current_scale_pos_weight = self.clfs['XGB'].get_params().get('scale_pos_weight', 1)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate, subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree, gamma=gamma,\n",
    "                                  random_state=Config.RANDOM_STATE,\n",
    "                                  eval_metric='logloss',\n",
    "                                  scale_pos_weight=current_scale_pos_weight)\n",
    "        elif model_name == 'SVC':\n",
    "            C_param = trial.suggest_loguniform('C', 1e-2, 1e2)\n",
    "            gamma_param = trial.suggest_loguniform('gamma', 1e-3, 1e1)\n",
    "            kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n",
    "            model = SVC(C=C_param, gamma=gamma_param, kernel=kernel, probability=True,\n",
    "                        random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        elif model_name == 'KN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, n_jobs=-1)\n",
    "        elif model_name == 'AdaBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'BgC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            model = BaggingClassifier(n_estimators=n_estimators, random_state=Config.RANDOM_STATE, n_jobs=-1)\n",
    "        elif model_name == 'ETC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'GBDT':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'DT':\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf, criterion=criterion,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' is not configured for Optuna tuning.\")\n",
    "\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "        scores = cross_val_score(pipeline, self.X_train, self.y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    def tune_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.y_train is None:\n",
    "                logging.error(\"Data not split for tuning. Calling split_data().\")\n",
    "                self.split_data()\n",
    "\n",
    "            logging.info(\"Starting hyperparameter tuning with Optuna for selected models...\")\n",
    "            models_to_tune = ['LR', 'RF', 'XGB', 'SVC', 'ETC']\n",
    "\n",
    "            for name in models_to_tune:\n",
    "                if name not in self.clfs:\n",
    "                    logging.warning(f\"Model '{name}' not found in initialized classifiers, skipping tuning.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Tuning {name} model with {Config.N_TRIALS_OPTUNA} trials...\")\n",
    "                study = optuna.create_study(direction='maximize',\n",
    "                                            sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_STATE),\n",
    "                                            study_name=f\"{name}_tuning_study\")\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    study.optimize(lambda trial: self._objective(trial, name),\n",
    "                                   n_trials=Config.N_TRIALS_OPTUNA,\n",
    "                                   show_progress_bar=True,\n",
    "                                   gc_after_trial=True)\n",
    "\n",
    "                self.best_tuned_models_params[name] = study.best_trial.params\n",
    "                logging.info(f\"Best parameters for {name}: {study.best_trial.params}\")\n",
    "                logging.info(f\"Best cross-validated F1-score for {name}: {study.best_trial.value:.4f}\")\n",
    "\n",
    "                self.clfs[name].set_params(**study.best_trial.params)\n",
    "                if name == 'XGB':\n",
    "                    current_scale_pos_weight = self.clfs[name].get_params().get('scale_pos_weight', 1)\n",
    "                    self.clfs[name].set_params(scale_pos_weight=current_scale_pos_weight)\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning completed for all selected models.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Model tuning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def train_final_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.X_test is None:\n",
    "                 logging.error(\"Data not split for final training. Calling split_data().\")\n",
    "                 self.split_data()\n",
    "\n",
    "            logging.info(\"Applying SMOTE to the entire training data for final model training...\")\n",
    "            smote = SMOTE(random_state=Config.RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
    "            logging.info(f\"SMOTE applied. Original train: {len(self.X_train)} samples. Resampled train: {len(X_train_resampled)} samples.\")\n",
    "\n",
    "            results = []\n",
    "            best_f1_overall = -1\n",
    "            self.best_model = None\n",
    "            self.best_model_name = None\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            # Training loop now includes the VotingClassifier\n",
    "            for name, model in self.clfs.items():\n",
    "                logging.info(f\"Training final {name} model on resampled data and evaluating...\")\n",
    "                try:\n",
    "                    # To use the ImbPipeline, we need to pass the model, not just the classifier\n",
    "                    pipeline = ImbPipeline([('smote', SMOTE(random_state=Config.RANDOM_STATE)), ('classifier', model)])\n",
    "                    pipeline.fit(self.X_train, self.y_train)\n",
    "                    y_pred = pipeline.predict(self.X_test)\n",
    "\n",
    "                    accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                    precision = precision_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    recall = recall_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    f1 = f1_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "\n",
    "                    report_dict = classification_report(self.y_test, y_pred, target_names=self.encoder.classes_, output_dict=True)\n",
    "\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'Precision (Spam)': precision,\n",
    "                        'Recall (Spam)': recall,\n",
    "                        'F1-Score (Spam)': f1,\n",
    "                        'Full Classification Report': report_dict\n",
    "                    })\n",
    "\n",
    "                    logging.info(f\"\\n--- Performance for {name} ---\")\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    logging.info(f\"Precision (Spam): {precision:.4f}\")\n",
    "                    logging.info(f\"Recall (Spam): {recall:.4f}\")\n",
    "                    logging.info(f\"F1-Score (Spam): {f1:.4f}\")\n",
    "                    logging.info(f\"\\nFull Classification Report for {name}:\\n{classification_report(self.y_test, y_pred, target_names=self.encoder.classes_)}\")\n",
    "\n",
    "                    cm = confusion_matrix(self.y_test, y_pred)\n",
    "                    logging.info(f\"\\nRaw Confusion Matrix for {name}:\\n{cm}\")\n",
    "\n",
    "                    fig_cm, ax_cm = plt.subplots(figsize=(7, 6))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=self.encoder.classes_,\n",
    "                                yticklabels=self.encoder.classes_,\n",
    "                                linecolor='gray', linewidths=0.5,\n",
    "                                annot_kws={\"size\": 14})\n",
    "                    ax_cm.set_xlabel('Predicted Label', fontsize=12)\n",
    "                    ax_cm.set_ylabel('True Label', fontsize=12)\n",
    "                    ax_cm.set_title(f'Confusion Matrix for {name}', fontsize=14)\n",
    "                    cm_filename = os.path.join(Config.PLOTS_DIR, f'confusion_matrix_{name}_{timestamp}.png')\n",
    "                    plt.savefig(cm_filename, bbox_inches='tight')\n",
    "                    plt.close(fig_cm)\n",
    "                    logging.info(f\"Confusion matrix plot for {name} saved to {cm_filename}.\")\n",
    "\n",
    "                    if f1 > best_f1_overall:\n",
    "                        best_f1_overall = f1\n",
    "                        self.best_model_name = name\n",
    "                        self.best_model = pipeline # Store the entire pipeline\n",
    "                except Exception as model_e:\n",
    "                    logging.error(f\"Error training or evaluating model {name}: {model_e}\")\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision (Spam)': np.nan,\n",
    "                        'Recall (Spam)': np.nan,\n",
    "                        'F1-Score (Spam)': np.nan,\n",
    "                        'Full Classification Report': {'error': str(model_e)}\n",
    "                    })\n",
    "\n",
    "            self.performance_df = pd.DataFrame(results)\n",
    "            self.performance_df = self.performance_df.sort_values(by='F1-Score (Spam)', ascending=False).reset_index(drop=True)\n",
    "            logging.info(f\"\\n--- Overall Best Model Identified: {self.best_model_name} (F1-Score on Spam: {best_f1_overall:.4f}) ---\")\n",
    "            logging.info(\"All model evaluations completed.\")\n",
    "            self._save_best_model()\n",
    "            self._plot_performance_comparison(timestamp)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Final model training and evaluation failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _save_best_model(self) -> None:\n",
    "        \"\"\"Saves the best performing model and related components to a pickle file.\"\"\"\n",
    "        try:\n",
    "            if self.best_model is None or self.best_model_name is None:\n",
    "                logging.warning(\"No best model identified or stored. Skipping model save operation.\")\n",
    "                return\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = os.path.join(Config.MODELS_DIR, f'best_model_{self.best_model_name}_{timestamp}.pkl')\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.best_model,\n",
    "                    'transformer': Config.SENTENCE_TRANSFORMER_MODEL,\n",
    "                    'encoder': self.encoder,\n",
    "                    'model_name': self.best_model_name,\n",
    "                    'performance_summary': self.performance_df.to_dict('records')\n",
    "                }, f)\n",
    "            logging.info(f\"Best performing model ({self.best_model_name}) saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save the best model: {e}\")\n",
    "\n",
    "    def _plot_performance_comparison(self, timestamp: str) -> None:\n",
    "        if self.performance_df.empty:\n",
    "            logging.warning(\"Performance DataFrame is empty, cannot plot comparison.\")\n",
    "            return\n",
    "        plot_df = self.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        sns.barplot(x='Model', y='Score', hue='Metric', data=plot_df_melted, palette='tab10', ax=ax)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title('Model Performance Comparison (Test Set)')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(Config.PLOTS_DIR, f'model_performance_comparison_{timestamp}.png')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Model performance comparison plot saved to {plot_filename}.\")\n",
    "\n",
    "    def run_pipeline(self) -> bool:\n",
    "        steps = [\n",
    "            ('Data Loading', self.load_data),\n",
    "            ('Data Cleaning', self.clean_data),\n",
    "            ('EDA and Feature Engineering', self.eda),\n",
    "            ('Text Preprocessing for EDA', self.preprocess_text),\n",
    "            ('Text Vectorization (Embeddings)', self.vectorize_text_with_embeddings),\n",
    "            ('Data Splitting', self.split_data),\n",
    "            ('Hyperparameter Tuning', self.tune_models),\n",
    "            ('Final Model Training & Evaluation', self.train_final_models)\n",
    "        ]\n",
    "        for name, step in steps:\n",
    "            try:\n",
    "                logging.info(f\"\\n--- Starting Pipeline Step: {name} ---\")\n",
    "                step()\n",
    "                logging.info(f\"--- Completed Pipeline Step: {name} ---\\n\")\n",
    "            except SystemExit:\n",
    "                logging.critical(f\"Pipeline stopped due to critical error in step: '{name}'.\")\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                logging.critical(f\"Pipeline failed unexpectedly in step '{name}': {e}\")\n",
    "                return False\n",
    "        logging.info(\"Spam classification pipeline completed successfully.\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def load_for_inference(model_path: str) -> 'SpamClassifier':\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found at {os.path.abspath(model_path)}\")\n",
    "            with open(model_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            classifier = SpamClassifier()\n",
    "            classifier.best_model = data['model']\n",
    "            classifier.encoder = data['encoder']\n",
    "            classifier.best_model_name = data.get('model_name', 'Unknown_Model')\n",
    "            transformer_data = data['transformer']\n",
    "            if isinstance(transformer_data, str):\n",
    "                logging.info(f\"Loading SentenceTransformer by name: '{transformer_data}'\")\n",
    "                classifier.sentence_transformer_model = SentenceTransformer(transformer_data)\n",
    "            else:\n",
    "                logging.warning(\"Loaded SentenceTransformer object directly from pickle.\")\n",
    "                classifier.sentence_transformer_model = transformer_data\n",
    "            classifier.ps = PorterStemmer()\n",
    "            logging.info(f\"Model '{classifier.best_model_name}' loaded successfully from {model_path} for inference.\")\n",
    "            return classifier\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to load model for inference from {model_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the label for a given text.\n",
    "        This method is now a simplified wrapper for predict_with_confidence.\n",
    "        \"\"\"\n",
    "        prediction_label, _, _ = self.predict_with_confidence(text)\n",
    "        return prediction_label\n",
    "\n",
    "    def predict_with_confidence(self, text: str) -> tuple[str, float, float]:\n",
    "        \"\"\"\n",
    "        Predicts the label and returns the confidence score for spam/ham.\n",
    "        Returns: (prediction_label, spam_confidence, ham_confidence)\n",
    "        \"\"\"\n",
    "        if self.best_model is None or self.sentence_transformer_model is None or self.encoder is None:\n",
    "            logging.error(\"Model components not loaded. Please load model using load_for_inference() before calling predict().\")\n",
    "            raise RuntimeError(\"Model components not available for prediction.\")\n",
    "        try:\n",
    "            vector = self.sentence_transformer_model.encode([text], convert_to_tensor=False)\n",
    "            prediction_encoded = self.best_model.predict(vector)[0]\n",
    "            prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n",
    "\n",
    "            # Get probabilities and confidence\n",
    "            prediction_proba = self.best_model.predict_proba(vector)[0]\n",
    "            classes = self.best_model.named_steps['classifier'].classes_\n",
    "            \n",
    "            spam_prob_idx = np.where(classes == self.encoder.transform(['spam'])[0])[0]\n",
    "            ham_prob_idx = np.where(classes == self.encoder.transform(['ham'])[0])[0]\n",
    "            \n",
    "            spam_confidence = prediction_proba[spam_prob_idx][0] if spam_prob_idx.size > 0 else 0.0\n",
    "            ham_confidence = prediction_proba[ham_prob_idx][0] if ham_prob_idx.size > 0 else 0.0\n",
    "            \n",
    "            return prediction_label, spam_confidence, ham_confidence\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction failed for text '{text[:50]}...': {e}\")\n",
    "            return \"error\", 0.0, 0.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = SpamClassifier()\n",
    "    pipeline_success = classifier.run_pipeline()\n",
    "\n",
    "    if pipeline_success:\n",
    "        logging.info(\"\\n=== Spam Classification Pipeline Completed Successfully ===\")\n",
    "        logging.info(\"Overall Model Performance Summary (Sorted by F1-Score on Spam):\")\n",
    "        print(classifier.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].to_string())\n",
    "        logging.info(f\"\\nBest Performing Model Identified: {classifier.best_model_name}\")\n",
    "        logging.info(f\"Check '{Config.PLOTS_DIR}' for EDA and Confusion Matrix plots.\")\n",
    "        logging.info(f\"Check '{Config.MODELS_DIR}' for the saved best model.\")\n",
    "\n",
    "        try:\n",
    "            logging.info(\"\\n--- Demonstrating Model Inference from Saved Model ---\")\n",
    "            model_files = [f for f in os.listdir(Config.MODELS_DIR) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join(Config.MODELS_DIR, f)))\n",
    "                latest_model_path = os.path.join(Config.MODELS_DIR, latest_model_file)\n",
    "                logging.info(f\"Attempting to load the latest best model from: {latest_model_path}\")\n",
    "                loaded_classifier = SpamClassifier.load_for_inference(latest_model_path)\n",
    "\n",
    "                test_spam_text_1 = \"WINNER! You have been selected for a 1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.\"\n",
    "                test_ham_text_1 = \"Hey, just checking in. How are you doing today? Let's catch up soon for coffee!\"\n",
    "                \n",
    "                label, spam_conf, ham_conf = loaded_classifier.predict_with_confidence(test_spam_text_1)\n",
    "                print(f\"Prediction for SPAM text: '{test_spam_text_1}' -> Label: {label}, Spam Confidence: {spam_conf:.4f}\")\n",
    "                \n",
    "                label, spam_conf, ham_conf = loaded_classifier.predict_with_confidence(test_ham_text_1)\n",
    "                print(f\"Prediction for HAM text: '{test_ham_text_1}' -> Label: {label}, Ham Confidence: {ham_conf:.4f}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(\"No model files found in the 'models' directory to demonstrate inference. Run the pipeline first.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the inference demonstration: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.critical(\"Spam classification pipeline failed during execution. Please review the log file for details.\")\n",
    "        sys.exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
