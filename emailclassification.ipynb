{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64908aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spam Classification Pipeline with EDA, Text Preprocessing, and Model Training\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                             BaggingClassifier, ExtraTreesClassifier,\n",
    "                             GradientBoostingClassifier, VotingClassifier,\n",
    "                             StackingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Determine Base Directory for Notebook/Script ---\n",
    "try:\n",
    "    current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    base_directory = current_script_dir\n",
    "    print(f\"Running as a script. Base directory set to: '{base_directory}'\")\n",
    "except NameError:\n",
    "    base_directory = os.getcwd()\n",
    "    print(f\"Running in a notebook environment. Base directory set to CWD: '{base_directory}'\")\n",
    "\n",
    "\n",
    "# --- Configuration (Externalize for production) ---\n",
    "class Config:\n",
    "    DATA_PATH = os.path.join(base_directory, 'spam.csv')\n",
    "    SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n",
    "    LOG_FILE = os.path.join(base_directory, 'spam_classifier.log')\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    N_TRIALS_OPTUNA = 15\n",
    "    PLOTS_DIR = os.path.join(base_directory, 'plots')\n",
    "    MODELS_DIR = os.path.join(base_directory, 'models')\n",
    "\n",
    "# Ensure plot and model directories exist at startup\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self._configure_logging()\n",
    "        self._verify_nltk_resources()\n",
    "        self._configure_matplotlib()\n",
    "        self.df = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.sentence_transformer_model = None\n",
    "        self.X, self.y = None, None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None]*4\n",
    "        self.clfs = {}\n",
    "        self.best_tuned_models_params = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.performance_df = pd.DataFrame()\n",
    "        self._initialize_classifiers()\n",
    "        logging.info(\"SpamClassifier initialized successfully.\")\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(Config.LOG_FILE),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "    def _verify_nltk_resources(self) -> None:\n",
    "        resources = [\n",
    "            ('tokenizers/punkt', 'punkt'),\n",
    "            ('corpora/stopwords', 'stopwords'),\n",
    "            ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "        ]\n",
    "        for path, package in resources:\n",
    "            try:\n",
    "                nltk.data.find(path)\n",
    "                logging.info(f\"NLTK {package} resource found.\")\n",
    "            except LookupError:\n",
    "                logging.warning(f\"NLTK {package} not found. Attempting to download...\")\n",
    "                try:\n",
    "                    nltk.download(package, quiet=True)\n",
    "                    logging.info(f\"NLTK {package} downloaded successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.critical(f\"Failed to download NLTK {package}. Error: {e}\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _configure_matplotlib(self) -> None:\n",
    "        plt.ioff()\n",
    "        sns.set(style='whitegrid', palette='viridis')\n",
    "\n",
    "    def _initialize_classifiers(self) -> None:\n",
    "        self.clfs = {\n",
    "            'LR': LogisticRegression(\n",
    "                solver='liblinear',\n",
    "                penalty='l1',\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'RF': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'XGB': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=1\n",
    "            ),\n",
    "            'SVC': SVC(kernel='sigmoid', gamma=1.0, probability=True, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "            'KN': KNeighborsClassifier(),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'BgC': BaggingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1),\n",
    "            'ETC': ExtraTreesClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "            'GBDT': GradientBoostingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'DT': DecisionTreeClassifier(max_depth=5, random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        }\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        try:\n",
    "            if not os.path.exists(Config.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {os.path.abspath(Config.DATA_PATH)}\")\n",
    "            self.df = pd.read_csv(Config.DATA_PATH, encoding='latin-1')\n",
    "            if len(self.df) < 100:\n",
    "                raise ValueError(f\"Dataset too small ({len(self.df)} samples). Minimum 100 samples required for robust analysis.\")\n",
    "            logging.info(f\"Loaded {len(self.df)} records from {Config.DATA_PATH}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data loading failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def clean_data(self) -> None:\n",
    "        try:\n",
    "            if 'v1' in self.df.columns and 'v2' in self.df.columns:\n",
    "                self.df = self.df[['v1', 'v2']].copy()\n",
    "                logging.info(\"Selected 'v1' and 'v2' columns from the dataset.\")\n",
    "            else:\n",
    "                found_v1 = next((col for col in self.df.columns if 'target' in col.lower() or 'label' in col.lower() or 'type' in col.lower()), None)\n",
    "                found_v2 = next((col for col in self.df.columns if 'text' in col.lower() or 'message' in col.lower() or 'sms' in col.lower()), None)\n",
    "                if found_v1 and found_v2:\n",
    "                    self.df = self.df[[found_v1, found_v2]].copy()\n",
    "                    logging.info(f\"Mapped columns '{found_v1}' to 'target' and '{found_v2}' to 'text' using heuristics.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find required 'target' and 'text' columns (v1/v2 or equivalents) in dataset. Found columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "            self.df.columns = ['target', 'text']\n",
    "            valid_targets = {'ham', 'spam'}\n",
    "            invalid_targets = set(self.df['target'].unique()) - valid_targets\n",
    "            if invalid_targets:\n",
    "                logging.warning(f\"Invalid target values found: {invalid_targets}. Filtering out rows with these values.\")\n",
    "                self.df = self.df[self.df['target'].isin(valid_targets)]\n",
    "                if self.df.empty:\n",
    "                    raise ValueError(\"No valid 'ham' or 'spam' records remaining after filtering invalid targets. Dataset is empty.\")\n",
    "\n",
    "            self.df['target'] = self.encoder.fit_transform(self.df['target'])\n",
    "            initial_rows = len(self.df)\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            self.df.dropna(inplace=True)\n",
    "\n",
    "            logging.info(f\"Cleaned dataset. Removed {initial_rows - len(self.df)} duplicates/nulls. Remaining: {len(self.df)} records.\")\n",
    "            if self.df.empty:\n",
    "                raise ValueError(\"Dataset became empty after cleaning steps. Check data quality or initial loading.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data cleaning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _safe_tokenize(self, text: str) -> list[str]:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for tokenization: {text[:50]}...\")\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "            return [t for t in tokens if t.isalnum() and t not in string.punctuation]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Tokenization failed for text (first 50 chars: '{text[:50]}...'). Returning empty list. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def eda(self) -> None:\n",
    "        try:\n",
    "            self.df['num_words'] = self.df['text'].apply(lambda x: len(self._safe_tokenize(x)))\n",
    "            self.df['num_chars'] = self.df['text'].apply(len)\n",
    "            self.df['num_sentences'] = self.df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "\n",
    "            ham_count = self.df[self.df['target'] == self.encoder.transform(['ham'])[0]].shape[0]\n",
    "            spam_count = self.df[self.df['target'] == self.encoder.transform(['spam'])[0]].shape[0]\n",
    "            if spam_count > 0:\n",
    "                scale_pos_weight_val = ham_count / spam_count\n",
    "                self.clfs['XGB'].set_params(scale_pos_weight=scale_pos_weight_val)\n",
    "                logging.info(f\"Set XGBoost scale_pos_weight to: {scale_pos_weight_val:.2f} (Ham:{ham_count}, Spam:{spam_count})\")\n",
    "            else:\n",
    "                logging.warning(\"No spam samples found to calculate scale_pos_weight for XGBoost. Defaulting to 1.\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "            self.df['target'].value_counts().plot(\n",
    "                kind='pie', ax=ax1, autopct='%1.1f%%',\n",
    "                labels=self.encoder.inverse_transform(self.df['target'].value_counts().index),\n",
    "                colors=sns.color_palette('pastel')[0:2],\n",
    "                explode=[0, 0.1]\n",
    "            )\n",
    "            ax1.set_title('Target Class Distribution')\n",
    "            ax1.set_ylabel('')\n",
    "            fig1_filename = os.path.join(Config.PLOTS_DIR, f'target_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig1_filename, bbox_inches='tight')\n",
    "            plt.close(fig1)\n",
    "            logging.info(f\"Target distribution plot saved to {fig1_filename}.\")\n",
    "\n",
    "            fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax2.set_title('Word Count Distribution by Target Class')\n",
    "            ax2.set_xlabel('Number of Words')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.legend()\n",
    "            fig2_filename = os.path.join(Config.PLOTS_DIR, f'word_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig2_filename, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logging.info(f\"Word count distribution plot saved to {fig2_filename}.\")\n",
    "\n",
    "            fig3, ax3 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax3.set_title('Character Count Distribution by Target Class')\n",
    "            ax3.set_xlabel('Number of Characters')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.legend()\n",
    "            fig3_filename = os.path.join(Config.PLOTS_DIR, f'char_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig3_filename, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logging.info(f\"Character count distribution plot saved to {fig3_filename}.\")\n",
    "\n",
    "            fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(self.df[['num_chars', 'num_words', 'num_sentences', 'target']].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
    "            ax4.set_title('Correlation Matrix of Text Features and Target')\n",
    "            fig4_filename = os.path.join(Config.PLOTS_DIR, f'correlation_heatmap_{timestamp}.png')\n",
    "            plt.savefig(fig4_filename, bbox_inches='tight')\n",
    "            plt.close(fig4)\n",
    "            logging.info(f\"Correlation heatmap plot saved to {fig4_filename}.\")\n",
    "\n",
    "            logging.info(f\"Descriptive statistics for Ham emails:\\n{self.df[self.df['target'] == self.encoder.transform(['ham'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "            logging.info(f\"Descriptive statistics for Spam emails:\\n{self.df[self.df['target'] == self.encoder.transform(['spam'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EDA process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform_text(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for transform_text: {text[:50]}...\")\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        processed_tokens = [token for token in tokens if token.isalnum()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in processed_tokens if token not in stop_words and token not in string.punctuation]\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "        final_tokens = [token for token in stemmed_tokens if len(token) > 1 or token.isdigit()]\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def preprocess_text(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"\\n--- Text Preprocessing for EDA and Visualizations ---\")\n",
    "            self.df['transformed_text'] = self.df['text'].apply(self.transform_text)\n",
    "            logging.info(\"Text transformation for EDA complete. Example:\")\n",
    "            logging.info(f\"\\n{self.df[['text', 'transformed_text']].head().to_string()}\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            logging.info(\"\\nGenerating Word Clouds (saved to plots directory):\")\n",
    "            spam_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(spam_wc)\n",
    "            plt.title('Spam Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_spam_filename = os.path.join(Config.PLOTS_DIR, f'spam_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_spam_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Spam word cloud saved to {wc_spam_filename}.\")\n",
    "\n",
    "            ham_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(ham_wc)\n",
    "            plt.title('Ham Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_ham_filename = os.path.join(Config.PLOTS_DIR, f'ham_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_ham_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Ham word cloud saved to {wc_ham_filename}.\")\n",
    "\n",
    "            logging.info(\"\\nMost common words in Spam (saved as plot):\")\n",
    "            spam_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(spam_corpus, title='Top 30 Spam Words', filename=f'top_spam_words_{timestamp}.png')\n",
    "\n",
    "            logging.info(\"\\nMost common words in Ham (saved as plot):\")\n",
    "            ham_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(ham_corpus, title='Top 30 Ham Words', filename=f'top_ham_words_{timestamp}.png')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text preprocessing for EDA failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _plot_most_common_words(self, corpus: list[str], title: str, n: int = 30, filename: str = \"common_words.png\") -> None:\n",
    "        common_words = Counter(corpus).most_common(n)\n",
    "        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(x='Word', y='Count', data=df_common_words, ax=ax, palette='viridis')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "        ax.set_title(title)\n",
    "        plot_filepath = os.path.join(Config.PLOTS_DIR, filename)\n",
    "        plt.savefig(plot_filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Plot '{title}' saved to {plot_filepath}.\")\n",
    "\n",
    "    def vectorize_text_with_embeddings(self) -> None:\n",
    "        try:\n",
    "            logging.info(f\"\\n--- Text Vectorization (SentenceTransformer: {Config.SENTENCE_TRANSFORMER_MODEL}) ---\")\n",
    "            if self.sentence_transformer_model is None:\n",
    "                self.sentence_transformer_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "            self.X = self.sentence_transformer_model.encode(\n",
    "                self.df['text'].tolist(),\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                batch_size=64\n",
    "            )\n",
    "            self.y = self.df['target'].values\n",
    "            logging.info(f\"SentenceTransformer embedding complete. X shape: {self.X.shape}, Y shape: {self.y.shape}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text vectorization failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=Config.TEST_SIZE,\n",
    "                random_state=Config.RANDOM_STATE, stratify=self.y)\n",
    "            logging.info(f\"Data split: Train {len(self.X_train)} samples, Test {len(self.X_test)} samples.\")\n",
    "            logging.info(f\"Train target distribution: {np.bincount(self.y_train)}\")\n",
    "            logging.info(f\"Test target distribution: {np.bincount(self.y_test)}\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data splitting failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _objective(self, trial: optuna.trial.Trial, model_name: str) -> float:\n",
    "        if model_name == 'LR':\n",
    "            c_param = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "            solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            model = LogisticRegression(C=c_param, solver=solver, random_state=Config.RANDOM_STATE,\n",
    "                                       class_weight='balanced', max_iter=2000,\n",
    "                                       n_jobs=-1 if solver == 'saga' else None)\n",
    "        elif model_name == 'RF':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'XGB':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.005, 0.5)\n",
    "            subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "            gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "            current_scale_pos_weight = self.clfs['XGB'].get_params().get('scale_pos_weight', 1)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate, subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree, gamma=gamma,\n",
    "                                  random_state=Config.RANDOM_STATE,\n",
    "                                  eval_metric='logloss',\n",
    "                                  scale_pos_weight=current_scale_pos_weight)\n",
    "        elif model_name == 'SVC':\n",
    "            C_param = trial.suggest_loguniform('C', 1e-2, 1e2)\n",
    "            gamma_param = trial.suggest_loguniform('gamma', 1e-3, 1e1)\n",
    "            kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n",
    "            model = SVC(C=C_param, gamma=gamma_param, kernel=kernel, probability=True,\n",
    "                        random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        elif model_name == 'KN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, n_jobs=-1)\n",
    "        elif model_name == 'AdaBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'BgC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            model = BaggingClassifier(n_estimators=n_estimators, random_state=Config.RANDOM_STATE, n_jobs=-1)\n",
    "        elif model_name == 'ETC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'GBDT':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'DT':\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf, criterion=criterion,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' is not configured for Optuna tuning.\")\n",
    "\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "        scores = cross_val_score(pipeline, self.X_train, self.y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    def tune_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.y_train is None:\n",
    "                logging.error(\"Data not split for tuning. Calling split_data().\")\n",
    "                self.split_data()\n",
    "\n",
    "            logging.info(\"Starting hyperparameter tuning with Optuna for selected models...\")\n",
    "            models_to_tune = ['LR', 'RF', 'XGB', 'SVC', 'ETC']\n",
    "\n",
    "            for name in models_to_tune:\n",
    "                if name not in self.clfs:\n",
    "                    logging.warning(f\"Model '{name}' not found in initialized classifiers, skipping tuning.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Tuning {name} model with {Config.N_TRIALS_OPTUNA} trials...\")\n",
    "                study = optuna.create_study(direction='maximize',\n",
    "                                            sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_STATE),\n",
    "                                            study_name=f\"{name}_tuning_study\")\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    study.optimize(lambda trial: self._objective(trial, name),\n",
    "                                   n_trials=Config.N_TRIALS_OPTUNA,\n",
    "                                   show_progress_bar=True,\n",
    "                                   gc_after_trial=True)\n",
    "\n",
    "                self.best_tuned_models_params[name] = study.best_trial.params\n",
    "                logging.info(f\"Best parameters for {name}: {study.best_trial.params}\")\n",
    "                logging.info(f\"Best cross-validated F1-score for {name}: {study.best_trial.value:.4f}\")\n",
    "\n",
    "                self.clfs[name].set_params(**study.best_trial.params)\n",
    "                if name == 'XGB':\n",
    "                    current_scale_pos_weight = self.clfs[name].get_params().get('scale_pos_weight', 1)\n",
    "                    self.clfs[name].set_params(scale_pos_weight=current_scale_pos_weight)\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning completed for all selected models.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Model tuning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def train_final_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.X_test is None:\n",
    "                 logging.error(\"Data not split for final training. Calling split_data().\")\n",
    "                 self.split_data()\n",
    "\n",
    "            logging.info(\"Applying SMOTE to the entire training data for final model training...\")\n",
    "            smote = SMOTE(random_state=Config.RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
    "            logging.info(f\"SMOTE applied. Original train: {len(self.X_train)} samples. Resampled train: {len(X_train_resampled)} samples.\")\n",
    "\n",
    "            results = []\n",
    "            best_f1_overall = -1\n",
    "            self.best_model = None\n",
    "            self.best_model_name = None\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            for name, model in self.clfs.items():\n",
    "                logging.info(f\"Training final {name} model on resampled data and evaluating...\")\n",
    "                try:\n",
    "                    # To use the ImbPipeline, we need to pass the model, not just the classifier\n",
    "                    pipeline = ImbPipeline([('smote', SMOTE(random_state=Config.RANDOM_STATE)), ('classifier', model)])\n",
    "                    pipeline.fit(self.X_train, self.y_train)\n",
    "                    y_pred = pipeline.predict(self.X_test)\n",
    "\n",
    "                    accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                    precision = precision_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    recall = recall_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    f1 = f1_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "\n",
    "                    report_dict = classification_report(self.y_test, y_pred, target_names=self.encoder.classes_, output_dict=True)\n",
    "\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'Precision (Spam)': precision,\n",
    "                        'Recall (Spam)': recall,\n",
    "                        'F1-Score (Spam)': f1,\n",
    "                        'Full Classification Report': report_dict\n",
    "                    })\n",
    "\n",
    "                    logging.info(f\"\\n--- Performance for {name} ---\")\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    logging.info(f\"Precision (Spam): {precision:.4f}\")\n",
    "                    logging.info(f\"Recall (Spam): {recall:.4f}\")\n",
    "                    logging.info(f\"F1-Score (Spam): {f1:.4f}\")\n",
    "                    logging.info(f\"\\nFull Classification Report for {name}:\\n{classification_report(self.y_test, y_pred, target_names=self.encoder.classes_)}\")\n",
    "\n",
    "                    cm = confusion_matrix(self.y_test, y_pred)\n",
    "                    logging.info(f\"\\nRaw Confusion Matrix for {name}:\\n{cm}\")\n",
    "\n",
    "                    fig_cm, ax_cm = plt.subplots(figsize=(7, 6))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=self.encoder.classes_,\n",
    "                                yticklabels=self.encoder.classes_,\n",
    "                                linecolor='gray', linewidths=0.5,\n",
    "                                annot_kws={\"size\": 14})\n",
    "                    ax_cm.set_xlabel('Predicted Label', fontsize=12)\n",
    "                    ax_cm.set_ylabel('True Label', fontsize=12)\n",
    "                    ax_cm.set_title(f'Confusion Matrix for {name}', fontsize=14)\n",
    "                    cm_filename = os.path.join(Config.PLOTS_DIR, f'confusion_matrix_{name}_{timestamp}.png')\n",
    "                    plt.savefig(cm_filename, bbox_inches='tight')\n",
    "                    plt.close(fig_cm)\n",
    "                    logging.info(f\"Confusion matrix plot for {name} saved to {cm_filename}.\")\n",
    "\n",
    "                    if f1 > best_f1_overall:\n",
    "                        best_f1_overall = f1\n",
    "                        self.best_model_name = name\n",
    "                        self.best_model = pipeline # Store the entire pipeline\n",
    "                except Exception as model_e:\n",
    "                    logging.error(f\"Error training or evaluating model {name}: {model_e}\")\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision (Spam)': np.nan,\n",
    "                        'Recall (Spam)': np.nan,\n",
    "                        'F1-Score (Spam)': np.nan,\n",
    "                        'Full Classification Report': {'error': str(model_e)}\n",
    "                    })\n",
    "\n",
    "            self.performance_df = pd.DataFrame(results)\n",
    "            self.performance_df = self.performance_df.sort_values(by='F1-Score (Spam)', ascending=False).reset_index(drop=True)\n",
    "            logging.info(f\"\\n--- Overall Best Model Identified: {self.best_model_name} (F1-Score on Spam: {best_f1_overall:.4f}) ---\")\n",
    "            logging.info(\"All model evaluations completed.\")\n",
    "            self._save_best_model()\n",
    "            self._plot_performance_comparison(timestamp)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Final model training and evaluation failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _save_best_model(self) -> None:\n",
    "        \"\"\"Saves the best performing model and related components to a pickle file.\"\"\"\n",
    "        try:\n",
    "            if self.best_model is None or self.best_model_name is None:\n",
    "                logging.warning(\"No best model identified or stored. Skipping model save operation.\")\n",
    "                return\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = os.path.join(Config.MODELS_DIR, f'best_model_{self.best_model_name}_{timestamp}.pkl')\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.best_model,\n",
    "                    'transformer': Config.SENTENCE_TRANSFORMER_MODEL, # THIS IS THE FIX\n",
    "                    'encoder': self.encoder,\n",
    "                    'model_name': self.best_model_name,\n",
    "                    'performance_summary': self.performance_df.to_dict('records')\n",
    "                }, f)\n",
    "            logging.info(f\"Best performing model ({self.best_model_name}) saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save the best model: {e}\")\n",
    "\n",
    "    def _plot_performance_comparison(self, timestamp: str) -> None:\n",
    "        if self.performance_df.empty:\n",
    "            logging.warning(\"Performance DataFrame is empty, cannot plot comparison.\")\n",
    "            return\n",
    "        plot_df = self.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        sns.barplot(x='Model', y='Score', hue='Metric', data=plot_df_melted, palette='tab10', ax=ax)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title('Model Performance Comparison (Test Set)')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(Config.PLOTS_DIR, f'model_performance_comparison_{timestamp}.png')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Model performance comparison plot saved to {plot_filename}.\")\n",
    "\n",
    "    def run_pipeline(self) -> bool:\n",
    "        steps = [\n",
    "            ('Data Loading', self.load_data),\n",
    "            ('Data Cleaning', self.clean_data),\n",
    "            ('EDA and Feature Engineering', self.eda),\n",
    "            ('Text Preprocessing for EDA', self.preprocess_text),\n",
    "            ('Text Vectorization (Embeddings)', self.vectorize_text_with_embeddings),\n",
    "            ('Data Splitting', self.split_data),\n",
    "            ('Hyperparameter Tuning', self.tune_models),\n",
    "            ('Final Model Training & Evaluation', self.train_final_models)\n",
    "        ]\n",
    "        for name, step in steps:\n",
    "            try:\n",
    "                logging.info(f\"\\n--- Starting Pipeline Step: {name} ---\")\n",
    "                step()\n",
    "                logging.info(f\"--- Completed Pipeline Step: {name} ---\\n\")\n",
    "            except SystemExit:\n",
    "                logging.critical(f\"Pipeline stopped due to critical error in step: '{name}'.\")\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                logging.critical(f\"Pipeline failed unexpectedly in step '{name}': {e}\")\n",
    "                return False\n",
    "        logging.info(\"Spam classification pipeline completed successfully.\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def load_for_inference(model_path: str) -> 'SpamClassifier':\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found at {os.path.abspath(model_path)}\")\n",
    "            with open(model_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            classifier = SpamClassifier()\n",
    "            classifier.best_model = data['model']\n",
    "            classifier.encoder = data['encoder']\n",
    "            classifier.best_model_name = data.get('model_name', 'Unknown_Model')\n",
    "            transformer_data = data['transformer']\n",
    "            if isinstance(transformer_data, str):\n",
    "                logging.info(f\"Loading SentenceTransformer by name: '{transformer_data}'\")\n",
    "                classifier.sentence_transformer_model = SentenceTransformer(transformer_data)\n",
    "            else:\n",
    "                logging.warning(\"Loaded SentenceTransformer object directly from pickle.\")\n",
    "                classifier.sentence_transformer_model = transformer_data\n",
    "            classifier.ps = PorterStemmer()\n",
    "            logging.info(f\"Model '{classifier.best_model_name}' loaded successfully from {model_path} for inference.\")\n",
    "            return classifier\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to load model for inference from {model_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        if self.best_model is None or self.sentence_transformer_model is None or self.encoder is None:\n",
    "            logging.error(\"Model components not loaded. Please run run_pipeline() or load model using load_for_inference() before calling predict().\")\n",
    "            raise RuntimeError(\"Model components not available for prediction.\")\n",
    "        try:\n",
    "            vector = self.sentence_transformer_model.encode([text], convert_to_tensor=False)\n",
    "            prediction_encoded = self.best_model.predict(vector)[0]\n",
    "            prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n",
    "            return prediction_label\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction failed for text '{text[:50]}...': {e}\")\n",
    "            return \"error\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = SpamClassifier()\n",
    "    pipeline_success = classifier.run_pipeline()\n",
    "\n",
    "    if pipeline_success:\n",
    "        logging.info(\"\\n=== Spam Classification Pipeline Completed Successfully ===\")\n",
    "        logging.info(\"Overall Model Performance Summary (Sorted by F1-Score on Spam):\")\n",
    "        print(classifier.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].to_string())\n",
    "        logging.info(f\"\\nBest Performing Model Identified: {classifier.best_model_name}\")\n",
    "        logging.info(f\"Check '{Config.PLOTS_DIR}' for EDA and Confusion Matrix plots.\")\n",
    "        logging.info(f\"Check '{Config.MODELS_DIR}' for the saved best model.\")\n",
    "\n",
    "        try:\n",
    "            logging.info(\"\\n--- Demonstrating Model Inference from Saved Model ---\")\n",
    "            model_files = [f for f in os.listdir(Config.MODELS_DIR) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join(Config.MODELS_DIR, f)))\n",
    "                latest_model_path = os.path.join(Config.MODELS_DIR, latest_model_file)\n",
    "                logging.info(f\"Attempting to load the latest best model from: {latest_model_path}\")\n",
    "                loaded_classifier = SpamClassifier.load_for_inference(latest_model_path)\n",
    "                test_spam_text_1 = \"WINNER! You have been selected for a Â£1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.\"\n",
    "                test_spam_text_2 = \"URGENT! Your bank account has been locked due to suspicious activity. Verify immediately at http://bit.ly/malicious-site to avoid closure.\"\n",
    "                test_ham_text_1 = \"Hey, just checking in. How are you doing today? Let's catch up soon for coffee!\"\n",
    "                test_ham_text_2 = \"Hi mom, can you pick up milk and bread on your way home? Thanks, love you!\"\n",
    "                test_empty_text = \"???!!!#@%\"\n",
    "                print(f\"\\nPrediction for SPAM text 1: '{test_spam_text_1}' -> {loaded_classifier.predict(test_spam_text_1)}\")\n",
    "                print(f\"Prediction for SPAM text 2: '{test_spam_text_2}' -> {loaded_classifier.predict(test_spam_text_2)}\")\n",
    "                print(f\"Prediction for HAM text 1: '{test_ham_text_1}' -> {loaded_classifier.predict(test_ham_text_1)}\")\n",
    "                print(f\"Prediction for HAM text 2: '{test_ham_text_2}' -> {loaded_classifier.predict(test_ham_text_2)}\")\n",
    "                print(f\"Prediction for EMPTY/NOISY text: '{test_empty_text}' -> {loaded_classifier.predict(test_empty_text)}\")\n",
    "            else:\n",
    "                logging.warning(\"No model files found in the 'models' directory to demonstrate inference. Run the pipeline first.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the inference demonstration: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.critical(\"Spam classification pipeline failed during execution. Please review the log file for details.\")\n",
    "        sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b45039a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/spam_classifier_project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in a notebook environment. Base directory set to CWD: '/home/dev/spam_classifier_project'\n",
      "2025-08-11 15:27:02,647 - INFO - NLTK punkt resource found.\n",
      "2025-08-11 15:27:02,649 - INFO - NLTK stopwords resource found.\n",
      "2025-08-11 15:27:02,650 - INFO - NLTK punkt_tab resource found.\n",
      "2025-08-11 15:27:02,652 - INFO - Initialized all individual and ensemble classifiers.\n",
      "2025-08-11 15:27:02,652 - INFO - SpamClassifier initialized successfully.\n",
      "2025-08-11 15:27:02,654 - INFO - \n",
      "--- Starting Pipeline Step: Data Loading ---\n",
      "2025-08-11 15:27:02,667 - INFO - Loaded 5572 records from /home/dev/spam_classifier_project/spam.csv.\n",
      "2025-08-11 15:27:02,668 - INFO - --- Completed Pipeline Step: Data Loading ---\n",
      "\n",
      "2025-08-11 15:27:02,669 - INFO - \n",
      "--- Starting Pipeline Step: Data Cleaning ---\n",
      "2025-08-11 15:27:02,674 - INFO - Selected 'v1' and 'v2' columns from the dataset.\n",
      "2025-08-11 15:27:02,869 - INFO - Cleaned dataset. Removed 403 duplicates/nulls. Remaining: 5169 records.\n",
      "2025-08-11 15:27:02,870 - INFO - --- Completed Pipeline Step: Data Cleaning ---\n",
      "\n",
      "2025-08-11 15:27:02,871 - INFO - \n",
      "--- Starting Pipeline Step: EDA and Feature Engineering ---\n",
      "2025-08-11 15:27:03,596 - INFO - Set XGBoost scale_pos_weight to: 6.92 (Ham:4516, Spam:653)\n",
      "2025-08-11 15:27:03,718 - INFO - Target distribution plot saved to /home/dev/spam_classifier_project/plots/target_distribution_20250811_152703.png.\n",
      "2025-08-11 15:27:04,224 - INFO - Word count distribution plot saved to /home/dev/spam_classifier_project/plots/word_count_distribution_20250811_152703.png.\n",
      "2025-08-11 15:27:04,695 - INFO - Character count distribution plot saved to /home/dev/spam_classifier_project/plots/char_count_distribution_20250811_152703.png.\n",
      "2025-08-11 15:27:04,934 - INFO - Correlation heatmap plot saved to /home/dev/spam_classifier_project/plots/correlation_heatmap_20250811_152703.png.\n",
      "2025-08-11 15:27:04,945 - INFO - Descriptive statistics for Ham emails:\n",
      "         num_chars    num_words  num_sentences\n",
      "count  4516.000000  4516.000000    4516.000000\n",
      "mean     70.459256    13.908769       1.820195\n",
      "std      56.358207    10.835081       1.383657\n",
      "min       2.000000     0.000000       1.000000\n",
      "25%      34.000000     7.000000       1.000000\n",
      "50%      52.000000    10.500000       1.000000\n",
      "75%      90.000000    18.000000       2.000000\n",
      "max     910.000000   162.000000      38.000000\n",
      "2025-08-11 15:27:04,956 - INFO - Descriptive statistics for Spam emails:\n",
      "        num_chars   num_words  num_sentences\n",
      "count  653.000000  653.000000     653.000000\n",
      "mean   137.891271   22.166922       2.970904\n",
      "std     30.137753    5.926027       1.488425\n",
      "min     13.000000    1.000000       1.000000\n",
      "25%    132.000000   19.000000       2.000000\n",
      "50%    149.000000   24.000000       3.000000\n",
      "75%    157.000000   26.000000       4.000000\n",
      "max    224.000000   34.000000       9.000000\n",
      "2025-08-11 15:27:04,957 - INFO - --- Completed Pipeline Step: EDA and Feature Engineering ---\n",
      "\n",
      "2025-08-11 15:27:04,958 - INFO - \n",
      "--- Starting Pipeline Step: Text Preprocessing for EDA ---\n",
      "2025-08-11 15:27:04,959 - INFO - \n",
      "--- Text Preprocessing for EDA and Visualizations ---\n",
      "2025-08-11 15:27:06,905 - INFO - Text transformation for EDA complete. Example:\n",
      "2025-08-11 15:27:06,908 - INFO - \n",
      "                                                                                                                                                          text                                                                                                               transformed_text\n",
      "0                                              Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                                       go jurong point crazi avail bugi great world la buffet cine got amor wat\n",
      "1                                                                                                                                Ok lar... Joking wif u oni...                                                                                                            ok lar joke wif oni\n",
      "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  free entri 2 wkli comp win fa cup final tkt 21st may text fa 87121 receiv entri question std txt rate appli 08452810075over18\n",
      "3                                                                                                            U dun say so early hor... U c already then say...                                                                                                  dun say earli hor alreadi say\n",
      "4                                                                                                Nah I don't think he goes to usf, he lives around here though                                                                                           nah think goe usf live around though\n",
      "2025-08-11 15:27:06,909 - INFO - \n",
      "Generating Word Clouds (saved to plots directory):\n",
      "2025-08-11 15:27:07,651 - INFO - Spam word cloud saved to /home/dev/spam_classifier_project/plots/spam_wordcloud_20250811_152706.png.\n",
      "2025-08-11 15:27:08,664 - INFO - Ham word cloud saved to /home/dev/spam_classifier_project/plots/ham_wordcloud_20250811_152706.png.\n",
      "2025-08-11 15:27:08,665 - INFO - \n",
      "Most common words in Spam (saved as plot):\n",
      "2025-08-11 15:27:09,129 - INFO - Plot 'Top 30 Spam Words' saved to /home/dev/spam_classifier_project/plots/top_spam_words_20250811_152706.png.\n",
      "2025-08-11 15:27:09,129 - INFO - \n",
      "Most common words in Ham (saved as plot):\n",
      "2025-08-11 15:27:09,648 - INFO - Plot 'Top 30 Ham Words' saved to /home/dev/spam_classifier_project/plots/top_ham_words_20250811_152706.png.\n",
      "2025-08-11 15:27:09,650 - INFO - --- Completed Pipeline Step: Text Preprocessing for EDA ---\n",
      "\n",
      "2025-08-11 15:27:09,650 - INFO - \n",
      "--- Starting Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "2025-08-11 15:27:09,652 - INFO - \n",
      "--- Text Vectorization (SentenceTransformer: all-MiniLM-L6-v2) ---\n",
      "2025-08-11 15:27:10,151 - INFO - Use pytorch device_name: cpu\n",
      "2025-08-11 15:27:10,152 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 81/81 [00:32<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:27:49,719 - INFO - SentenceTransformer embedding complete. X shape: (5169, 384), Y shape: (5169,).\n",
      "2025-08-11 15:27:49,720 - INFO - --- Completed Pipeline Step: Text Vectorization (Embeddings) ---\n",
      "\n",
      "2025-08-11 15:27:49,721 - INFO - \n",
      "--- Starting Pipeline Step: Data Splitting ---\n",
      "2025-08-11 15:27:49,727 - INFO - Data split: Train 4135 samples, Test 1034 samples.\n",
      "2025-08-11 15:27:49,728 - INFO - Train target distribution: [3613  522]\n",
      "2025-08-11 15:27:49,729 - INFO - Test target distribution: [903 131]\n",
      "2025-08-11 15:27:49,730 - INFO - --- Completed Pipeline Step: Data Splitting ---\n",
      "\n",
      "2025-08-11 15:27:49,731 - INFO - \n",
      "--- Starting Pipeline Step: Hyperparameter Tuning ---\n",
      "2025-08-11 15:27:49,731 - INFO - Starting hyperparameter tuning with Optuna for selected models...\n",
      "2025-08-11 15:27:49,732 - INFO - Tuning LR model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-11 15:27:49,734] A new study created in memory with name: LR_tuning_study\n",
      "  0%|          | 0/15 [00:03<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:27:52,899] Trial 0 finished with value: 0.8195201694380092 and parameters: {'C': 0.017670169402947963, 'solver': 'liblinear'}. Best is trial 0 with value: 0.8195201694380092.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.81952:   7%|â         | 1/15 [00:06<00:47,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:27:55,737] Trial 1 finished with value: 0.9067637144074732 and parameters: {'C': 0.39079671568228835, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  13%|ââ        | 2/15 [00:06<00:39,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:27:56,480] Trial 2 finished with value: 0.5636527533822596 and parameters: {'C': 0.00022310108018679258, 'solver': 'liblinear'}. Best is trial 1 with value: 0.9067637144074732.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.906764:  20%|ââ        | 3/15 [00:09<00:24,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:27:58,931] Trial 3 finished with value: 0.9252627284490291 and parameters: {'C': 1.7718847354806828, 'solver': 'saga'}. Best is trial 3 with value: 0.9252627284490291.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.925263:  27%|âââ       | 4/15 [00:11<00:24,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:00,923] Trial 4 finished with value: 0.9358347576312023 and parameters: {'C': 9.877700294007917, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  33%|ââââ      | 5/15 [00:13<00:21,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:03,010] Trial 5 finished with value: 0.7982814551985044 and parameters: {'C': 0.0012601639723276807, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  40%|ââââ      | 6/15 [00:15<00:18,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:04,878] Trial 6 finished with value: 0.8533267901688955 and parameters: {'C': 0.039054412752107935, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  53%|ââââââ    | 8/15 [00:17<00:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:06,796] Trial 7 finished with value: 0.7962869532134003 and parameters: {'C': 0.0006870101665590031, 'solver': 'saga'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  53%|ââââââ    | 8/15 [00:17<00:13,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:07,708] Trial 8 finished with value: 0.8553585616674987 and parameters: {'C': 0.054502936945582565, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  60%|ââââââ    | 9/15 [00:19<00:10,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:08,756] Trial 9 finished with value: 0.8757717445900015 and parameters: {'C': 0.12173252504194051, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  67%|âââââââ   | 10/15 [00:21<00:07,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:11,044] Trial 10 finished with value: 0.9347469338394623 and parameters: {'C': 73.7864208342295, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  73%|ââââââââ  | 11/15 [00:23<00:06,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:13,419] Trial 11 finished with value: 0.9338781698536784 and parameters: {'C': 65.64817611753449, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  80%|ââââââââ  | 12/15 [00:25<00:05,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:15,571] Trial 12 finished with value: 0.9357473287321831 and parameters: {'C': 78.72383571224226, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  87%|âââââââââ | 13/15 [00:27<00:03,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:17,163] Trial 13 finished with value: 0.9331448068701527 and parameters: {'C': 8.224108054741553, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835:  93%|ââââââââââ| 14/15 [00:28<00:01,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:18,708] Trial 14 finished with value: 0.9331984783268465 and parameters: {'C': 6.611757606926467, 'solver': 'liblinear'}. Best is trial 4 with value: 0.9358347576312023.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 4. Best value: 0.935835: 100%|ââââââââââ| 15/15 [00:29<00:00,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:28:18,978 - INFO - Best parameters for LR: {'C': 9.877700294007917, 'solver': 'liblinear'}\n",
      "2025-08-11 15:28:18,980 - INFO - Best cross-validated F1-score for LR: 0.9358\n",
      "2025-08-11 15:28:18,981 - INFO - Tuning RF model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-11 15:28:18,983] A new study created in memory with name: RF_tuning_study\n",
      "  0%|          | 0/15 [00:23<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:42,567] Trial 0 finished with value: 0.9105635300372142 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.9105635300372142.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.910564:   7%|â         | 1/15 [00:32<05:33, 23.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:28:51,273] Trial 1 finished with value: 0.9127467132964885 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  13%|ââ        | 2/15 [01:03<03:14, 14.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:29:22,101] Trial 2 finished with value: 0.9091251939576921 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 1 with value: 0.9127467132964885.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.912747:  20%|ââ        | 3/15 [01:31<04:26, 22.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:29:50,938] Trial 3 finished with value: 0.9170959825132309 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  33%|ââââ      | 5/15 [01:52<03:52, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:30:11,503] Trial 4 finished with value: 0.9058311877181092 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  33%|ââââ      | 5/15 [02:12<03:52, 23.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:30:31,200] Trial 5 finished with value: 0.9090006738787435 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  40%|ââââ      | 6/15 [02:39<03:18, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:30:58,411] Trial 6 finished with value: 0.9097470973538184 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  47%|âââââ     | 7/15 [02:56<03:10, 23.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:31:15,212] Trial 7 finished with value: 0.9093436919014714 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  53%|ââââââ    | 8/15 [03:07<02:30, 21.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:31:26,967] Trial 8 finished with value: 0.9052117927047891 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  60%|ââââââ    | 9/15 [03:20<01:50, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:31:39,392] Trial 9 finished with value: 0.9089022907441778 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  67%|âââââââ   | 10/15 [04:00<01:23, 16.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:32:19,378] Trial 10 finished with value: 0.9116850200695618 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  73%|ââââââââ  | 11/15 [04:34<01:35, 23.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:32:53,242] Trial 11 finished with value: 0.9108888736746182 and parameters: {'n_estimators': 277, 'max_depth': 9, 'min_samples_split': 2, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  80%|ââââââââ  | 12/15 [04:41<01:20, 26.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:33:00,271] Trial 12 finished with value: 0.9084614524269989 and parameters: {'n_estimators': 59, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 8}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  87%|âââââââââ | 13/15 [05:18<00:41, 20.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:33:37,383] Trial 13 finished with value: 0.911159221076747 and parameters: {'n_estimators': 250, 'max_depth': 13, 'min_samples_split': 5, 'min_samples_leaf': 7}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096:  93%|ââââââââââ| 14/15 [05:27<00:25, 25.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:33:46,808] Trial 14 finished with value: 0.9118333987728944 and parameters: {'n_estimators': 90, 'max_depth': 7, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9170959825132309.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.917096: 100%|ââââââââââ| 15/15 [05:28<00:00, 21.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:33:47,071 - INFO - Best parameters for RF: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-08-11 15:33:47,072 - INFO - Best cross-validated F1-score for RF: 0.9171\n",
      "2025-08-11 15:33:47,073 - INFO - Tuning XGB model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-11 15:33:47,075] A new study created in memory with name: XGB_tuning_study\n",
      "  0%|          | 0/15 [00:19<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:34:06,641] Trial 0 finished with value: 0.922533456170615 and parameters: {'n_estimators': 144, 'max_depth': 12, 'learning_rate': 0.14553179565665345, 'subsample': 0.8394633936788146, 'colsample_bytree': 0.6624074561769746, 'gamma': 1.7699302940633311e-07}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  13%|ââ        | 2/15 [00:40<04:24, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:34:27,499] Trial 1 finished with value: 0.8968282232826492 and parameters: {'n_estimators': 64, 'max_depth': 11, 'learning_rate': 0.07965261308120507, 'subsample': 0.8832290311184181, 'colsample_bytree': 0.608233797718321, 'gamma': 0.574485163632042}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  13%|ââ        | 2/15 [01:08<04:24, 20.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:34:55,911] Trial 2 finished with value: 0.8411553008352888 and parameters: {'n_estimators': 258, 'max_depth': 5, 'learning_rate': 0.011551009439226469, 'subsample': 0.6733618039413735, 'colsample_bytree': 0.7216968971838151, 'gamma': 0.00015777981883364995}. Best is trial 0 with value: 0.922533456170615.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.922533:  20%|ââ        | 3/15 [01:27<04:48, 24.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:35:14,348] Trial 3 finished with value: 0.924591148223439 and parameters: {'n_estimators': 158, 'max_depth': 5, 'learning_rate': 0.08369042894376064, 'subsample': 0.6557975442608167, 'colsample_bytree': 0.7168578594140873, 'gamma': 8.528933855762793e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  33%|ââââ      | 5/15 [02:23<05:41, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:36:10,288] Trial 4 finished with value: 0.8409469883992775 and parameters: {'n_estimators': 164, 'max_depth': 10, 'learning_rate': 0.01254057843022616, 'subsample': 0.8056937753654446, 'colsample_bytree': 0.836965827544817, 'gamma': 2.3528990899815284e-08}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  33%|ââââ      | 5/15 [02:45<05:41, 34.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:36:32,339] Trial 5 finished with value: 0.6888456060996102 and parameters: {'n_estimators': 202, 'max_depth': 4, 'learning_rate': 0.006746417134006626, 'subsample': 0.9795542149013333, 'colsample_bytree': 0.9862528132298237, 'gamma': 0.02932100047183291}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  40%|ââââ      | 6/15 [02:53<04:30, 30.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:36:40,556] Trial 6 finished with value: 0.909142632349273 and parameters: {'n_estimators': 126, 'max_depth': 3, 'learning_rate': 0.11679817513130797, 'subsample': 0.7760609974958406, 'colsample_bytree': 0.6488152939379115, 'gamma': 9.149877525022172e-05}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  47%|âââââ     | 7/15 [03:12<03:03, 22.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:36:59,236] Trial 7 finished with value: 0.7659710875806484 and parameters: {'n_estimators': 58, 'max_depth': 12, 'learning_rate': 0.01646379567211809, 'subsample': 0.8650089137415928, 'colsample_bytree': 0.7246844304357644, 'gamma': 0.00014472520367197597}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  53%|ââââââ    | 8/15 [03:19<02:30, 21.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:37:07,026] Trial 8 finished with value: 0.9134719551686679 and parameters: {'n_estimators': 187, 'max_depth': 4, 'learning_rate': 0.43464957555697725, 'subsample': 0.9100531293444458, 'colsample_bytree': 0.9757995766256756, 'gamma': 0.14408501080722544}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  67%|âââââââ   | 10/15 [04:18<02:29, 29.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:38:05,061] Trial 9 finished with value: 0.8332102676585844 and parameters: {'n_estimators': 200, 'max_depth': 12, 'learning_rate': 0.007515450322528414, 'subsample': 0.6783931449676581, 'colsample_bytree': 0.6180909155642152, 'gamma': 4.005370050283172e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  73%|ââââââââ  | 11/15 [05:02<02:17, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:38:49,863] Trial 10 finished with value: 0.9143035711477321 and parameters: {'n_estimators': 287, 'max_depth': 7, 'learning_rate': 0.03621799474202481, 'subsample': 0.6071847502459278, 'colsample_bytree': 0.8391524267229545, 'gamma': 0.0033264162114920023}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  73%|ââââââââ  | 11/15 [05:13<02:17, 34.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:39:01,055] Trial 11 finished with value: 0.9243616144942959 and parameters: {'n_estimators': 121, 'max_depth': 8, 'learning_rate': 0.22955406185548316, 'subsample': 0.7518416973680894, 'colsample_bytree': 0.7248996679248748, 'gamma': 1.3465901496770342e-07}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  80%|ââââââââ  | 12/15 [05:23<01:22, 27.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:39:10,395] Trial 12 finished with value: 0.9179095040995241 and parameters: {'n_estimators': 106, 'max_depth': 8, 'learning_rate': 0.32976584052032165, 'subsample': 0.7273145000499233, 'colsample_bytree': 0.7624979833916741, 'gamma': 1.307420395434413e-06}. Best is trial 3 with value: 0.924591148223439.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.924591:  87%|âââââââââ | 13/15 [05:34<00:43, 21.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:39:21,349] Trial 13 finished with value: 0.9265312829611185 and parameters: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531:  93%|ââââââââââ| 14/15 [05:48<00:18, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:39:35,308] Trial 14 finished with value: 0.8645482549698402 and parameters: {'n_estimators': 78, 'max_depth': 6, 'learning_rate': 0.04374146076402921, 'subsample': 0.6041792656687146, 'colsample_bytree': 0.8955427636018558, 'gamma': 9.146410590181663e-06}. Best is trial 13 with value: 0.9265312829611185.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 13. Best value: 0.926531: 100%|ââââââââââ| 15/15 [05:48<00:00, 23.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 15:39:35,529 - INFO - Best parameters for XGB: {'n_estimators': 102, 'max_depth': 7, 'learning_rate': 0.2197325710169943, 'subsample': 0.610547233762323, 'colsample_bytree': 0.7874820875551369, 'gamma': 6.7169034639277175e-06}\n",
      "2025-08-11 15:39:35,531 - INFO - Best cross-validated F1-score for XGB: 0.9265\n",
      "2025-08-11 15:39:35,532 - INFO - Tuning SVC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-11 15:39:35,534] A new study created in memory with name: SVC_tuning_study\n",
      "  0%|          | 0/15 [03:11<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:42:47,160] Trial 0 finished with value: 0.48185640015644654 and parameters: {'C': 0.31489116479568624, 'gamma': 6.351221010640703, 'kernel': 'rbf'}. Best is trial 0 with value: 0.48185640015644654.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.481856:   7%|â         | 1/15 [07:04<44:45, 191.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:46:40,260] Trial 1 finished with value: 0.8040040979459049 and parameters: {'C': 0.04207988669606638, 'gamma': 0.004207053950287938, 'kernel': 'sigmoid'}. Best is trial 1 with value: 0.8040040979459049.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 1. Best value: 0.804004:  13%|ââ        | 2/15 [07:24<46:49, 216.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:46:59,615] Trial 2 finished with value: 0.9221662225082857 and parameters: {'C': 2.5378155082656657, 'gamma': 0.679657809075816, 'kernel': 'sigmoid'}. Best is trial 2 with value: 0.9221662225082857.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 2. Best value: 0.922166:  20%|ââ        | 3/15 [08:08<25:15, 126.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:47:43,759] Trial 3 finished with value: 0.9274468578122226 and parameters: {'C': 21.368329072358772, 'gamma': 0.0070689749506246055, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  27%|âââ       | 4/15 [09:18<17:12, 93.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:48:53,794] Trial 4 finished with value: 0.9000954051681628 and parameters: {'C': 0.1648044642797898, 'gamma': 0.12561043700013558, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  33%|ââââ      | 5/15 [11:40<14:12, 85.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:51:15,568] Trial 5 finished with value: 0.8466772244638058 and parameters: {'C': 2.801635158716261, 'gamma': 0.003613894271216527, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  40%|ââââ      | 6/15 [12:01<15:40, 104.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:51:37,106] Trial 6 finished with value: 0.8754548416250257 and parameters: {'C': 0.6672367170464207, 'gamma': 1.382623217936987, 'kernel': 'sigmoid'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  53%|ââââââ    | 8/15 [14:29<11:38, 99.79s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:54:04,947] Trial 7 finished with value: 0.839863591318843 and parameters: {'C': 2.342384984711291, 'gamma': 0.0015339162591163618, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  53%|ââââââ    | 8/15 [18:20<11:38, 99.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 15:57:56,270] Trial 8 finished with value: 0.08042653953868908 and parameters: {'C': 0.018205657658407266, 'gamma': 6.245139574743075, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.927447:  60%|ââââââ    | 9/15 [22:10<14:05, 140.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:01:45,853] Trial 9 finished with value: 0.8047218721060352 and parameters: {'C': 0.1653693718282443, 'gamma': 0.002458603276328005, 'kernel': 'rbf'}. Best is trial 3 with value: 0.9274468578122226.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 10. Best value: 0.94153:  73%|ââââââââ  | 11/15 [22:26<08:06, 121.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:02:01,605] Trial 10 finished with value: 0.9415302848927896 and parameters: {'C': 60.33178530661243, 'gamma': 0.028504320627871515, 'kernel': 'sigmoid'}. Best is trial 10 with value: 0.9415302848927896.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 10. Best value: 0.94153:  73%|ââââââââ  | 11/15 [22:42<08:06, 121.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:02:17,825] Trial 11 finished with value: 0.9424880088028337 and parameters: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  80%|ââââââââ  | 12/15 [22:56<04:28, 89.56s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:02:31,361] Trial 12 finished with value: 0.9392053629128879 and parameters: {'C': 88.19429776626716, 'gamma': 0.03713740624438133, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  93%|ââââââââââ| 14/15 [23:09<00:50, 50.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:02:45,101] Trial 13 finished with value: 0.9385834226508812 and parameters: {'C': 97.65296156943181, 'gamma': 0.025774482038992817, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488:  93%|ââââââââââ| 14/15 [23:24<00:50, 50.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:00,022] Trial 14 finished with value: 0.9365904399909507 and parameters: {'C': 15.59864319752155, 'gamma': 0.13883438990307442, 'kernel': 'sigmoid'}. Best is trial 11 with value: 0.9424880088028337.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 11. Best value: 0.942488: 100%|ââââââââââ| 15/15 [23:24<00:00, 93.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:03:00,243 - INFO - Best parameters for SVC: {'C': 64.64947866087911, 'gamma': 0.024218157448679556, 'kernel': 'sigmoid'}\n",
      "2025-08-11 16:03:00,244 - INFO - Best cross-validated F1-score for SVC: 0.9425\n",
      "2025-08-11 16:03:00,246 - INFO - Tuning ETC model with 15 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[I 2025-08-11 16:03:00,248] A new study created in memory with name: ETC_tuning_study\n",
      "  0%|          | 0/15 [00:04<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:04,478] Trial 0 finished with value: 0.905345439765127 and parameters: {'n_estimators': 144, 'max_depth': 36, 'min_samples_split': 15, 'min_samples_leaf': 6}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:   7%|â         | 1/15 [00:06<01:03,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:06,811] Trial 1 finished with value: 0.9033105221386787 and parameters: {'n_estimators': 89, 'max_depth': 6, 'min_samples_split': 3, 'min_samples_leaf': 9}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  13%|ââ        | 2/15 [00:12<00:41,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:12,695] Trial 2 finished with value: 0.9048320961326024 and parameters: {'n_estimators': 200, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.905345439765127.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 0. Best value: 0.905345:  20%|ââ        | 3/15 [00:18<00:53,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:18,848] Trial 3 finished with value: 0.9191989621106709 and parameters: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  27%|âââ       | 4/15 [00:22<00:56,  5.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:22,832] Trial 4 finished with value: 0.9037124762642886 and parameters: {'n_estimators': 126, 'max_depth': 14, 'min_samples_split': 10, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  33%|ââââ      | 5/15 [00:27<00:47,  4.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:27,333] Trial 5 finished with value: 0.9055210896400828 and parameters: {'n_estimators': 203, 'max_depth': 6, 'min_samples_split': 7, 'min_samples_leaf': 4}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  40%|ââââ      | 6/15 [00:32<00:41,  4.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:32,376] Trial 6 finished with value: 0.8996331187012563 and parameters: {'n_estimators': 164, 'max_depth': 25, 'min_samples_split': 5, 'min_samples_leaf': 6}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  47%|âââââ     | 7/15 [00:36<00:38,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:36,376] Trial 7 finished with value: 0.8992309400726045 and parameters: {'n_estimators': 198, 'max_depth': 5, 'min_samples_split': 13, 'min_samples_leaf': 2}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  53%|ââââââ    | 8/15 [00:38<00:31,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:38,697] Trial 8 finished with value: 0.8987381998441906 and parameters: {'n_estimators': 66, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 9}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  60%|ââââââ    | 9/15 [00:41<00:23,  3.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:41,800] Trial 9 finished with value: 0.9065322904908435 and parameters: {'n_estimators': 126, 'max_depth': 6, 'min_samples_split': 15, 'min_samples_leaf': 5}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  67%|âââââââ   | 10/15 [00:49<00:18,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:49,510] Trial 10 finished with value: 0.9112263682741635 and parameters: {'n_estimators': 287, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  73%|ââââââââ  | 11/15 [00:57<00:19,  4.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:03:57,260] Trial 11 finished with value: 0.907783505879283 and parameters: {'n_estimators': 290, 'max_depth': 10, 'min_samples_split': 8, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  80%|ââââââââ  | 12/15 [01:04<00:17,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:04:05,107] Trial 12 finished with value: 0.9131610037820834 and parameters: {'n_estimators': 293, 'max_depth': 10, 'min_samples_split': 10, 'min_samples_leaf': 1}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  87%|âââââââââ | 13/15 [01:11<00:12,  6.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:04:11,632] Trial 13 finished with value: 0.9150964771322074 and parameters: {'n_estimators': 250, 'max_depth': 9, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199:  93%|ââââââââââ| 14/15 [01:17<00:06,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-08-11 16:04:17,806] Trial 14 finished with value: 0.9171971586951543 and parameters: {'n_estimators': 242, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 3 with value: 0.9191989621106709.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Best trial: 3. Best value: 0.919199: 100%|ââââââââââ| 15/15 [01:17<00:00,  5.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:04:18,087 - INFO - Best parameters for ETC: {'n_estimators': 258, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "2025-08-11 16:04:18,088 - INFO - Best cross-validated F1-score for ETC: 0.9192\n",
      "2025-08-11 16:04:18,090 - INFO - Hyperparameter tuning completed for all selected models.\n",
      "2025-08-11 16:04:18,091 - INFO - --- Completed Pipeline Step: Hyperparameter Tuning ---\n",
      "\n",
      "2025-08-11 16:04:18,092 - INFO - \n",
      "--- Starting Pipeline Step: Final Model Training & Evaluation ---\n",
      "2025-08-11 16:04:18,092 - INFO - Applying SMOTE to the entire training data for final model training...\n",
      "2025-08-11 16:04:18,189 - INFO - SMOTE applied. Original train: 4135 samples. Resampled train: 7226 samples.\n",
      "2025-08-11 16:04:18,190 - INFO - Training final LR model on resampled data and evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-08-11 16:04:19,046 - INFO - \n",
      "--- Performance for LR ---\n",
      "2025-08-11 16:04:19,046 - INFO - Accuracy: 0.9787\n",
      "2025-08-11 16:04:19,047 - INFO - Precision (Spam): 0.8978\n",
      "2025-08-11 16:04:19,047 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-11 16:04:19,048 - INFO - F1-Score (Spam): 0.9179\n",
      "2025-08-11 16:04:19,055 - INFO - \n",
      "Full Classification Report for LR:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.90      0.94      0.92       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.94      0.96      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:04:19,058 - INFO - \n",
      "Raw Confusion Matrix for LR:\n",
      "[[889  14]\n",
      " [  8 123]]\n",
      "2025-08-11 16:04:19,169 - INFO - Confusion matrix plot for LR saved to /home/dev/spam_classifier_project/plots/confusion_matrix_LR_20250811_160418.png.\n",
      "2025-08-11 16:04:19,170 - INFO - Training final RF model on resampled data and evaluating...\n",
      "2025-08-11 16:04:25,399 - INFO - \n",
      "--- Performance for RF ---\n",
      "2025-08-11 16:04:25,400 - INFO - Accuracy: 0.9826\n",
      "2025-08-11 16:04:25,400 - INFO - Precision (Spam): 0.9593\n",
      "2025-08-11 16:04:25,401 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-11 16:04:25,401 - INFO - F1-Score (Spam): 0.9291\n",
      "2025-08-11 16:04:25,410 - INFO - \n",
      "Full Classification Report for RF:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.96      0.90      0.93       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.95      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:04:25,412 - INFO - \n",
      "Raw Confusion Matrix for RF:\n",
      "[[898   5]\n",
      " [ 13 118]]\n",
      "2025-08-11 16:04:25,527 - INFO - Confusion matrix plot for RF saved to /home/dev/spam_classifier_project/plots/confusion_matrix_RF_20250811_160418.png.\n",
      "2025-08-11 16:04:25,528 - INFO - Training final XGB model on resampled data and evaluating...\n",
      "2025-08-11 16:04:34,650 - INFO - \n",
      "--- Performance for XGB ---\n",
      "2025-08-11 16:04:34,651 - INFO - Accuracy: 0.9845\n",
      "2025-08-11 16:04:34,651 - INFO - Precision (Spam): 0.9389\n",
      "2025-08-11 16:04:34,652 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-11 16:04:34,652 - INFO - F1-Score (Spam): 0.9389\n",
      "2025-08-11 16:04:34,664 - INFO - \n",
      "Full Classification Report for XGB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:04:34,667 - INFO - \n",
      "Raw Confusion Matrix for XGB:\n",
      "[[895   8]\n",
      " [  8 123]]\n",
      "2025-08-11 16:04:34,807 - INFO - Confusion matrix plot for XGB saved to /home/dev/spam_classifier_project/plots/confusion_matrix_XGB_20250811_160418.png.\n",
      "2025-08-11 16:04:34,808 - INFO - Training final SVC model on resampled data and evaluating...\n",
      "2025-08-11 16:04:43,688 - INFO - \n",
      "--- Performance for SVC ---\n",
      "2025-08-11 16:04:43,689 - INFO - Accuracy: 0.9855\n",
      "2025-08-11 16:04:43,690 - INFO - Precision (Spam): 0.9394\n",
      "2025-08-11 16:04:43,690 - INFO - Recall (Spam): 0.9466\n",
      "2025-08-11 16:04:43,691 - INFO - F1-Score (Spam): 0.9430\n",
      "2025-08-11 16:04:43,699 - INFO - \n",
      "Full Classification Report for SVC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.94      0.95      0.94       131\n",
      "\n",
      "    accuracy                           0.99      1034\n",
      "   macro avg       0.97      0.97      0.97      1034\n",
      "weighted avg       0.99      0.99      0.99      1034\n",
      "\n",
      "2025-08-11 16:04:43,701 - INFO - \n",
      "Raw Confusion Matrix for SVC:\n",
      "[[895   8]\n",
      " [  7 124]]\n",
      "2025-08-11 16:04:43,805 - INFO - Confusion matrix plot for SVC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_SVC_20250811_160418.png.\n",
      "2025-08-11 16:04:43,806 - INFO - Training final KN model on resampled data and evaluating...\n",
      "2025-08-11 16:04:43,943 - INFO - \n",
      "--- Performance for KN ---\n",
      "2025-08-11 16:04:43,944 - INFO - Accuracy: 0.7611\n",
      "2025-08-11 16:04:43,944 - INFO - Precision (Spam): 0.3466\n",
      "2025-08-11 16:04:43,945 - INFO - Recall (Spam): 1.0000\n",
      "2025-08-11 16:04:43,945 - INFO - F1-Score (Spam): 0.5147\n",
      "2025-08-11 16:04:43,953 - INFO - \n",
      "Full Classification Report for KN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.73      0.84       903\n",
      "        spam       0.35      1.00      0.51       131\n",
      "\n",
      "    accuracy                           0.76      1034\n",
      "   macro avg       0.67      0.86      0.68      1034\n",
      "weighted avg       0.92      0.76      0.80      1034\n",
      "\n",
      "2025-08-11 16:04:43,955 - INFO - \n",
      "Raw Confusion Matrix for KN:\n",
      "[[656 247]\n",
      " [  0 131]]\n",
      "2025-08-11 16:04:44,048 - INFO - Confusion matrix plot for KN saved to /home/dev/spam_classifier_project/plots/confusion_matrix_KN_20250811_160418.png.\n",
      "2025-08-11 16:04:44,048 - INFO - Training final AdaBoost model on resampled data and evaluating...\n",
      "2025-08-11 16:05:32,563 - INFO - \n",
      "--- Performance for AdaBoost ---\n",
      "2025-08-11 16:05:32,565 - INFO - Accuracy: 0.9758\n",
      "2025-08-11 16:05:32,565 - INFO - Precision (Spam): 0.8681\n",
      "2025-08-11 16:05:32,566 - INFO - Recall (Spam): 0.9542\n",
      "2025-08-11 16:05:32,567 - INFO - F1-Score (Spam): 0.9091\n",
      "2025-08-11 16:05:32,574 - INFO - \n",
      "Full Classification Report for AdaBoost:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.98      0.99       903\n",
      "        spam       0.87      0.95      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.93      0.97      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:05:32,575 - INFO - \n",
      "Raw Confusion Matrix for AdaBoost:\n",
      "[[884  19]\n",
      " [  6 125]]\n",
      "2025-08-11 16:05:32,667 - INFO - Confusion matrix plot for AdaBoost saved to /home/dev/spam_classifier_project/plots/confusion_matrix_AdaBoost_20250811_160418.png.\n",
      "2025-08-11 16:05:32,667 - INFO - Training final BgC model on resampled data and evaluating...\n",
      "2025-08-11 16:07:41,243 - INFO - \n",
      "--- Performance for BgC ---\n",
      "2025-08-11 16:07:41,245 - INFO - Accuracy: 0.9787\n",
      "2025-08-11 16:07:41,246 - INFO - Precision (Spam): 0.9291\n",
      "2025-08-11 16:07:41,248 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-11 16:07:41,250 - INFO - F1-Score (Spam): 0.9147\n",
      "2025-08-11 16:07:41,281 - INFO - \n",
      "Full Classification Report for BgC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.90      0.91       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.95      0.95      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:07:41,291 - INFO - \n",
      "Raw Confusion Matrix for BgC:\n",
      "[[894   9]\n",
      " [ 13 118]]\n",
      "2025-08-11 16:07:41,560 - INFO - Confusion matrix plot for BgC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_BgC_20250811_160418.png.\n",
      "2025-08-11 16:07:41,561 - INFO - Training final ETC model on resampled data and evaluating...\n",
      "2025-08-11 16:07:43,104 - INFO - \n",
      "--- Performance for ETC ---\n",
      "2025-08-11 16:07:43,105 - INFO - Accuracy: 0.9749\n",
      "2025-08-11 16:07:43,106 - INFO - Precision (Spam): 0.9200\n",
      "2025-08-11 16:07:43,108 - INFO - Recall (Spam): 0.8779\n",
      "2025-08-11 16:07:43,109 - INFO - F1-Score (Spam): 0.8984\n",
      "2025-08-11 16:07:43,136 - INFO - \n",
      "Full Classification Report for ETC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.99       903\n",
      "        spam       0.92      0.88      0.90       131\n",
      "\n",
      "    accuracy                           0.97      1034\n",
      "   macro avg       0.95      0.93      0.94      1034\n",
      "weighted avg       0.97      0.97      0.97      1034\n",
      "\n",
      "2025-08-11 16:07:43,140 - INFO - \n",
      "Raw Confusion Matrix for ETC:\n",
      "[[893  10]\n",
      " [ 16 115]]\n",
      "2025-08-11 16:07:43,341 - INFO - Confusion matrix plot for ETC saved to /home/dev/spam_classifier_project/plots/confusion_matrix_ETC_20250811_160418.png.\n",
      "2025-08-11 16:07:43,342 - INFO - Training final GBDT model on resampled data and evaluating...\n",
      "2025-08-11 16:11:25,070 - INFO - \n",
      "--- Performance for GBDT ---\n",
      "2025-08-11 16:11:25,071 - INFO - Accuracy: 0.9836\n",
      "2025-08-11 16:11:25,072 - INFO - Precision (Spam): 0.9318\n",
      "2025-08-11 16:11:25,073 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-11 16:11:25,074 - INFO - F1-Score (Spam): 0.9354\n",
      "2025-08-11 16:11:25,087 - INFO - \n",
      "Full Classification Report for GBDT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99       903\n",
      "        spam       0.93      0.94      0.94       131\n",
      "\n",
      "    accuracy                           0.98      1034\n",
      "   macro avg       0.96      0.96      0.96      1034\n",
      "weighted avg       0.98      0.98      0.98      1034\n",
      "\n",
      "2025-08-11 16:11:25,090 - INFO - \n",
      "Raw Confusion Matrix for GBDT:\n",
      "[[894   9]\n",
      " [  8 123]]\n",
      "2025-08-11 16:11:25,243 - INFO - Confusion matrix plot for GBDT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_GBDT_20250811_160418.png.\n",
      "2025-08-11 16:11:25,244 - INFO - Training final DT model on resampled data and evaluating...\n",
      "2025-08-11 16:11:29,233 - INFO - \n",
      "--- Performance for DT ---\n",
      "2025-08-11 16:11:29,234 - INFO - Accuracy: 0.9381\n",
      "2025-08-11 16:11:29,235 - INFO - Precision (Spam): 0.6982\n",
      "2025-08-11 16:11:29,236 - INFO - Recall (Spam): 0.9008\n",
      "2025-08-11 16:11:29,236 - INFO - F1-Score (Spam): 0.7867\n",
      "2025-08-11 16:11:29,249 - INFO - \n",
      "Full Classification Report for DT:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.94      0.96       903\n",
      "        spam       0.70      0.90      0.79       131\n",
      "\n",
      "    accuracy                           0.94      1034\n",
      "   macro avg       0.84      0.92      0.88      1034\n",
      "weighted avg       0.95      0.94      0.94      1034\n",
      "\n",
      "2025-08-11 16:11:29,252 - INFO - \n",
      "Raw Confusion Matrix for DT:\n",
      "[[852  51]\n",
      " [ 13 118]]\n",
      "2025-08-11 16:11:29,402 - INFO - Confusion matrix plot for DT saved to /home/dev/spam_classifier_project/plots/confusion_matrix_DT_20250811_160418.png.\n",
      "2025-08-11 16:11:29,403 - INFO - Training final Voting model on resampled data and evaluating...\n",
      "2025-08-11 16:11:43,124 - INFO - \n",
      "--- Performance for Voting ---\n",
      "2025-08-11 16:11:43,126 - INFO - Accuracy: 0.9913\n",
      "2025-08-11 16:11:43,127 - INFO - Precision (Spam): 0.9919\n",
      "2025-08-11 16:11:43,130 - INFO - Recall (Spam): 0.9389\n",
      "2025-08-11 16:11:43,131 - INFO - F1-Score (Spam): 0.9647\n",
      "2025-08-11 16:11:43,154 - INFO - \n",
      "Full Classification Report for Voting:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      1.00       903\n",
      "        spam       0.99      0.94      0.96       131\n",
      "\n",
      "    accuracy                           0.99      1034\n",
      "   macro avg       0.99      0.97      0.98      1034\n",
      "weighted avg       0.99      0.99      0.99      1034\n",
      "\n",
      "2025-08-11 16:11:43,159 - INFO - \n",
      "Raw Confusion Matrix for Voting:\n",
      "[[902   1]\n",
      " [  8 123]]\n",
      "2025-08-11 16:11:43,354 - INFO - Confusion matrix plot for Voting saved to /home/dev/spam_classifier_project/plots/confusion_matrix_Voting_20250811_160418.png.\n",
      "2025-08-11 16:11:43,357 - INFO - \n",
      "--- Overall Best Model Identified: Voting (F1-Score on Spam: 0.9647) ---\n",
      "2025-08-11 16:11:43,358 - INFO - All model evaluations completed.\n",
      "2025-08-11 16:11:43,406 - INFO - Best performing model (Voting) saved to /home/dev/spam_classifier_project/models/best_model_Voting_20250811_161143.pkl\n",
      "2025-08-11 16:11:44,051 - INFO - Model performance comparison plot saved to /home/dev/spam_classifier_project/plots/model_performance_comparison_20250811_160418.png.\n",
      "2025-08-11 16:11:44,052 - INFO - --- Completed Pipeline Step: Final Model Training & Evaluation ---\n",
      "\n",
      "2025-08-11 16:11:44,053 - INFO - Spam classification pipeline completed successfully.\n",
      "2025-08-11 16:11:44,054 - INFO - \n",
      "=== Spam Classification Pipeline Completed Successfully ===\n",
      "2025-08-11 16:11:44,055 - INFO - Overall Model Performance Summary (Sorted by F1-Score on Spam):\n",
      "       Model  Accuracy  Precision (Spam)  Recall (Spam)  F1-Score (Spam)\n",
      "0     Voting  0.991296          0.991935       0.938931         0.964706\n",
      "1        SVC  0.985493          0.939394       0.946565         0.942966\n",
      "2        XGB  0.984526          0.938931       0.938931         0.938931\n",
      "3       GBDT  0.983559          0.931818       0.938931         0.935361\n",
      "4         RF  0.982592          0.959350       0.900763         0.929134\n",
      "5         LR  0.978723          0.897810       0.938931         0.917910\n",
      "6        BgC  0.978723          0.929134       0.900763         0.914729\n",
      "7   AdaBoost  0.975822          0.868056       0.954198         0.909091\n",
      "8        ETC  0.974855          0.920000       0.877863         0.898438\n",
      "9         DT  0.938104          0.698225       0.900763         0.786667\n",
      "10        KN  0.761122          0.346561       1.000000         0.514735\n",
      "2025-08-11 16:11:44,060 - INFO - \n",
      "Best Performing Model Identified: Voting\n",
      "2025-08-11 16:11:44,061 - INFO - Check '/home/dev/spam_classifier_project/plots' for EDA and Confusion Matrix plots.\n",
      "2025-08-11 16:11:44,062 - INFO - Check '/home/dev/spam_classifier_project/models' for the saved best model.\n",
      "2025-08-11 16:11:44,063 - INFO - \n",
      "--- Demonstrating Model Inference from Saved Model ---\n",
      "2025-08-11 16:11:44,066 - INFO - Attempting to load the latest best model from: /home/dev/spam_classifier_project/models/best_model_Voting_20250811_161143.pkl\n",
      "2025-08-11 16:11:44,108 - INFO - NLTK punkt resource found.\n",
      "2025-08-11 16:11:44,109 - INFO - NLTK stopwords resource found.\n",
      "2025-08-11 16:11:44,110 - INFO - NLTK punkt_tab resource found.\n",
      "2025-08-11 16:11:44,112 - INFO - Initialized all individual and ensemble classifiers.\n",
      "2025-08-11 16:11:44,113 - INFO - SpamClassifier initialized successfully.\n",
      "2025-08-11 16:11:44,114 - INFO - Loading SentenceTransformer by name: 'all-MiniLM-L6-v2'\n",
      "2025-08-11 16:11:44,117 - INFO - Use pytorch device_name: cpu\n",
      "2025-08-11 16:11:44,117 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-08-11 16:11:49,946 - INFO - Model 'Voting' loaded successfully from /home/dev/spam_classifier_project/models/best_model_Voting_20250811_161143.pkl for inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 16.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for SPAM text: 'WINNER! You have been selected for a Â£1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.' -> Label: spam, Spam Confidence: 0.9887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|ââââââââââ| 1/1 [00:00<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for HAM text: 'Hey, just checking in. How are you doing today? Let's catch up soon for coffee!' -> Label: ham, Ham Confidence: 0.9926\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import pickle\n",
    "import warnings\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, f1_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier,\n",
    "                             BaggingClassifier, ExtraTreesClassifier,\n",
    "                             GradientBoostingClassifier, VotingClassifier,\n",
    "                             StackingClassifier)\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import optuna\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Determine Base Directory for Notebook/Script ---\n",
    "try:\n",
    "    current_script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    base_directory = current_script_dir\n",
    "    print(f\"Running as a script. Base directory set to: '{base_directory}'\")\n",
    "except NameError:\n",
    "    base_directory = os.getcwd()\n",
    "    print(f\"Running in a notebook environment. Base directory set to CWD: '{base_directory}'\")\n",
    "\n",
    "\n",
    "# --- Configuration (Externalize for production) ---\n",
    "class Config:\n",
    "    DATA_PATH = os.path.join(base_directory, 'spam.csv')\n",
    "    SENTENCE_TRANSFORMER_MODEL = 'all-MiniLM-L6-v2'\n",
    "    LOG_FILE = os.path.join(base_directory, 'spam_classifier.log')\n",
    "    RANDOM_STATE = 42\n",
    "    TEST_SIZE = 0.2\n",
    "    N_TRIALS_OPTUNA = 15\n",
    "    PLOTS_DIR = os.path.join(base_directory, 'plots')\n",
    "    MODELS_DIR = os.path.join(base_directory, 'models')\n",
    "\n",
    "# Ensure plot and model directories exist at startup\n",
    "os.makedirs(Config.PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(Config.MODELS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "class SpamClassifier:\n",
    "    def __init__(self):\n",
    "        self._configure_logging()\n",
    "        self._verify_nltk_resources()\n",
    "        self._configure_matplotlib()\n",
    "        self.df = None\n",
    "        self.encoder = LabelEncoder()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.sentence_transformer_model = None\n",
    "        self.X, self.y = None, None\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = [None]*4\n",
    "        self.clfs = {}\n",
    "        self.best_tuned_models_params = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "        self.performance_df = pd.DataFrame()\n",
    "        self._initialize_classifiers()\n",
    "        logging.info(\"SpamClassifier initialized successfully.\")\n",
    "\n",
    "    def _configure_logging(self) -> None:\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "            handlers=[\n",
    "                logging.FileHandler(Config.LOG_FILE),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "        warnings.filterwarnings('ignore')\n",
    "\n",
    "    def _verify_nltk_resources(self) -> None:\n",
    "        resources = [\n",
    "            ('tokenizers/punkt', 'punkt'),\n",
    "            ('corpora/stopwords', 'stopwords'),\n",
    "            ('tokenizers/punkt_tab', 'punkt_tab')\n",
    "        ]\n",
    "        for path, package in resources:\n",
    "            try:\n",
    "                nltk.data.find(path)\n",
    "                logging.info(f\"NLTK {package} resource found.\")\n",
    "            except LookupError:\n",
    "                logging.warning(f\"NLTK {package} not found. Attempting to download...\")\n",
    "                try:\n",
    "                    nltk.download(package, quiet=True)\n",
    "                    logging.info(f\"NLTK {package} downloaded successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.critical(f\"Failed to download NLTK {package}. Error: {e}\")\n",
    "                    sys.exit(1)\n",
    "\n",
    "    def _configure_matplotlib(self) -> None:\n",
    "        plt.ioff()\n",
    "        sns.set(style='whitegrid', palette='viridis')\n",
    "\n",
    "    def _initialize_classifiers(self) -> None:\n",
    "        \"\"\"Initializes all individual and ensemble classifiers.\"\"\"\n",
    "        self.clfs = {\n",
    "            'LR': LogisticRegression(\n",
    "                solver='liblinear',\n",
    "                penalty='l1',\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                max_iter=1000\n",
    "            ),\n",
    "            'RF': RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=-1\n",
    "            ),\n",
    "            'XGB': XGBClassifier(\n",
    "                n_estimators=100,\n",
    "                random_state=Config.RANDOM_STATE,\n",
    "                eval_metric='logloss',\n",
    "                scale_pos_weight=1\n",
    "            ),\n",
    "            'SVC': SVC(kernel='sigmoid', gamma=1.0, probability=True, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "            'KN': KNeighborsClassifier(),\n",
    "            'AdaBoost': AdaBoostClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'BgC': BaggingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, n_jobs=-1),\n",
    "            'ETC': ExtraTreesClassifier(n_estimators=100, random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1),\n",
    "            'GBDT': GradientBoostingClassifier(n_estimators=100, random_state=Config.RANDOM_STATE),\n",
    "            'DT': DecisionTreeClassifier(max_depth=5, random_state=Config.RANDOM_STATE, class_weight='balanced'),\n",
    "        }\n",
    "        # Add a VotingClassifier using some of the best models\n",
    "        self.clfs['Voting'] = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('xgb', self.clfs['XGB']),\n",
    "                ('svc', self.clfs['SVC']),\n",
    "                ('rf', self.clfs['RF']),\n",
    "            ],\n",
    "            voting='soft',  # Use 'soft' voting for probability-based prediction\n",
    "            weights=[0.3, 0.4, 0.3],  # Example weights (can be tuned)\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        logging.info(\"Initialized all individual and ensemble classifiers.\")\n",
    "\n",
    "    def load_data(self) -> None:\n",
    "        try:\n",
    "            if not os.path.exists(Config.DATA_PATH):\n",
    "                raise FileNotFoundError(f\"Data file not found at {os.path.abspath(Config.DATA_PATH)}\")\n",
    "            self.df = pd.read_csv(Config.DATA_PATH, encoding='latin-1')\n",
    "            if len(self.df) < 100:\n",
    "                raise ValueError(f\"Dataset too small ({len(self.df)} samples). Minimum 100 samples required for robust analysis.\")\n",
    "            logging.info(f\"Loaded {len(self.df)} records from {Config.DATA_PATH}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data loading failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def clean_data(self) -> None:\n",
    "        try:\n",
    "            if 'v1' in self.df.columns and 'v2' in self.df.columns:\n",
    "                self.df = self.df[['v1', 'v2']].copy()\n",
    "                logging.info(\"Selected 'v1' and 'v2' columns from the dataset.\")\n",
    "            else:\n",
    "                found_v1 = next((col for col in self.df.columns if 'target' in col.lower() or 'label' in col.lower() or 'type' in col.lower()), None)\n",
    "                found_v2 = next((col for col in self.df.columns if 'text' in col.lower() or 'message' in col.lower() or 'sms' in col.lower()), None)\n",
    "                if found_v1 and found_v2:\n",
    "                    self.df = self.df[[found_v1, found_v2]].copy()\n",
    "                    logging.info(f\"Mapped columns '{found_v1}' to 'target' and '{found_v2}' to 'text' using heuristics.\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not find required 'target' and 'text' columns (v1/v2 or equivalents) in dataset. Found columns: {self.df.columns.tolist()}\")\n",
    "\n",
    "            self.df.columns = ['target', 'text']\n",
    "            valid_targets = {'ham', 'spam'}\n",
    "            invalid_targets = set(self.df['target'].unique()) - valid_targets\n",
    "            if invalid_targets:\n",
    "                logging.warning(f\"Invalid target values found: {invalid_targets}. Filtering out rows with these values.\")\n",
    "                self.df = self.df[self.df['target'].isin(valid_targets)]\n",
    "                if self.df.empty:\n",
    "                    raise ValueError(\"No valid 'ham' or 'spam' records remaining after filtering invalid targets. Dataset is empty.\")\n",
    "\n",
    "            self.df['target'] = self.encoder.fit_transform(self.df['target'])\n",
    "            initial_rows = len(self.df)\n",
    "            self.df.drop_duplicates(inplace=True)\n",
    "            self.df.dropna(inplace=True)\n",
    "\n",
    "            logging.info(f\"Cleaned dataset. Removed {initial_rows - len(self.df)} duplicates/nulls. Remaining: {len(self.df)} records.\")\n",
    "            if self.df.empty:\n",
    "                raise ValueError(\"Dataset became empty after cleaning steps. Check data quality or initial loading.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data cleaning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _safe_tokenize(self, text: str) -> list[str]:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for tokenization: {text[:50]}...\")\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "            return [t for t in tokens if t.isalnum() and t not in string.punctuation]\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Tokenization failed for text (first 50 chars: '{text[:50]}...'). Returning empty list. Error: {e}\")\n",
    "            return []\n",
    "\n",
    "    def eda(self) -> None:\n",
    "        try:\n",
    "            self.df['num_words'] = self.df['text'].apply(lambda x: len(self._safe_tokenize(x)))\n",
    "            self.df['num_chars'] = self.df['text'].apply(len)\n",
    "            self.df['num_sentences'] = self.df['text'].apply(lambda x:len(nltk.sent_tokenize(x)))\n",
    "\n",
    "            ham_count = self.df[self.df['target'] == self.encoder.transform(['ham'])[0]].shape[0]\n",
    "            spam_count = self.df[self.df['target'] == self.encoder.transform(['spam'])[0]].shape[0]\n",
    "            if spam_count > 0:\n",
    "                scale_pos_weight_val = ham_count / spam_count\n",
    "                if 'XGB' in self.clfs:\n",
    "                    self.clfs['XGB'].set_params(scale_pos_weight=scale_pos_weight_val)\n",
    "                logging.info(f\"Set XGBoost scale_pos_weight to: {scale_pos_weight_val:.2f} (Ham:{ham_count}, Spam:{spam_count})\")\n",
    "            else:\n",
    "                logging.warning(\"No spam samples found to calculate scale_pos_weight for XGBoost. Defaulting to 1.\")\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            fig1, ax1 = plt.subplots(figsize=(8, 8))\n",
    "            self.df['target'].value_counts().plot(\n",
    "                kind='pie', ax=ax1, autopct='%1.1f%%',\n",
    "                labels=self.encoder.inverse_transform(self.df['target'].value_counts().index),\n",
    "                colors=sns.color_palette('pastel')[0:2],\n",
    "                explode=[0, 0.1]\n",
    "            )\n",
    "            ax1.set_title('Target Class Distribution')\n",
    "            ax1.set_ylabel('')\n",
    "            fig1_filename = os.path.join(Config.PLOTS_DIR, f'target_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig1_filename, bbox_inches='tight')\n",
    "            plt.close(fig1)\n",
    "            logging.info(f\"Target distribution plot saved to {fig1_filename}.\")\n",
    "\n",
    "            fig2, ax2 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_words', ax=ax2, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax2.set_title('Word Count Distribution by Target Class')\n",
    "            ax2.set_xlabel('Number of Words')\n",
    "            ax2.set_ylabel('Count')\n",
    "            ax2.legend()\n",
    "            fig2_filename = os.path.join(Config.PLOTS_DIR, f'word_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig2_filename, bbox_inches='tight')\n",
    "            plt.close(fig2)\n",
    "            logging.info(f\"Word count distribution plot saved to {fig2_filename}.\")\n",
    "\n",
    "            fig3, ax3 = plt.subplots(figsize=(14, 6))\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['ham'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='blue', label='Ham')\n",
    "            sns.histplot(data=self.df[self.df['target'] == self.encoder.transform(['spam'])[0]], x='num_chars', ax=ax3, bins=50, kde=True, color='red', label='Spam')\n",
    "            ax3.set_title('Character Count Distribution by Target Class')\n",
    "            ax3.set_xlabel('Number of Characters')\n",
    "            ax3.set_ylabel('Count')\n",
    "            ax3.legend()\n",
    "            fig3_filename = os.path.join(Config.PLOTS_DIR, f'char_count_distribution_{timestamp}.png')\n",
    "            plt.savefig(fig3_filename, bbox_inches='tight')\n",
    "            plt.close(fig3)\n",
    "            logging.info(f\"Character count distribution plot saved to {fig3_filename}.\")\n",
    "\n",
    "            fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(self.df[['num_chars', 'num_words', 'num_sentences', 'target']].corr(), annot=True, cmap='coolwarm', ax=ax4)\n",
    "            ax4.set_title('Correlation Matrix of Text Features and Target')\n",
    "            fig4_filename = os.path.join(Config.PLOTS_DIR, f'correlation_heatmap_{timestamp}.png')\n",
    "            plt.savefig(fig4_filename, bbox_inches='tight')\n",
    "            plt.close(fig4)\n",
    "            logging.info(f\"Correlation heatmap plot saved to {fig4_filename}.\")\n",
    "\n",
    "            logging.info(f\"Descriptive statistics for Ham emails:\\n{self.df[self.df['target'] == self.encoder.transform(['ham'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "            logging.info(f\"Descriptive statistics for Spam emails:\\n{self.df[self.df['target'] == self.encoder.transform(['spam'])[0]][['num_chars', 'num_words', 'num_sentences']].describe()}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"EDA process failed: {e}\")\n",
    "            raise\n",
    "\n",
    "    def transform_text(self, text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "            logging.debug(f\"Coerced non-string text to string for transform_text: {text[:50]}...\")\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        processed_tokens = [token for token in tokens if token.isalnum()]\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_tokens = [token for token in processed_tokens if token not in stop_words and token not in string.punctuation]\n",
    "        stemmed_tokens = [self.ps.stem(token) for token in filtered_tokens]\n",
    "        final_tokens = [token for token in stemmed_tokens if len(token) > 1 or token.isdigit()]\n",
    "        return \" \".join(final_tokens)\n",
    "\n",
    "    def preprocess_text(self) -> None:\n",
    "        try:\n",
    "            logging.info(\"\\n--- Text Preprocessing for EDA and Visualizations ---\")\n",
    "            self.df['transformed_text'] = self.df['text'].apply(self.transform_text)\n",
    "            logging.info(\"Text transformation for EDA complete. Example:\")\n",
    "            logging.info(f\"\\n{self.df[['text', 'transformed_text']].head().to_string()}\")\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            logging.info(\"\\nGenerating Word Clouds (saved to plots directory):\")\n",
    "            spam_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(spam_wc)\n",
    "            plt.title('Spam Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_spam_filename = os.path.join(Config.PLOTS_DIR, f'spam_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_spam_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Spam word cloud saved to {wc_spam_filename}.\")\n",
    "\n",
    "            ham_wc = WordCloud(width=800, height=400, min_font_size=10, background_color='white').generate(\n",
    "                self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text'].str.cat(sep=\" \")\n",
    "            )\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(ham_wc)\n",
    "            plt.title('Ham Word Cloud')\n",
    "            plt.axis('off')\n",
    "            wc_ham_filename = os.path.join(Config.PLOTS_DIR, f'ham_wordcloud_{timestamp}.png')\n",
    "            plt.savefig(wc_ham_filename, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            logging.info(f\"Ham word cloud saved to {wc_ham_filename}.\")\n",
    "\n",
    "            logging.info(\"\\nMost common words in Spam (saved as plot):\")\n",
    "            spam_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['spam'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(spam_corpus, title='Top 30 Spam Words', filename=f'top_spam_words_{timestamp}.png')\n",
    "\n",
    "            logging.info(\"\\nMost common words in Ham (saved as plot):\")\n",
    "            ham_corpus = ' '.join(self.df[self.df['target'] == self.encoder.transform(['ham'])[0]]['transformed_text']).split()\n",
    "            self._plot_most_common_words(ham_corpus, title='Top 30 Ham Words', filename=f'top_ham_words_{timestamp}.png')\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text preprocessing for EDA failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _plot_most_common_words(self, corpus: list[str], title: str, n: int = 30, filename: str = \"common_words.png\") -> None:\n",
    "        common_words = Counter(corpus).most_common(n)\n",
    "        df_common_words = pd.DataFrame(common_words, columns=['Word', 'Count'])\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.barplot(x='Word', y='Count', data=df_common_words, ax=ax, palette='viridis')\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation='vertical')\n",
    "        ax.set_title(title)\n",
    "        plot_filepath = os.path.join(Config.PLOTS_DIR, filename)\n",
    "        plt.savefig(plot_filepath, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Plot '{title}' saved to {plot_filepath}.\")\n",
    "\n",
    "    def vectorize_text_with_embeddings(self) -> None:\n",
    "        try:\n",
    "            logging.info(f\"\\n--- Text Vectorization (SentenceTransformer: {Config.SENTENCE_TRANSFORMER_MODEL}) ---\")\n",
    "            if self.sentence_transformer_model is None:\n",
    "                self.sentence_transformer_model = SentenceTransformer(Config.SENTENCE_TRANSFORMER_MODEL)\n",
    "\n",
    "            self.X = self.sentence_transformer_model.encode(\n",
    "                self.df['text'].tolist(),\n",
    "                show_progress_bar=True,\n",
    "                convert_to_tensor=False,\n",
    "                batch_size=64\n",
    "            )\n",
    "            self.y = self.df['target'].values\n",
    "            logging.info(f\"SentenceTransformer embedding complete. X shape: {self.X.shape}, Y shape: {self.y.shape}.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Text vectorization failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def split_data(self) -> None:\n",
    "        try:\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                self.X, self.y, test_size=Config.TEST_SIZE,\n",
    "                random_state=Config.RANDOM_STATE, stratify=self.y)\n",
    "            logging.info(f\"Data split: Train {len(self.X_train)} samples, Test {len(self.X_test)} samples.\")\n",
    "            logging.info(f\"Train target distribution: {np.bincount(self.y_train)}\")\n",
    "            logging.info(f\"Test target distribution: {np.bincount(self.y_test)}\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Data splitting failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _objective(self, trial: optuna.trial.Trial, model_name: str) -> float:\n",
    "        if model_name == 'LR':\n",
    "            c_param = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
    "            solver = trial.suggest_categorical('solver', ['liblinear', 'saga'])\n",
    "            model = LogisticRegression(C=c_param, solver=solver, random_state=Config.RANDOM_STATE,\n",
    "                                       class_weight='balanced', max_iter=2000,\n",
    "                                       n_jobs=-1 if solver == 'saga' else None)\n",
    "        elif model_name == 'RF':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                           min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'XGB':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 12)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.005, 0.5)\n",
    "            subsample = trial.suggest_uniform('subsample', 0.6, 1.0)\n",
    "            colsample_bytree = trial.suggest_uniform('colsample_bytree', 0.6, 1.0)\n",
    "            gamma = trial.suggest_loguniform('gamma', 1e-8, 1.0)\n",
    "            current_scale_pos_weight = self.clfs['XGB'].get_params().get('scale_pos_weight', 1)\n",
    "            model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                  learning_rate=learning_rate, subsample=subsample,\n",
    "                                  colsample_bytree=colsample_bytree, gamma=gamma,\n",
    "                                  random_state=Config.RANDOM_STATE,\n",
    "                                  eval_metric='logloss',\n",
    "                                  scale_pos_weight=current_scale_pos_weight)\n",
    "        elif model_name == 'SVC':\n",
    "            C_param = trial.suggest_loguniform('C', 1e-2, 1e2)\n",
    "            gamma_param = trial.suggest_loguniform('gamma', 1e-3, 1e1)\n",
    "            kernel = trial.suggest_categorical('kernel', ['rbf', 'sigmoid'])\n",
    "            model = SVC(C=C_param, gamma=gamma_param, kernel=kernel, probability=True,\n",
    "                        random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        elif model_name == 'KN':\n",
    "            n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n",
    "            weights = trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            algorithm = trial.suggest_categorical('algorithm', ['auto', 'ball_tree', 'kd_tree', 'brute'])\n",
    "            model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, algorithm=algorithm, n_jobs=-1)\n",
    "        elif model_name == 'AdaBoost':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            model = AdaBoostClassifier(n_estimators=n_estimators, learning_rate=learning_rate, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'BgC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            model = BaggingClassifier(n_estimators=n_estimators, random_state=Config.RANDOM_STATE, n_jobs=-1)\n",
    "        elif model_name == 'ETC':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            max_depth = trial.suggest_int('max_depth', 5, 40, log=True)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            model = ExtraTreesClassifier(n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                         min_samples_split=min_samples_split,\n",
    "                                         min_samples_leaf=min_samples_leaf,\n",
    "                                         random_state=Config.RANDOM_STATE, class_weight='balanced', n_jobs=-1)\n",
    "        elif model_name == 'GBDT':\n",
    "            n_estimators = trial.suggest_int('n_estimators', 50, 300)\n",
    "            learning_rate = trial.suggest_loguniform('learning_rate', 0.01, 1.0)\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=Config.RANDOM_STATE)\n",
    "        elif model_name == 'DT':\n",
    "            max_depth = trial.suggest_int('max_depth', 3, 20)\n",
    "            min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
    "            min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "            criterion = trial.suggest_categorical('criterion', ['gini', 'entropy'])\n",
    "            model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split,\n",
    "                                           min_samples_leaf=min_samples_leaf, criterion=criterion,\n",
    "                                           random_state=Config.RANDOM_STATE, class_weight='balanced')\n",
    "        else:\n",
    "            raise ValueError(f\"Model '{model_name}' is not configured for Optuna tuning.\")\n",
    "\n",
    "        pipeline = ImbPipeline([\n",
    "            ('smote', SMOTE(random_state=Config.RANDOM_STATE)),\n",
    "            ('classifier', model)\n",
    "        ])\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_STATE)\n",
    "        scores = cross_val_score(pipeline, self.X_train, self.y_train, cv=cv, scoring='f1', n_jobs=-1)\n",
    "        return scores.mean()\n",
    "\n",
    "    def tune_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.y_train is None:\n",
    "                logging.error(\"Data not split for tuning. Calling split_data().\")\n",
    "                self.split_data()\n",
    "\n",
    "            logging.info(\"Starting hyperparameter tuning with Optuna for selected models...\")\n",
    "            models_to_tune = ['LR', 'RF', 'XGB', 'SVC', 'ETC']\n",
    "\n",
    "            for name in models_to_tune:\n",
    "                if name not in self.clfs:\n",
    "                    logging.warning(f\"Model '{name}' not found in initialized classifiers, skipping tuning.\")\n",
    "                    continue\n",
    "\n",
    "                logging.info(f\"Tuning {name} model with {Config.N_TRIALS_OPTUNA} trials...\")\n",
    "                study = optuna.create_study(direction='maximize',\n",
    "                                            sampler=optuna.samplers.TPESampler(seed=Config.RANDOM_STATE),\n",
    "                                            study_name=f\"{name}_tuning_study\")\n",
    "\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", UserWarning)\n",
    "                    study.optimize(lambda trial: self._objective(trial, name),\n",
    "                                   n_trials=Config.N_TRIALS_OPTUNA,\n",
    "                                   show_progress_bar=True,\n",
    "                                   gc_after_trial=True)\n",
    "\n",
    "                self.best_tuned_models_params[name] = study.best_trial.params\n",
    "                logging.info(f\"Best parameters for {name}: {study.best_trial.params}\")\n",
    "                logging.info(f\"Best cross-validated F1-score for {name}: {study.best_trial.value:.4f}\")\n",
    "\n",
    "                self.clfs[name].set_params(**study.best_trial.params)\n",
    "                if name == 'XGB':\n",
    "                    current_scale_pos_weight = self.clfs[name].get_params().get('scale_pos_weight', 1)\n",
    "                    self.clfs[name].set_params(scale_pos_weight=current_scale_pos_weight)\n",
    "\n",
    "            logging.info(\"Hyperparameter tuning completed for all selected models.\")\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Model tuning failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def train_final_models(self) -> None:\n",
    "        try:\n",
    "            if self.X_train is None or self.X_test is None:\n",
    "                 logging.error(\"Data not split for final training. Calling split_data().\")\n",
    "                 self.split_data()\n",
    "\n",
    "            logging.info(\"Applying SMOTE to the entire training data for final model training...\")\n",
    "            smote = SMOTE(random_state=Config.RANDOM_STATE)\n",
    "            X_train_resampled, y_train_resampled = smote.fit_resample(self.X_train, self.y_train)\n",
    "            logging.info(f\"SMOTE applied. Original train: {len(self.X_train)} samples. Resampled train: {len(X_train_resampled)} samples.\")\n",
    "\n",
    "            results = []\n",
    "            best_f1_overall = -1\n",
    "            self.best_model = None\n",
    "            self.best_model_name = None\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            # Training loop now includes the VotingClassifier\n",
    "            for name, model in self.clfs.items():\n",
    "                logging.info(f\"Training final {name} model on resampled data and evaluating...\")\n",
    "                try:\n",
    "                    # To use the ImbPipeline, we need to pass the model, not just the classifier\n",
    "                    pipeline = ImbPipeline([('smote', SMOTE(random_state=Config.RANDOM_STATE)), ('classifier', model)])\n",
    "                    pipeline.fit(self.X_train, self.y_train)\n",
    "                    y_pred = pipeline.predict(self.X_test)\n",
    "\n",
    "                    accuracy = accuracy_score(self.y_test, y_pred)\n",
    "                    precision = precision_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    recall = recall_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "                    f1 = f1_score(self.y_test, y_pred, pos_label=self.encoder.transform(['spam'])[0], zero_division=0)\n",
    "\n",
    "                    report_dict = classification_report(self.y_test, y_pred, target_names=self.encoder.classes_, output_dict=True)\n",
    "\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': accuracy,\n",
    "                        'Precision (Spam)': precision,\n",
    "                        'Recall (Spam)': recall,\n",
    "                        'F1-Score (Spam)': f1,\n",
    "                        'Full Classification Report': report_dict\n",
    "                    })\n",
    "\n",
    "                    logging.info(f\"\\n--- Performance for {name} ---\")\n",
    "                    logging.info(f\"Accuracy: {accuracy:.4f}\")\n",
    "                    logging.info(f\"Precision (Spam): {precision:.4f}\")\n",
    "                    logging.info(f\"Recall (Spam): {recall:.4f}\")\n",
    "                    logging.info(f\"F1-Score (Spam): {f1:.4f}\")\n",
    "                    logging.info(f\"\\nFull Classification Report for {name}:\\n{classification_report(self.y_test, y_pred, target_names=self.encoder.classes_)}\")\n",
    "\n",
    "                    cm = confusion_matrix(self.y_test, y_pred)\n",
    "                    logging.info(f\"\\nRaw Confusion Matrix for {name}:\\n{cm}\")\n",
    "\n",
    "                    fig_cm, ax_cm = plt.subplots(figsize=(7, 6))\n",
    "                    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                                xticklabels=self.encoder.classes_,\n",
    "                                yticklabels=self.encoder.classes_,\n",
    "                                linecolor='gray', linewidths=0.5,\n",
    "                                annot_kws={\"size\": 14})\n",
    "                    ax_cm.set_xlabel('Predicted Label', fontsize=12)\n",
    "                    ax_cm.set_ylabel('True Label', fontsize=12)\n",
    "                    ax_cm.set_title(f'Confusion Matrix for {name}', fontsize=14)\n",
    "                    cm_filename = os.path.join(Config.PLOTS_DIR, f'confusion_matrix_{name}_{timestamp}.png')\n",
    "                    plt.savefig(cm_filename, bbox_inches='tight')\n",
    "                    plt.close(fig_cm)\n",
    "                    logging.info(f\"Confusion matrix plot for {name} saved to {cm_filename}.\")\n",
    "\n",
    "                    if f1 > best_f1_overall:\n",
    "                        best_f1_overall = f1\n",
    "                        self.best_model_name = name\n",
    "                        self.best_model = pipeline # Store the entire pipeline\n",
    "                except Exception as model_e:\n",
    "                    logging.error(f\"Error training or evaluating model {name}: {model_e}\")\n",
    "                    results.append({\n",
    "                        'Model': name,\n",
    "                        'Accuracy': np.nan,\n",
    "                        'Precision (Spam)': np.nan,\n",
    "                        'Recall (Spam)': np.nan,\n",
    "                        'F1-Score (Spam)': np.nan,\n",
    "                        'Full Classification Report': {'error': str(model_e)}\n",
    "                    })\n",
    "\n",
    "            self.performance_df = pd.DataFrame(results)\n",
    "            self.performance_df = self.performance_df.sort_values(by='F1-Score (Spam)', ascending=False).reset_index(drop=True)\n",
    "            logging.info(f\"\\n--- Overall Best Model Identified: {self.best_model_name} (F1-Score on Spam: {best_f1_overall:.4f}) ---\")\n",
    "            logging.info(\"All model evaluations completed.\")\n",
    "            self._save_best_model()\n",
    "            self._plot_performance_comparison(timestamp)\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Final model training and evaluation failed: {e}\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    def _save_best_model(self) -> None:\n",
    "        \"\"\"Saves the best performing model and related components to a pickle file.\"\"\"\n",
    "        try:\n",
    "            if self.best_model is None or self.best_model_name is None:\n",
    "                logging.warning(\"No best model identified or stored. Skipping model save operation.\")\n",
    "                return\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            model_filename = os.path.join(Config.MODELS_DIR, f'best_model_{self.best_model_name}_{timestamp}.pkl')\n",
    "            with open(model_filename, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'model': self.best_model,\n",
    "                    'transformer': Config.SENTENCE_TRANSFORMER_MODEL,\n",
    "                    'encoder': self.encoder,\n",
    "                    'model_name': self.best_model_name,\n",
    "                    'performance_summary': self.performance_df.to_dict('records')\n",
    "                }, f)\n",
    "            logging.info(f\"Best performing model ({self.best_model_name}) saved to {model_filename}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save the best model: {e}\")\n",
    "\n",
    "    def _plot_performance_comparison(self, timestamp: str) -> None:\n",
    "        if self.performance_df.empty:\n",
    "            logging.warning(\"Performance DataFrame is empty, cannot plot comparison.\")\n",
    "            return\n",
    "        plot_df = self.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].copy()\n",
    "        plot_df_melted = plot_df.melt(id_vars=\"Model\", var_name=\"Metric\", value_name=\"Score\")\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        sns.barplot(x='Model', y='Score', hue='Metric', data=plot_df_melted, palette='tab10', ax=ax)\n",
    "        ax.set_ylim(0.5, 1.0)\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax.set_title('Model Performance Comparison (Test Set)')\n",
    "        ax.set_ylabel('Score')\n",
    "        ax.set_xlabel('Model')\n",
    "        ax.legend(title='Metrics', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.tight_layout()\n",
    "        plot_filename = os.path.join(Config.PLOTS_DIR, f'model_performance_comparison_{timestamp}.png')\n",
    "        plt.savefig(plot_filename, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        logging.info(f\"Model performance comparison plot saved to {plot_filename}.\")\n",
    "\n",
    "    def run_pipeline(self) -> bool:\n",
    "        steps = [\n",
    "            ('Data Loading', self.load_data),\n",
    "            ('Data Cleaning', self.clean_data),\n",
    "            ('EDA and Feature Engineering', self.eda),\n",
    "            ('Text Preprocessing for EDA', self.preprocess_text),\n",
    "            ('Text Vectorization (Embeddings)', self.vectorize_text_with_embeddings),\n",
    "            ('Data Splitting', self.split_data),\n",
    "            ('Hyperparameter Tuning', self.tune_models),\n",
    "            ('Final Model Training & Evaluation', self.train_final_models)\n",
    "        ]\n",
    "        for name, step in steps:\n",
    "            try:\n",
    "                logging.info(f\"\\n--- Starting Pipeline Step: {name} ---\")\n",
    "                step()\n",
    "                logging.info(f\"--- Completed Pipeline Step: {name} ---\\n\")\n",
    "            except SystemExit:\n",
    "                logging.critical(f\"Pipeline stopped due to critical error in step: '{name}'.\")\n",
    "                return False\n",
    "            except Exception as e:\n",
    "                logging.critical(f\"Pipeline failed unexpectedly in step '{name}': {e}\")\n",
    "                return False\n",
    "        logging.info(\"Spam classification pipeline completed successfully.\")\n",
    "        return True\n",
    "\n",
    "    @staticmethod\n",
    "    def load_for_inference(model_path: str) -> 'SpamClassifier':\n",
    "        try:\n",
    "            if not os.path.exists(model_path):\n",
    "                raise FileNotFoundError(f\"Model file not found at {os.path.abspath(model_path)}\")\n",
    "            with open(model_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            classifier = SpamClassifier()\n",
    "            classifier.best_model = data['model']\n",
    "            classifier.encoder = data['encoder']\n",
    "            classifier.best_model_name = data.get('model_name', 'Unknown_Model')\n",
    "            transformer_data = data['transformer']\n",
    "            if isinstance(transformer_data, str):\n",
    "                logging.info(f\"Loading SentenceTransformer by name: '{transformer_data}'\")\n",
    "                classifier.sentence_transformer_model = SentenceTransformer(transformer_data)\n",
    "            else:\n",
    "                logging.warning(\"Loaded SentenceTransformer object directly from pickle.\")\n",
    "                classifier.sentence_transformer_model = transformer_data\n",
    "            classifier.ps = PorterStemmer()\n",
    "            logging.info(f\"Model '{classifier.best_model_name}' loaded successfully from {model_path} for inference.\")\n",
    "            return classifier\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Failed to load model for inference from {model_path}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def predict(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Predicts the label for a given text.\n",
    "        This method is now a simplified wrapper for predict_with_confidence.\n",
    "        \"\"\"\n",
    "        prediction_label, _, _ = self.predict_with_confidence(text)\n",
    "        return prediction_label\n",
    "\n",
    "    def predict_with_confidence(self, text: str) -> tuple[str, float, float]:\n",
    "        \"\"\"\n",
    "        Predicts the label and returns the confidence score for spam/ham.\n",
    "        Returns: (prediction_label, spam_confidence, ham_confidence)\n",
    "        \"\"\"\n",
    "        if self.best_model is None or self.sentence_transformer_model is None or self.encoder is None:\n",
    "            logging.error(\"Model components not loaded. Please load model using load_for_inference() before calling predict().\")\n",
    "            raise RuntimeError(\"Model components not available for prediction.\")\n",
    "        try:\n",
    "            vector = self.sentence_transformer_model.encode([text], convert_to_tensor=False)\n",
    "            prediction_encoded = self.best_model.predict(vector)[0]\n",
    "            prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n",
    "\n",
    "            # Get probabilities and confidence\n",
    "            prediction_proba = self.best_model.predict_proba(vector)[0]\n",
    "            classes = self.best_model.named_steps['classifier'].classes_\n",
    "            \n",
    "            spam_prob_idx = np.where(classes == self.encoder.transform(['spam'])[0])[0]\n",
    "            ham_prob_idx = np.where(classes == self.encoder.transform(['ham'])[0])[0]\n",
    "            \n",
    "            spam_confidence = prediction_proba[spam_prob_idx][0] if spam_prob_idx.size > 0 else 0.0\n",
    "            ham_confidence = prediction_proba[ham_prob_idx][0] if ham_prob_idx.size > 0 else 0.0\n",
    "            \n",
    "            return prediction_label, spam_confidence, ham_confidence\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Prediction failed for text '{text[:50]}...': {e}\")\n",
    "            return \"error\", 0.0, 0.0\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    classifier = SpamClassifier()\n",
    "    pipeline_success = classifier.run_pipeline()\n",
    "\n",
    "    if pipeline_success:\n",
    "        logging.info(\"\\n=== Spam Classification Pipeline Completed Successfully ===\")\n",
    "        logging.info(\"Overall Model Performance Summary (Sorted by F1-Score on Spam):\")\n",
    "        print(classifier.performance_df[['Model', 'Accuracy', 'Precision (Spam)', 'Recall (Spam)', 'F1-Score (Spam)']].to_string())\n",
    "        logging.info(f\"\\nBest Performing Model Identified: {classifier.best_model_name}\")\n",
    "        logging.info(f\"Check '{Config.PLOTS_DIR}' for EDA and Confusion Matrix plots.\")\n",
    "        logging.info(f\"Check '{Config.MODELS_DIR}' for the saved best model.\")\n",
    "\n",
    "        try:\n",
    "            logging.info(\"\\n--- Demonstrating Model Inference from Saved Model ---\")\n",
    "            model_files = [f for f in os.listdir(Config.MODELS_DIR) if f.startswith('best_model_') and f.endswith('.pkl')]\n",
    "            if model_files:\n",
    "                latest_model_file = max(model_files, key=lambda f: os.path.getmtime(os.path.join(Config.MODELS_DIR, f)))\n",
    "                latest_model_path = os.path.join(Config.MODELS_DIR, latest_model_file)\n",
    "                logging.info(f\"Attempting to load the latest best model from: {latest_model_path}\")\n",
    "                loaded_classifier = SpamClassifier.load_for_inference(latest_model_path)\n",
    "\n",
    "                test_spam_text_1 = \"WINNER! You have been selected for a Â£1000 prize! Call 09061701300 now or claim at link.co.uk/prize. T&C's apply.\"\n",
    "                test_ham_text_1 = \"Hey, just checking in. How are you doing today? Let's catch up soon for coffee!\"\n",
    "                \n",
    "                label, spam_conf, ham_conf = loaded_classifier.predict_with_confidence(test_spam_text_1)\n",
    "                print(f\"Prediction for SPAM text: '{test_spam_text_1}' -> Label: {label}, Spam Confidence: {spam_conf:.4f}\")\n",
    "                \n",
    "                label, spam_conf, ham_conf = loaded_classifier.predict_with_confidence(test_ham_text_1)\n",
    "                print(f\"Prediction for HAM text: '{test_ham_text_1}' -> Label: {label}, Ham Confidence: {ham_conf:.4f}\")\n",
    "\n",
    "            else:\n",
    "                logging.warning(\"No model files found in the 'models' directory to demonstrate inference. Run the pipeline first.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"An error occurred during the inference demonstration: {e}\")\n",
    "            sys.exit(1)\n",
    "    else:\n",
    "        logging.critical(\"Spam classification pipeline failed during execution. Please review the log file for details.\")\n",
    "        sys.exit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
